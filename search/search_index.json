{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the documentation for Clinical Data Mining @ MSKCC!","text":""},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>CDM Data on cBioPortal</li> <li>CDM Data on Dremio</li> <li>Github Repos</li> <li>CDM Airflow</li> </ul>"},{"location":"reference/","title":"Directory","text":""},{"location":"reference/#data-access-tools","title":"Data Access Tools","text":"<ul> <li>Databricks</li> <li>Minio</li> <li>Dremio</li> </ul>"},{"location":"reference/About-Us/","title":"About Us","text":"<p>The Clinical Data Mining (CDM) team extracts notes, treatment records, and health measurements from the electronic health record generated during the process of cancer care and transforms it into usable, real-world data that can fuel scientific insights.</p> <p>Retrieval of clinical annotation of tumor samples and patients presents a major challenge for data integration, as the current approach of manual abstraction from largely unstructured electronic medical records (EMR) is difficult to scale. However, the use of clinical text classification by means of natural language processing (NLP) and advanced machine learning methods has the potential to unlock information embedded in clinical narratives. The CDM team is creating a hybrid NLP system to leverage against structured and unstructured EMR to identify patient and sample-specific attributes. The development of this system will lead to a robust, large-scale system for enhanced clinical integration with genomic databases that can be used to predict outcomes and treatment responses of individual cancer patients.</p>"},{"location":"reference/databricks/","title":"msk_cdm.databricks","text":""},{"location":"reference/databricks/#msk_cdm.databricks.databricks_api.DatabricksAPI","title":"<code>DatabricksAPI</code>","text":"<p>               Bases: <code>object</code></p> <p>A class to interact with Databricks through its SQL API. This class allows connecting to a Databricks cluster, executing queries, and retrieving the results as pandas DataFrames.</p> Source code in <code>msk_cdm/databricks/databricks_api.py</code> <pre><code>class DatabricksAPI(object):\n\"\"\"A class to interact with Databricks through its SQL API. This class allows\n    connecting to a Databricks cluster, executing queries, and retrieving\n    the results as pandas DataFrames.\"\"\"\n    def __init__(\n            self,\n            client_id: Optional[str] = None,  # Client ID for Service Principal\n            client_secret: Optional[str] = None,  # Client Secret for Service Principal\n            token: Optional[str] = None,\n            hostname: Optional[str] = None,\n            http_path: Optional[str] = None,\n            fname_databricks_env: Optional[str] = None\n    ) -&gt; None:\n\"\"\"Initializes the DatabricksAPI class with minimal changes for OAuth.\n\n\n        Args:\n            client_id: Client ID for Service Principal.\n            client_secret: Client Secret for Service Principal.\n            token: The access token for authentication with Databricks (default is None).\n            hostname: The hostname of the Databricks server (default is None).\n            http_path: The HTTP path for the Databricks SQL endpoint (default is None).\n            fname_databricks_env: The file name of the environment file containing connection parameters (default is None).\n        \"\"\"\n        self._client_id = client_id\n        self._client_secret = client_secret\n        self._TOKEN = token\n        self._HOSTNAME = hostname\n        self._HTTP_PATH = http_path\n        self._sql_client = None\n        self._URL = None\n        self._workspace_client = None\n\n        if fname_databricks_env is not None:\n            print('Parsing env file')\n            self._process_env(fname_databricks_env=fname_databricks_env)\n\n        if self._client_secret is not None:\n            self._connect_with_oauth(\n                client_id=self._client_id,\n                client_secret=self._client_secret,\n                hostname=self._HOSTNAME,\n                http_path=self._HTTP_PATH\n            )\n\n        if self._TOKEN is not None:\n            self._connect_with_token(\n                token=self._TOKEN,\n                hostname=self._HOSTNAME,\n                http_path=self._HTTP_PATH\n            )\n\n        return None\n\n\n    def _connect_with_oauth(\n        self,\n        client_id: str,\n        client_secret: str,\n        hostname: str,\n        http_path: str\n    ) -&gt; None:\n\"\"\" Connect with Service Principle credentials.\n        Establishes a connection to the Databricks cluster using OAuth authentication.\n\n        Args:\n            client_id: The client ID of the service principal for OAuth authentication.\n            client_secret: The client secret of the service principal for OAuth authentication.\n            hostname: The hostname of the Databricks server.\n            http_path: The HTTP path for the Databricks SQL endpoint.\n\n        Returns:\n            None\n        \"\"\"\n        print('Making databricks connection')\n\n        def credential_provider():\n            config = Config(\n                host          = hostname,\n                client_id     = client_id,\n                client_secret = client_secret)\n            return oauth_service_principal(config)\n\n        connection = sql.connect(\n            server_hostname=hostname,\n            http_path=http_path,\n            credentials_provider=credential_provider\n        )\n\n        workspace_client = WorkspaceClient(\n            host=hostname,\n            client_id=client_id,\n            client_secret=client_secret\n        )\n\n        print('Connected.')\n\n        self._sql_client = connection\n        self._workspace_client = workspace_client\n\n        return None\n\n    def _connect_with_token(\n            self,\n            token: str,\n            hostname: str,\n            http_path: str\n    ) -&gt; None:\n\"\"\" Connection with personal token\n        Establishes a connection to the Databricks cluster using the provided\n        access token, hostname, and HTTP path.\n\n        Args:\n            token: The access token for authentication with Databricks.\n            hostname: The hostname of the Databricks server.\n            http_path: The HTTP path for the Databricks SQL endpoint.\n\n        Returns:\n            None\n        \"\"\"\n        print('Making databricks connection')\n        connection = sql.connect(\n            server_hostname=hostname,\n            http_path=http_path,\n            access_token=token\n        )\n\n        workspace_client = WorkspaceClient(\n            host=hostname,\n            token=token\n        )\n\n        print('Connected.')\n\n        self._sql_client = connection\n        self._workspace_client = workspace_client\n\n        return None\n\n    def _process_env(\n            self,\n            fname_databricks_env: str\n    ) -&gt; None:\n\"\"\"\n        Processes the environment file to extract connection parameters such as\n        the access token, hostname, HTTP path, and URL.\n\n        Args:\n            fname_databricks_env: The file name of the environment file containing connection parameters.\n\n        Returns:\n            None\n        \"\"\"\n\n        dict_config = dotenv_values(fname_databricks_env)\n\n        if not self._client_id:\n            self._client_id = dict_config.get(\"CLIENT_ID\", None)  # Retrieve client_id from the environment\n        if not self._client_secret:\n            self._client_secret = dict_config.get(\"CLIENT_SECRET\", None)  # Retrieve client_secret from the environment\n        if not self._TOKEN:\n            self._TOKEN = dict_config.get(\"TOKEN\", None)\n        if not self._HOSTNAME:\n            self._HOSTNAME = dict_config.get(\"HOSTNAME\", None)\n        if not self._URL:\n            self._URL = dict_config.get(\"URL\", None)\n        if not self._HTTP_PATH:\n            self._HTTP_PATH = dict_config.get(\"HTTP_PATH\", None)\n\n        return None\n\n    def query_from_file(\n            self,\n            *,\n            fname_sql: str\n    ) -&gt; pd.DataFrame:\n\"\"\"Query Databricks from a SQL file\n        Executes a Spark SQL query from a file and returns the result as a pandas\n        DataFrame.\n\n        Args:\n            fname_sql: The file name of the SQL file containing the query.\n\n        Returns:\n            df: A DataFrame containing the results of the query.\n        \"\"\"\n        # open SQL file\n        fd = open(fname_sql, 'r')\n        sqlFile = fd.read()\n        fd.close()\n\n        print('Preview of SQL in %s:' % fname_sql)\n        print(sqlFile[:50])\n\n        df = self.query_from_sql(sql=sqlFile)\n\n        return df\n\n    def query_from_sql(\n            self,\n            *,\n            sql: str\n    ) -&gt; pd.DataFrame:\n\"\"\"Query Databricks from a SQL string\n        Executes a Spark SQL query from a string and returns the result as a pandas\n        DataFrame.\n\n        Args:\n            sql: The Spark SQL query string to be executed.\n\n        Returns:\n            df: A DataFrame containing the results of the query.\n        \"\"\"\n\n        cursor = self._sql_client.cursor()\n        for i,query in enumerate(sql.split(';')):\n            cursor.execute(query)\n\n        ### Another way to do the query above is through SQLalchemy\n        # engine = create_engine(\n        #     url = f\"databricks://token:{token}@{hostname}?\" +\n        #           f\"http_path={http_path}&amp;catalog={catalog}&amp;schema={schema}\"\n        # )\n        #\n        # with engine.connect() as conn:\n        #     # This will read the contents of `main.test.some_table`\n        #     df_sql = pd.read_sql(f\"SELECT *, _metadata FROM {catalog}.{schema}.{table}\", conn)\n\n\n        # Gather column names from query\n        column_names = [desc[0] for desc in cursor.description]\n        data = cursor.fetchall()\n\n        # Convert to pandas dataframe\n        df = pd.DataFrame(\n            data,\n            columns=column_names\n        )\n\n        return df\n\n    def read_db_obj(\n            self,\n            volume_path: str,\n            sep: Optional[str] ='\\t'\n    ) -&gt; pd.DataFrame:\n\"\"\"Read object from Databricks volume\n        Reads a CSV/TSV file from the Databricks volume and converts it into a\n        pandas DataFrame.\n\n        Args:\n            volume_path: The path to the file on the Databricks volume.\n            sep: The separator used in the file.\n\n        Returns:\n            df: A DataFrame containing the data from the file.\n        \"\"\"\n        # Read csv/tsv file from volume and convert into Pandas dataframe\n        response = self._workspace_client.files.download(volume_path)\n        data_str = BytesIO(response.contents.read())\n        df = pd.read_csv(data_str, sep=sep)\n\n        return df\n\n    def create_directory_on_volume(\n            self,\n            path: str\n    ) -&gt; None:\n\"\"\"\n        Creates a directory on the Databricks volume at the specified path.\n\n        Args:\n            path: The path where the directory should be created.\n\n        Returns:\n            None\n        \"\"\"\n        # Create a directory for file to be uploaded\n        print('Creating directory on volume: %s' % path)\n        self._workspace_client.files.create_directory(path)\n\n        print('Created')\n\n    def write_db_obj(\n            self,\n            df: pd.DataFrame,\n            volume_path: str,\n            sep: Optional[str] = '\\t',\n            overwrite: Optional[bool] = True,\n            dict_database_table_info: Optional[dict] = None\n    ):\n\"\"\"Write data to Databricks volume\n        Writes a pandas DataFrame to a CSV file on the Databricks volume. Optionally,\n        creates a table in Databricks from the CSV file.\n\n        Args:\n            df: The DataFrame to be written to the file.\n            volume_path: The path where the file should be saved on the Databricks volume.\n            sep: The separator used in the file.\n            overwrite: Whether to overwrite the existing file.\n            dict_database_table_info: A dictionary containing information about the\n                                      database table. If `dict_database_table_info` is used, it must contain these keys\n\n                                        - catalog: Databricks catalog used\n                                        - schema: Schema within the catalog\n                                        - table: Table in the schema that will contain the dataframe information\n                                        - volume_path: Path location on the volume of the object. A csv file for use of this\n                                        - sep: File separator used for the object. Typically, comma or tab separated\n\n        Returns:\n            None\n\n\n        \"\"\"\n        # Create directory on volume for data to be uploaded. If directory, exists, nothing will happen to existing data\n        dir_volume_path = os.path.dirname(volume_path)\n        self.create_directory_on_volume(path=dir_volume_path)\n\n        csv_bytes = df.to_csv(index=False, sep=sep).encode(\"utf-8\")\n        csv_buffer = BytesIO(csv_bytes)\n\n        print('Writing to %s' % volume_path)\n        self._workspace_client.files.upload(\n            volume_path,\n            csv_buffer,\n            overwrite=overwrite\n        )\n        print('Write to volume complete')\n\n        if dict_database_table_info is not None:\n            if sep != dict_database_table_info.get('sep'):\n                dict_database_table_info['sep'] = sep\n                print(\"Conflict with separator in dict; setting to value object was saved as.\")\n\n            self.create_table_from_volume(dict_database_table_info=dict_database_table_info)\n\n        return None\n\n    def _sql_write_creator(\n            self,\n            catalog: str,\n            schema: str,\n            table: str,\n            volume_path: str,\n            sep: Optional[str] = '\\t'\n    ) -&gt; str:\n\"\"\"\n        Generates a SQL query string to create a table in Databricks from a file\n        located on the Databricks volume.\n\n        Args:\n            catalog: The catalog in which the table will be created.\n            schema: The schema within the catalog.\n            table: The name of the table to be created.\n            volume_path: The path to the file on the Databricks volume.\n            sep: The separator used in the file.\n\n        Returns:\n            sql_write: A SQL query string to create the table.\n        \"\"\"\n        sql_write = f\"\"\"\n        DROP TABLE IF EXISTS {catalog}.{schema}.{table};\n        CREATE TABLE IF NOT EXISTS {catalog}.{schema}.{table};\n\n        COPY INTO {catalog}.{schema}.{table} FROM '{volume_path}'\n        FILEFORMAT = CSV \n        FORMAT_OPTIONS ('delimiter' = '{sep}', 'header' = 'true') \n        COPY_OPTIONS ('mergeSchema' = 'true');\n\n        \"\"\"\n\n        return sql_write\n\n    def create_table_from_volume(\n            self,\n            dict_database_table_info: dict\n    ) -&gt; None:\n\"\"\"\n        Creates a SQL table in Databricks from a file located on the Databricks volume.\n\n        Args:\n            dict_database_table_info: A dictionary containing information about the\n                                      database table. If `dict_database_table_info` is used, it must contain these keys\n\n                                        - catalog: Databricks catalog used\n                                        - schema: Schema within the catalog\n                                        - table: Table in the schema that will contain the dataframe information\n                                        - volume_path: Path location on the volume of the object. A csv file for use of this\n                                        - sep: File separator used for the object. Typically, comma or tab separated\n\n        Returns:\n            None\n        \"\"\"\n        catalog = dict_database_table_info.get('catalog')\n        schema = dict_database_table_info.get('schema')\n        table = dict_database_table_info.get('table')\n        volume_path = dict_database_table_info.get('volume_path')\n        sep_of_volume_obj = dict_database_table_info.get('sep')\n        print('Creating SQL table from volume:')\n        print('Catalog: %s' % catalog)\n        print('Schema: %s' % schema)\n        print('Table: %s' % table)\n        print('Volume path: %s' % volume_path)\n        print('Separator: %s' % sep_of_volume_obj)\n\n        sql_write = self._sql_write_creator(\n            catalog=catalog,\n            schema=schema,\n            table=table,\n            volume_path=volume_path,\n            sep=sep_of_volume_obj\n        )\n\n        cursor = self._sql_client.cursor()\n        for i,query in enumerate(sql_write.split(';')[:-1]):\n            print(query)\n            cursor.execute(query)\n\n        print('Table created')\n\n        return None\n\n\n    def close_connection(self):\n\"\"\"\n        Closes the connection to the Databricks cluster.\n\n        Returns:\n            None\n        \"\"\"\n        cursor = self._sql_client.cursor()\n        cursor.close()\n        self._sql_client.close()\n        print('Databricks connection closed')\n\n        return None\n</code></pre>"},{"location":"reference/databricks/#msk_cdm.databricks.databricks_api.DatabricksAPI.__init__","title":"<code>__init__(client_id=None, client_secret=None, token=None, hostname=None, http_path=None, fname_databricks_env=None)</code>","text":"<p>Initializes the DatabricksAPI class with minimal changes for OAuth.</p> <p>Parameters:</p> Name Type Description Default <code>client_id</code> <code>Optional[str]</code> <p>Client ID for Service Principal.</p> <code>None</code> <code>client_secret</code> <code>Optional[str]</code> <p>Client Secret for Service Principal.</p> <code>None</code> <code>token</code> <code>Optional[str]</code> <p>The access token for authentication with Databricks (default is None).</p> <code>None</code> <code>hostname</code> <code>Optional[str]</code> <p>The hostname of the Databricks server (default is None).</p> <code>None</code> <code>http_path</code> <code>Optional[str]</code> <p>The HTTP path for the Databricks SQL endpoint (default is None).</p> <code>None</code> <code>fname_databricks_env</code> <code>Optional[str]</code> <p>The file name of the environment file containing connection parameters (default is None).</p> <code>None</code> Source code in <code>msk_cdm/databricks/databricks_api.py</code> <pre><code>def __init__(\n        self,\n        client_id: Optional[str] = None,  # Client ID for Service Principal\n        client_secret: Optional[str] = None,  # Client Secret for Service Principal\n        token: Optional[str] = None,\n        hostname: Optional[str] = None,\n        http_path: Optional[str] = None,\n        fname_databricks_env: Optional[str] = None\n) -&gt; None:\n\"\"\"Initializes the DatabricksAPI class with minimal changes for OAuth.\n\n\n    Args:\n        client_id: Client ID for Service Principal.\n        client_secret: Client Secret for Service Principal.\n        token: The access token for authentication with Databricks (default is None).\n        hostname: The hostname of the Databricks server (default is None).\n        http_path: The HTTP path for the Databricks SQL endpoint (default is None).\n        fname_databricks_env: The file name of the environment file containing connection parameters (default is None).\n    \"\"\"\n    self._client_id = client_id\n    self._client_secret = client_secret\n    self._TOKEN = token\n    self._HOSTNAME = hostname\n    self._HTTP_PATH = http_path\n    self._sql_client = None\n    self._URL = None\n    self._workspace_client = None\n\n    if fname_databricks_env is not None:\n        print('Parsing env file')\n        self._process_env(fname_databricks_env=fname_databricks_env)\n\n    if self._client_secret is not None:\n        self._connect_with_oauth(\n            client_id=self._client_id,\n            client_secret=self._client_secret,\n            hostname=self._HOSTNAME,\n            http_path=self._HTTP_PATH\n        )\n\n    if self._TOKEN is not None:\n        self._connect_with_token(\n            token=self._TOKEN,\n            hostname=self._HOSTNAME,\n            http_path=self._HTTP_PATH\n        )\n\n    return None\n</code></pre>"},{"location":"reference/databricks/#msk_cdm.databricks.databricks_api.DatabricksAPI.close_connection","title":"<code>close_connection()</code>","text":"<p>Closes the connection to the Databricks cluster.</p> <p>Returns:</p> Type Description <p>None</p> Source code in <code>msk_cdm/databricks/databricks_api.py</code> <pre><code>def close_connection(self):\n\"\"\"\n    Closes the connection to the Databricks cluster.\n\n    Returns:\n        None\n    \"\"\"\n    cursor = self._sql_client.cursor()\n    cursor.close()\n    self._sql_client.close()\n    print('Databricks connection closed')\n\n    return None\n</code></pre>"},{"location":"reference/databricks/#msk_cdm.databricks.databricks_api.DatabricksAPI.create_directory_on_volume","title":"<code>create_directory_on_volume(path)</code>","text":"<p>Creates a directory on the Databricks volume at the specified path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path where the directory should be created.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>msk_cdm/databricks/databricks_api.py</code> <pre><code>def create_directory_on_volume(\n        self,\n        path: str\n) -&gt; None:\n\"\"\"\n    Creates a directory on the Databricks volume at the specified path.\n\n    Args:\n        path: The path where the directory should be created.\n\n    Returns:\n        None\n    \"\"\"\n    # Create a directory for file to be uploaded\n    print('Creating directory on volume: %s' % path)\n    self._workspace_client.files.create_directory(path)\n\n    print('Created')\n</code></pre>"},{"location":"reference/databricks/#msk_cdm.databricks.databricks_api.DatabricksAPI.create_table_from_volume","title":"<code>create_table_from_volume(dict_database_table_info)</code>","text":"<p>Creates a SQL table in Databricks from a file located on the Databricks volume.</p> <p>Parameters:</p> Name Type Description Default <code>dict_database_table_info</code> <code>dict</code> <p>A dictionary containing information about the                       database table. If <code>dict_database_table_info</code> is used, it must contain these keys</p> <pre><code>                    - catalog: Databricks catalog used\n                    - schema: Schema within the catalog\n                    - table: Table in the schema that will contain the dataframe information\n                    - volume_path: Path location on the volume of the object. A csv file for use of this\n                    - sep: File separator used for the object. Typically, comma or tab separated\n</code></pre> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>msk_cdm/databricks/databricks_api.py</code> <pre><code>def create_table_from_volume(\n        self,\n        dict_database_table_info: dict\n) -&gt; None:\n\"\"\"\n    Creates a SQL table in Databricks from a file located on the Databricks volume.\n\n    Args:\n        dict_database_table_info: A dictionary containing information about the\n                                  database table. If `dict_database_table_info` is used, it must contain these keys\n\n                                    - catalog: Databricks catalog used\n                                    - schema: Schema within the catalog\n                                    - table: Table in the schema that will contain the dataframe information\n                                    - volume_path: Path location on the volume of the object. A csv file for use of this\n                                    - sep: File separator used for the object. Typically, comma or tab separated\n\n    Returns:\n        None\n    \"\"\"\n    catalog = dict_database_table_info.get('catalog')\n    schema = dict_database_table_info.get('schema')\n    table = dict_database_table_info.get('table')\n    volume_path = dict_database_table_info.get('volume_path')\n    sep_of_volume_obj = dict_database_table_info.get('sep')\n    print('Creating SQL table from volume:')\n    print('Catalog: %s' % catalog)\n    print('Schema: %s' % schema)\n    print('Table: %s' % table)\n    print('Volume path: %s' % volume_path)\n    print('Separator: %s' % sep_of_volume_obj)\n\n    sql_write = self._sql_write_creator(\n        catalog=catalog,\n        schema=schema,\n        table=table,\n        volume_path=volume_path,\n        sep=sep_of_volume_obj\n    )\n\n    cursor = self._sql_client.cursor()\n    for i,query in enumerate(sql_write.split(';')[:-1]):\n        print(query)\n        cursor.execute(query)\n\n    print('Table created')\n\n    return None\n</code></pre>"},{"location":"reference/databricks/#msk_cdm.databricks.databricks_api.DatabricksAPI.query_from_file","title":"<code>query_from_file(*, fname_sql)</code>","text":"<p>Query Databricks from a SQL file Executes a Spark SQL query from a file and returns the result as a pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>fname_sql</code> <code>str</code> <p>The file name of the SQL file containing the query.</p> required <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>A DataFrame containing the results of the query.</p> Source code in <code>msk_cdm/databricks/databricks_api.py</code> <pre><code>def query_from_file(\n        self,\n        *,\n        fname_sql: str\n) -&gt; pd.DataFrame:\n\"\"\"Query Databricks from a SQL file\n    Executes a Spark SQL query from a file and returns the result as a pandas\n    DataFrame.\n\n    Args:\n        fname_sql: The file name of the SQL file containing the query.\n\n    Returns:\n        df: A DataFrame containing the results of the query.\n    \"\"\"\n    # open SQL file\n    fd = open(fname_sql, 'r')\n    sqlFile = fd.read()\n    fd.close()\n\n    print('Preview of SQL in %s:' % fname_sql)\n    print(sqlFile[:50])\n\n    df = self.query_from_sql(sql=sqlFile)\n\n    return df\n</code></pre>"},{"location":"reference/databricks/#msk_cdm.databricks.databricks_api.DatabricksAPI.query_from_sql","title":"<code>query_from_sql(*, sql)</code>","text":"<p>Query Databricks from a SQL string Executes a Spark SQL query from a string and returns the result as a pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>sql</code> <code>str</code> <p>The Spark SQL query string to be executed.</p> required <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>A DataFrame containing the results of the query.</p> Source code in <code>msk_cdm/databricks/databricks_api.py</code> <pre><code>def query_from_sql(\n        self,\n        *,\n        sql: str\n) -&gt; pd.DataFrame:\n\"\"\"Query Databricks from a SQL string\n    Executes a Spark SQL query from a string and returns the result as a pandas\n    DataFrame.\n\n    Args:\n        sql: The Spark SQL query string to be executed.\n\n    Returns:\n        df: A DataFrame containing the results of the query.\n    \"\"\"\n\n    cursor = self._sql_client.cursor()\n    for i,query in enumerate(sql.split(';')):\n        cursor.execute(query)\n\n    ### Another way to do the query above is through SQLalchemy\n    # engine = create_engine(\n    #     url = f\"databricks://token:{token}@{hostname}?\" +\n    #           f\"http_path={http_path}&amp;catalog={catalog}&amp;schema={schema}\"\n    # )\n    #\n    # with engine.connect() as conn:\n    #     # This will read the contents of `main.test.some_table`\n    #     df_sql = pd.read_sql(f\"SELECT *, _metadata FROM {catalog}.{schema}.{table}\", conn)\n\n\n    # Gather column names from query\n    column_names = [desc[0] for desc in cursor.description]\n    data = cursor.fetchall()\n\n    # Convert to pandas dataframe\n    df = pd.DataFrame(\n        data,\n        columns=column_names\n    )\n\n    return df\n</code></pre>"},{"location":"reference/databricks/#msk_cdm.databricks.databricks_api.DatabricksAPI.read_db_obj","title":"<code>read_db_obj(volume_path, sep='\\t')</code>","text":"<p>Read object from Databricks volume Reads a CSV/TSV file from the Databricks volume and converts it into a pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>volume_path</code> <code>str</code> <p>The path to the file on the Databricks volume.</p> required <code>sep</code> <code>Optional[str]</code> <p>The separator used in the file.</p> <code>'\\t'</code> <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>A DataFrame containing the data from the file.</p> Source code in <code>msk_cdm/databricks/databricks_api.py</code> <pre><code>def read_db_obj(\n        self,\n        volume_path: str,\n        sep: Optional[str] ='\\t'\n) -&gt; pd.DataFrame:\n\"\"\"Read object from Databricks volume\n    Reads a CSV/TSV file from the Databricks volume and converts it into a\n    pandas DataFrame.\n\n    Args:\n        volume_path: The path to the file on the Databricks volume.\n        sep: The separator used in the file.\n\n    Returns:\n        df: A DataFrame containing the data from the file.\n    \"\"\"\n    # Read csv/tsv file from volume and convert into Pandas dataframe\n    response = self._workspace_client.files.download(volume_path)\n    data_str = BytesIO(response.contents.read())\n    df = pd.read_csv(data_str, sep=sep)\n\n    return df\n</code></pre>"},{"location":"reference/databricks/#msk_cdm.databricks.databricks_api.DatabricksAPI.write_db_obj","title":"<code>write_db_obj(df, volume_path, sep='\\t', overwrite=True, dict_database_table_info=None)</code>","text":"<p>Write data to Databricks volume Writes a pandas DataFrame to a CSV file on the Databricks volume. Optionally, creates a table in Databricks from the CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The DataFrame to be written to the file.</p> required <code>volume_path</code> <code>str</code> <p>The path where the file should be saved on the Databricks volume.</p> required <code>sep</code> <code>Optional[str]</code> <p>The separator used in the file.</p> <code>'\\t'</code> <code>overwrite</code> <code>Optional[bool]</code> <p>Whether to overwrite the existing file.</p> <code>True</code> <code>dict_database_table_info</code> <code>Optional[dict]</code> <p>A dictionary containing information about the                       database table. If <code>dict_database_table_info</code> is used, it must contain these keys</p> <pre><code>                    - catalog: Databricks catalog used\n                    - schema: Schema within the catalog\n                    - table: Table in the schema that will contain the dataframe information\n                    - volume_path: Path location on the volume of the object. A csv file for use of this\n                    - sep: File separator used for the object. Typically, comma or tab separated\n</code></pre> <code>None</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>msk_cdm/databricks/databricks_api.py</code> <pre><code>def write_db_obj(\n        self,\n        df: pd.DataFrame,\n        volume_path: str,\n        sep: Optional[str] = '\\t',\n        overwrite: Optional[bool] = True,\n        dict_database_table_info: Optional[dict] = None\n):\n\"\"\"Write data to Databricks volume\n    Writes a pandas DataFrame to a CSV file on the Databricks volume. Optionally,\n    creates a table in Databricks from the CSV file.\n\n    Args:\n        df: The DataFrame to be written to the file.\n        volume_path: The path where the file should be saved on the Databricks volume.\n        sep: The separator used in the file.\n        overwrite: Whether to overwrite the existing file.\n        dict_database_table_info: A dictionary containing information about the\n                                  database table. If `dict_database_table_info` is used, it must contain these keys\n\n                                    - catalog: Databricks catalog used\n                                    - schema: Schema within the catalog\n                                    - table: Table in the schema that will contain the dataframe information\n                                    - volume_path: Path location on the volume of the object. A csv file for use of this\n                                    - sep: File separator used for the object. Typically, comma or tab separated\n\n    Returns:\n        None\n\n\n    \"\"\"\n    # Create directory on volume for data to be uploaded. If directory, exists, nothing will happen to existing data\n    dir_volume_path = os.path.dirname(volume_path)\n    self.create_directory_on_volume(path=dir_volume_path)\n\n    csv_bytes = df.to_csv(index=False, sep=sep).encode(\"utf-8\")\n    csv_buffer = BytesIO(csv_bytes)\n\n    print('Writing to %s' % volume_path)\n    self._workspace_client.files.upload(\n        volume_path,\n        csv_buffer,\n        overwrite=overwrite\n    )\n    print('Write to volume complete')\n\n    if dict_database_table_info is not None:\n        if sep != dict_database_table_info.get('sep'):\n            dict_database_table_info['sep'] = sep\n            print(\"Conflict with separator in dict; setting to value object was saved as.\")\n\n        self.create_table_from_volume(dict_database_table_info=dict_database_table_info)\n\n    return None\n</code></pre>"},{"location":"reference/dremio/","title":"msk_cdm.dremio","text":""},{"location":"reference/dremio/#msk_cdm.dremio._dremio_api.DremioAPI","title":"<code>DremioAPI</code>","text":"<p>               Bases: <code>object</code></p> <p>Object to simplify reading from Dremio (CDSI's SQL engine).</p> Source code in <code>msk_cdm/dremio/_dremio_api.py</code> <pre><code>class DremioAPI(object):\n\"\"\"Object to simplify reading from Dremio (CDSI's SQL engine).\"\"\"\n\n    def __init__(\n        self,\n        *,\n        fname_env: str,\n        env_key_user: Optional[str] = \"USER\",\n        env_key_pw: Optional[str] = \"PW\",\n        scheme: Optional[str] = \"grpc+tcp\",\n        hostname: Optional[str] = \"tlvidreamcord1\",\n        flightport: Optional[int] = 32010,\n    ):\n\"\"\"Initialization\n\n        Args:\n            fname_env: Environment file with username and pw\n            env_key_user: Key term to identify the username in fname_env\n            env_key_pw: Key term to identify the password in fname_env\n            scheme: The connection scheme used\n            hostname: Server hostname\n            flightport: Port number\n\n        \"\"\"\n\n        self._df = None\n        self._scheme = scheme\n        self._hostname = hostname\n        self._flightport = flightport\n\n        load_dotenv(fname_env)\n        self._authenticate(user=os.getenv(env_key_user), pw=os.getenv(env_key_pw))\n\n    def return_data(self):\n\"\"\"Return data queried from Dremio in a Pandas dataframe\n\n        Returns:\n            df\n\n        \"\"\"\n        df = self._df\n\n        return df\n\n    def _authenticate(self, user, pw):\n        scheme = self._scheme\n        hostname = self._hostname\n        flightport = self._flightport\n        connection_args = {}\n        # Two WLM settings can be provided upon initial authentication\n        # with the Dremio Server Flight Endpoint:\n        # - routing-tag\n        # - routing queue\n        initial_options = flight.FlightCallOptions(\n            headers=[\n                (b\"routing-tag\", b\"test-routing-tag\"),\n                (b\"routing-queue\", b\"Low Cost User Queries\"),\n            ]\n        )\n        client_auth_middleware = DremioClientAuthMiddlewareFactory()\n        client = flight.FlightClient(\n            f\"{scheme}://{hostname}:{flightport}\",\n            middleware=[client_auth_middleware],\n            **connection_args,\n        )\n        bearer_token = client.authenticate_basic_token(user, pw, initial_options)\n        print(\"[INFO] Authentication was successful\")\n\n        self._client = client\n        self._bearer_token = bearer_token\n\n    def query_data(self, sql):\n\"\"\"Query Dremio with SQL string\n\n        Args:\n            sql: SQL string used to query Dremio\n\n        Returns:\n            df_output\n\n        \"\"\"\n        client = self._client\n        bearer_token = self._bearer_token\n        # Get table from our dicom segments\n        flight_desc = flight.FlightDescriptor.for_command(sql)\n        options = flight.FlightCallOptions(headers=[bearer_token])\n        schema = client.get_schema(flight_desc, options)\n\n        flight_info = client.get_flight_info(\n            flight.FlightDescriptor.for_command(sql), options\n        )\n        reader = client.do_get(flight_info.endpoints[0].ticket, options)\n\n        df_output = reader.read_pandas()\n\n        self._df = df_output\n\n        return df_output\n</code></pre>"},{"location":"reference/dremio/#msk_cdm.dremio._dremio_api.DremioAPI.__init__","title":"<code>__init__(*, fname_env, env_key_user='USER', env_key_pw='PW', scheme='grpc+tcp', hostname='tlvidreamcord1', flightport=32010)</code>","text":"<p>Initialization</p> <p>Parameters:</p> Name Type Description Default <code>fname_env</code> <code>str</code> <p>Environment file with username and pw</p> required <code>env_key_user</code> <code>Optional[str]</code> <p>Key term to identify the username in fname_env</p> <code>'USER'</code> <code>env_key_pw</code> <code>Optional[str]</code> <p>Key term to identify the password in fname_env</p> <code>'PW'</code> <code>scheme</code> <code>Optional[str]</code> <p>The connection scheme used</p> <code>'grpc+tcp'</code> <code>hostname</code> <code>Optional[str]</code> <p>Server hostname</p> <code>'tlvidreamcord1'</code> <code>flightport</code> <code>Optional[int]</code> <p>Port number</p> <code>32010</code> Source code in <code>msk_cdm/dremio/_dremio_api.py</code> <pre><code>def __init__(\n    self,\n    *,\n    fname_env: str,\n    env_key_user: Optional[str] = \"USER\",\n    env_key_pw: Optional[str] = \"PW\",\n    scheme: Optional[str] = \"grpc+tcp\",\n    hostname: Optional[str] = \"tlvidreamcord1\",\n    flightport: Optional[int] = 32010,\n):\n\"\"\"Initialization\n\n    Args:\n        fname_env: Environment file with username and pw\n        env_key_user: Key term to identify the username in fname_env\n        env_key_pw: Key term to identify the password in fname_env\n        scheme: The connection scheme used\n        hostname: Server hostname\n        flightport: Port number\n\n    \"\"\"\n\n    self._df = None\n    self._scheme = scheme\n    self._hostname = hostname\n    self._flightport = flightport\n\n    load_dotenv(fname_env)\n    self._authenticate(user=os.getenv(env_key_user), pw=os.getenv(env_key_pw))\n</code></pre>"},{"location":"reference/dremio/#msk_cdm.dremio._dremio_api.DremioAPI.query_data","title":"<code>query_data(sql)</code>","text":"<p>Query Dremio with SQL string</p> <p>Parameters:</p> Name Type Description Default <code>sql</code> <p>SQL string used to query Dremio</p> required <p>Returns:</p> Type Description <p>df_output</p> Source code in <code>msk_cdm/dremio/_dremio_api.py</code> <pre><code>def query_data(self, sql):\n\"\"\"Query Dremio with SQL string\n\n    Args:\n        sql: SQL string used to query Dremio\n\n    Returns:\n        df_output\n\n    \"\"\"\n    client = self._client\n    bearer_token = self._bearer_token\n    # Get table from our dicom segments\n    flight_desc = flight.FlightDescriptor.for_command(sql)\n    options = flight.FlightCallOptions(headers=[bearer_token])\n    schema = client.get_schema(flight_desc, options)\n\n    flight_info = client.get_flight_info(\n        flight.FlightDescriptor.for_command(sql), options\n    )\n    reader = client.do_get(flight_info.endpoints[0].ticket, options)\n\n    df_output = reader.read_pandas()\n\n    self._df = df_output\n\n    return df_output\n</code></pre>"},{"location":"reference/dremio/#msk_cdm.dremio._dremio_api.DremioAPI.return_data","title":"<code>return_data()</code>","text":"<p>Return data queried from Dremio in a Pandas dataframe</p> <p>Returns:</p> Type Description <p>df</p> Source code in <code>msk_cdm/dremio/_dremio_api.py</code> <pre><code>def return_data(self):\n\"\"\"Return data queried from Dremio in a Pandas dataframe\n\n    Returns:\n        df\n\n    \"\"\"\n    df = self._df\n\n    return df\n</code></pre>"},{"location":"reference/dremio/#msk_cdm.dremio._dremio_api.DremioClientAuthMiddleware","title":"<code>DremioClientAuthMiddleware</code>","text":"<p>               Bases: <code>ClientMiddleware</code></p> <p>A ClientMiddleware that extracts the bearer token from the authorization header returned by the Dremio Flight Server Endpoint. Parameters</p> <p>factory : ClientHeaderAuthMiddlewareFactory     The factory to set call credentials if an     authorization header with bearer token is     returned by the Dremio server.</p> Source code in <code>msk_cdm/dremio/_dremio_api.py</code> <pre><code>class DremioClientAuthMiddleware(flight.ClientMiddleware):\n\"\"\"\n    A ClientMiddleware that extracts the bearer token from\n    the authorization header returned by the Dremio\n    Flight Server Endpoint.\n    Parameters\n    ----------\n    factory : ClientHeaderAuthMiddlewareFactory\n        The factory to set call credentials if an\n        authorization header with bearer token is\n        returned by the Dremio server.\n    \"\"\"\n\n    def __init__(self, factory):\n        self.factory = factory\n\n    def received_headers(self, headers):\n        auth_header_key = \"authorization\"\n        authorization_header = []\n        for key in headers:\n            if key.lower() == auth_header_key:\n                authorization_header = headers.get(auth_header_key)\n        self.factory.set_call_credential(\n            [b\"authorization\", authorization_header[0].encode(\"utf-8\")]\n        )\n</code></pre>"},{"location":"reference/dremio/#msk_cdm.dremio._dremio_api.DremioClientAuthMiddlewareFactory","title":"<code>DremioClientAuthMiddlewareFactory</code>","text":"<p>               Bases: <code>ClientMiddlewareFactory</code></p> <p>A factory that creates DremioClientAuthMiddleware(s).</p> Source code in <code>msk_cdm/dremio/_dremio_api.py</code> <pre><code>class DremioClientAuthMiddlewareFactory(flight.ClientMiddlewareFactory):\n\"\"\"A factory that creates DremioClientAuthMiddleware(s).\"\"\"\n\n    def __init__(self):\n        self.call_credential = []\n\n    def start_call(self, info):\n        return DremioClientAuthMiddleware(self)\n\n    def set_call_credential(self, call_credential):\n        self.call_credential = call_credential\n</code></pre>"},{"location":"reference/minio/","title":"msk_cdm.minio","text":""},{"location":"reference/minio/#msk_cdm.minio._minio_api.MinioAPI","title":"<code>MinioAPI</code>","text":"<p>               Bases: <code>object</code></p> <p>Object to simplify reading/writing to/from Minio.</p> Source code in <code>msk_cdm/minio/_minio_api.py</code> <pre><code>class MinioAPI(object):\n\"\"\"Object to simplify reading/writing to/from Minio.\"\"\"\n\n    def __init__(\n        self,\n        *,\n        ACCESS_KEY: Optional[str] = None,\n        SECRET_KEY: Optional[str] = None,\n        ca_certs: Optional[str] = None,\n        url_port: Optional[str] = \"pllimsksparky3:9000\", \n        fname_minio_env: Optional[Union[Path, str]] = None,\n        bucket: Optional[str] = None,\n    ):\n\"\"\"Initialization\n\n                Args:\n                    - ACCESS_KEY: Minio access key. Optional if `fname_minio_env` is passed, in which case it may be present in the env file picked up by .env\n                    - SECRET_KEY: Minio secret key. Optional if `fname_minio_env` is passed, in which case it may be present in the env file picked up by .env\n                    - ca_certs: optional filename pointer to ca_cert bundle for `urllib3`. Only specify if not passing `fname_minio_env`.\n                    - fname_minio_env: A filename with KEY=value lines with values for keys `CA_CERTS`, `URL_PORT`, `BUCKET`.\n                    - bucket: optional default minio bucket to use for operations. Can also be specified as environment variable $BUCKET.\n        \"\"\"\n        self._ACCESS_KEY = ACCESS_KEY\n        self._SECRET_KEY = SECRET_KEY\n        self._ca_certs = ca_certs\n        self._url_port = url_port\n\n        self._bucket = bucket\n        self._client = None\n        self._httpClient = None\n\n        if fname_minio_env is not None:\n            self._process_env(fname_minio_env)\n        self._connect()\n\n    def load_obj(\n            self,\n            path_object: str,\n            bucket_name: Optional[str] = None\n    ) -&gt; urllib3.response.HTTPResponse:\n\"\"\"Read an object from minio\n\n        Raises `urllib3.exceptions.HTTPError` if request is unsuccessful.\n\n        Args:\n            path_object: Object file to read from minio.\n            bucket_name: Optional bucket name, otherwise defaults to BUCKET passed\n            via minio env fniame to constructor\n\n        Returns:\n            urllib3.response.HTTPResponse\n\n        \"\"\"\n        if (self._bucket is not None) &amp; (bucket_name is None):\n            bucket_name = self._bucket\n\n        obj = self._client.get_object(bucket_name, path_object)\n\n        return obj\n\n        # try:\n        #     obj = self._client.get_object(bucket_name, path_object)\n        #     if obj.status != 200:\n        #         raise RuntimeError(\n        #             f\"Got non-OK HTTP status {obj.status} requesting \" \"{obj_name}.\"\n        #         )\n        #     return obj\n        #\n        #     # From here, the object can be read in pandas\n        #     # df = pd.read_csv(obj, sep=sep, low_memory=False)\n        #\n        # finally:\n        #     obj.close()\n        #     obj.release_conn()\n\n    def save_obj(\n        self,\n        df,\n        path_object: str,\n        sep: Optional[str] = \",\",\n        bucket_name: Optional[str] = None,\n    ):\n\"\"\"Save an object to minio\n\n        Args:\n            df: Pandas dataframe to be saved to Minio\n            path_object: Object filename for `df`\n            sep: Separator when saving the Pandas dataframe\n            bucket_name: Optional bucket name, otherwise defaults to BUCKET passed\n            via minio env fniame to constructor\n\n        \"\"\"\n\n        if (self._bucket is not None) &amp; (bucket_name is None):\n            bucket_name = self._bucket\n\n        csv_bytes = df.to_csv(index=False, sep=sep).encode(\"utf-8\")\n        csv_buffer = BytesIO(csv_bytes)\n\n        self._client.put_object(\n            bucket_name=bucket_name,\n            object_name=path_object,\n            data=csv_buffer,\n            length=len(csv_bytes),\n            content_type=\"application/csv\",\n        )\n\n        return None\n\n    def load_df(\n            self,\n            fname,\n            sep: Optional[str] = \"\\t\",\n            dtype: Optional[str] = object\n    ):\n        obj = self.load_obj(path_object=fname)\n        df= pd.read_csv(obj, dtype=dtype, sep=sep)\n        return df\n\n    def save_df(\n            self,\n            df,\n            fname,\n            sep: Optional[str] = \"\\t\"\n    ):\n        self.save_obj(\n            df=df,\n            path_object=fname,\n            sep=sep\n        )\n        print(f'Saved data to: {fname}')\n\n    def print_list_objects(\n        self,\n        bucket_name: Optional[str] = None,\n        prefix: Optional[str] = None,\n        recursive: Optional[bool] = True,\n    ):\n\"\"\"Create a Python list of objects in a specified minio bucket\n\n        Args:\n            bucket_name: Optional bucket name, otherwise defaults to  BUCKET passed via minio env fname to constructor\n            prefix: Optional string used to find an object starting with &lt;prefix&gt;\n\n        Returns:\n            obj_list: List of strings containing path locations in minio bucket.\n\n        \"\"\"\n        if (self._bucket is not None) &amp; (bucket_name is None):\n            bucket_name = self._bucket\n\n        objs = self._client.list_objects(\n            bucket_name=bucket_name,\n            recursive=recursive,\n            prefix=prefix\n        )\n        obj_list = []\n        for obj in objs:\n            obj_list.append(obj.object_name)\n\n        return obj_list\n\n    def remove_obj(self, path_object: str, bucket_name: Optional[str] = None):\n\"\"\"Remove an object from minio\n\n        Args:\n            path_object: Object file to be removed from minio\n            bucket_name: Optional bucket name, otherwise defaults to  BUCKET passed via minio env fname to constructor\n\n        \"\"\"\n        # Remove list of objects.\n        self._client.remove_object(bucket_name=bucket_name, object_name=path_object)\n        print(\"Object removed. Bucket: %s, Object: %s\" % (bucket_name, path_object))\n\n        return None\n\n    def copy_obj(\n        self,\n        source_path_object: str,\n        dest_path_object: str,\n        source_bucket: Optional[str] = None,\n        dest_bucket: Optional[str] = None,\n    ):\n\"\"\"Copy an object in minio.\n\n        Objects can be copied across different BUCKETS.\n        Warning: objects with greater than 1GB may fail using this.\n        Instead, use `load_obj` and `save_obj` in combination.\n\n        Args:\n            source_path_object: Object file to be copied\n            dest_path_object: Object filename that `source_path_object` will be copied to\n            bucket_name: Optional bucket name, otherwise defaults to  BUCKET passed\n            via minio env fniame to constructor\n\n        Returns:\n            output: Object name and version ID of object\n        \"\"\"\n        if (self._bucket is not None) &amp; (source_bucket is None):\n            source_bucket = self._bucket\n\n        if (self._bucket is not None) &amp; (dest_bucket is None):\n            dest_bucket = self._bucket\n\n        result = self._client.copy_object(\n            dest_bucket,\n            dest_path_object,\n            CopySource(source_bucket, source_path_object),\n        )\n\n        output = [result.object_name, result.version_id]\n\n        return output\n\n    def _process_env(self, fname_minio_env):\n        print(\"Minio environment file: %s\" % fname_minio_env)\n        dict_config = dotenv_values(fname_minio_env)\n\n        env_access_key = os.getenv(\"ACCESS_KEY\")\n        if env_access_key:\n            dict_config[\"ACCESS_KEY\"] = env_access_key\n\n        env_secret_key = os.getenv(\"SECRET_KEY\")\n        if env_secret_key:\n            dict_config[\"SECRET_KEY\"] = env_secret_key\n\n        if not self._ACCESS_KEY:\n            self._ACCESS_KEY = dict_config.get(\"ACCESS_KEY\", None)\n        if not self._SECRET_KEY:\n            self._SECRET_KEY = dict_config.get(\"SECRET_KEY\", None)\n        if not self._ca_certs:\n            self._ca_certs = dict_config.get(\"CA_CERTS\", None)\n        if not self._url_port:\n            self._url_port = dict_config.get(\"URL_PORT\", None)\n        if not self._bucket:\n            self._bucket = dict_config.get(\"BUCKET\", None)\n\n        return None\n\n    def _connect(self):\n        # required for self-signed certs\n        httpClient = urllib3.PoolManager(\n            cert_reqs=\"CERT_REQUIRED\",\n            ca_certs=self._ca_certs\n        )\n\n        # Create secure client with access key and secret key\n        client = Minio(\n            endpoint=self._url_port,\n            access_key=self._ACCESS_KEY,\n            secret_key=self._SECRET_KEY,\n            secure=True,\n            http_client=httpClient,\n        )\n\n        self._client = client\n        self._httpClient = httpClient\n\n        return None\n</code></pre>"},{"location":"reference/minio/#msk_cdm.minio._minio_api.MinioAPI.__init__","title":"<code>__init__(*, ACCESS_KEY=None, SECRET_KEY=None, ca_certs=None, url_port='pllimsksparky3:9000', fname_minio_env=None, bucket=None)</code>","text":"<p>Initialization</p> <p>Parameters:</p> Name Type Description Default <code>-</code> <code>ACCESS_KEY</code> <p>Minio access key. Optional if <code>fname_minio_env</code> is passed, in which case it may be present in the env file picked up by .env</p> required <code>-</code> <code>SECRET_KEY</code> <p>Minio secret key. Optional if <code>fname_minio_env</code> is passed, in which case it may be present in the env file picked up by .env</p> required <code>-</code> <code>ca_certs</code> <p>optional filename pointer to ca_cert bundle for <code>urllib3</code>. Only specify if not passing <code>fname_minio_env</code>.</p> required <code>-</code> <code>fname_minio_env</code> <p>A filename with KEY=value lines with values for keys <code>CA_CERTS</code>, <code>URL_PORT</code>, <code>BUCKET</code>.</p> required <code>-</code> <code>bucket</code> <p>optional default minio bucket to use for operations. Can also be specified as environment variable $BUCKET.</p> required Source code in <code>msk_cdm/minio/_minio_api.py</code> <pre><code>def __init__(\n    self,\n    *,\n    ACCESS_KEY: Optional[str] = None,\n    SECRET_KEY: Optional[str] = None,\n    ca_certs: Optional[str] = None,\n    url_port: Optional[str] = \"pllimsksparky3:9000\", \n    fname_minio_env: Optional[Union[Path, str]] = None,\n    bucket: Optional[str] = None,\n):\n\"\"\"Initialization\n\n            Args:\n                - ACCESS_KEY: Minio access key. Optional if `fname_minio_env` is passed, in which case it may be present in the env file picked up by .env\n                - SECRET_KEY: Minio secret key. Optional if `fname_minio_env` is passed, in which case it may be present in the env file picked up by .env\n                - ca_certs: optional filename pointer to ca_cert bundle for `urllib3`. Only specify if not passing `fname_minio_env`.\n                - fname_minio_env: A filename with KEY=value lines with values for keys `CA_CERTS`, `URL_PORT`, `BUCKET`.\n                - bucket: optional default minio bucket to use for operations. Can also be specified as environment variable $BUCKET.\n    \"\"\"\n    self._ACCESS_KEY = ACCESS_KEY\n    self._SECRET_KEY = SECRET_KEY\n    self._ca_certs = ca_certs\n    self._url_port = url_port\n\n    self._bucket = bucket\n    self._client = None\n    self._httpClient = None\n\n    if fname_minio_env is not None:\n        self._process_env(fname_minio_env)\n    self._connect()\n</code></pre>"},{"location":"reference/minio/#msk_cdm.minio._minio_api.MinioAPI.copy_obj","title":"<code>copy_obj(source_path_object, dest_path_object, source_bucket=None, dest_bucket=None)</code>","text":"<p>Copy an object in minio.</p> <p>Objects can be copied across different BUCKETS. Warning: objects with greater than 1GB may fail using this. Instead, use <code>load_obj</code> and <code>save_obj</code> in combination.</p> <p>Parameters:</p> Name Type Description Default <code>source_path_object</code> <code>str</code> <p>Object file to be copied</p> required <code>dest_path_object</code> <code>str</code> <p>Object filename that <code>source_path_object</code> will be copied to</p> required <code>bucket_name</code> <p>Optional bucket name, otherwise defaults to  BUCKET passed</p> required <p>Returns:</p> Name Type Description <code>output</code> <p>Object name and version ID of object</p> Source code in <code>msk_cdm/minio/_minio_api.py</code> <pre><code>def copy_obj(\n    self,\n    source_path_object: str,\n    dest_path_object: str,\n    source_bucket: Optional[str] = None,\n    dest_bucket: Optional[str] = None,\n):\n\"\"\"Copy an object in minio.\n\n    Objects can be copied across different BUCKETS.\n    Warning: objects with greater than 1GB may fail using this.\n    Instead, use `load_obj` and `save_obj` in combination.\n\n    Args:\n        source_path_object: Object file to be copied\n        dest_path_object: Object filename that `source_path_object` will be copied to\n        bucket_name: Optional bucket name, otherwise defaults to  BUCKET passed\n        via minio env fniame to constructor\n\n    Returns:\n        output: Object name and version ID of object\n    \"\"\"\n    if (self._bucket is not None) &amp; (source_bucket is None):\n        source_bucket = self._bucket\n\n    if (self._bucket is not None) &amp; (dest_bucket is None):\n        dest_bucket = self._bucket\n\n    result = self._client.copy_object(\n        dest_bucket,\n        dest_path_object,\n        CopySource(source_bucket, source_path_object),\n    )\n\n    output = [result.object_name, result.version_id]\n\n    return output\n</code></pre>"},{"location":"reference/minio/#msk_cdm.minio._minio_api.MinioAPI.load_obj","title":"<code>load_obj(path_object, bucket_name=None)</code>","text":"<p>Read an object from minio</p> <p>Raises <code>urllib3.exceptions.HTTPError</code> if request is unsuccessful.</p> <p>Parameters:</p> Name Type Description Default <code>path_object</code> <code>str</code> <p>Object file to read from minio.</p> required <code>bucket_name</code> <code>Optional[str]</code> <p>Optional bucket name, otherwise defaults to BUCKET passed</p> <code>None</code> <p>Returns:</p> Type Description <code>HTTPResponse</code> <p>urllib3.response.HTTPResponse</p> Source code in <code>msk_cdm/minio/_minio_api.py</code> <pre><code>def load_obj(\n        self,\n        path_object: str,\n        bucket_name: Optional[str] = None\n) -&gt; urllib3.response.HTTPResponse:\n\"\"\"Read an object from minio\n\n    Raises `urllib3.exceptions.HTTPError` if request is unsuccessful.\n\n    Args:\n        path_object: Object file to read from minio.\n        bucket_name: Optional bucket name, otherwise defaults to BUCKET passed\n        via minio env fniame to constructor\n\n    Returns:\n        urllib3.response.HTTPResponse\n\n    \"\"\"\n    if (self._bucket is not None) &amp; (bucket_name is None):\n        bucket_name = self._bucket\n\n    obj = self._client.get_object(bucket_name, path_object)\n\n    return obj\n</code></pre>"},{"location":"reference/minio/#msk_cdm.minio._minio_api.MinioAPI.print_list_objects","title":"<code>print_list_objects(bucket_name=None, prefix=None, recursive=True)</code>","text":"<p>Create a Python list of objects in a specified minio bucket</p> <p>Parameters:</p> Name Type Description Default <code>bucket_name</code> <code>Optional[str]</code> <p>Optional bucket name, otherwise defaults to  BUCKET passed via minio env fname to constructor</p> <code>None</code> <code>prefix</code> <code>Optional[str]</code> <p>Optional string used to find an object starting with  <code>None</code> <p>Returns:</p> Name Type Description <code>obj_list</code> <p>List of strings containing path locations in minio bucket.</p> Source code in <code>msk_cdm/minio/_minio_api.py</code> <pre><code>def print_list_objects(\n    self,\n    bucket_name: Optional[str] = None,\n    prefix: Optional[str] = None,\n    recursive: Optional[bool] = True,\n):\n\"\"\"Create a Python list of objects in a specified minio bucket\n\n    Args:\n        bucket_name: Optional bucket name, otherwise defaults to  BUCKET passed via minio env fname to constructor\n        prefix: Optional string used to find an object starting with &lt;prefix&gt;\n\n    Returns:\n        obj_list: List of strings containing path locations in minio bucket.\n\n    \"\"\"\n    if (self._bucket is not None) &amp; (bucket_name is None):\n        bucket_name = self._bucket\n\n    objs = self._client.list_objects(\n        bucket_name=bucket_name,\n        recursive=recursive,\n        prefix=prefix\n    )\n    obj_list = []\n    for obj in objs:\n        obj_list.append(obj.object_name)\n\n    return obj_list\n</code></pre>"},{"location":"reference/minio/#msk_cdm.minio._minio_api.MinioAPI.remove_obj","title":"<code>remove_obj(path_object, bucket_name=None)</code>","text":"<p>Remove an object from minio</p> <p>Parameters:</p> Name Type Description Default <code>path_object</code> <code>str</code> <p>Object file to be removed from minio</p> required <code>bucket_name</code> <code>Optional[str]</code> <p>Optional bucket name, otherwise defaults to  BUCKET passed via minio env fname to constructor</p> <code>None</code> Source code in <code>msk_cdm/minio/_minio_api.py</code> <pre><code>def remove_obj(self, path_object: str, bucket_name: Optional[str] = None):\n\"\"\"Remove an object from minio\n\n    Args:\n        path_object: Object file to be removed from minio\n        bucket_name: Optional bucket name, otherwise defaults to  BUCKET passed via minio env fname to constructor\n\n    \"\"\"\n    # Remove list of objects.\n    self._client.remove_object(bucket_name=bucket_name, object_name=path_object)\n    print(\"Object removed. Bucket: %s, Object: %s\" % (bucket_name, path_object))\n\n    return None\n</code></pre>"},{"location":"reference/minio/#msk_cdm.minio._minio_api.MinioAPI.save_obj","title":"<code>save_obj(df, path_object, sep=',', bucket_name=None)</code>","text":"<p>Save an object to minio</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <p>Pandas dataframe to be saved to Minio</p> required <code>path_object</code> <code>str</code> <p>Object filename for <code>df</code></p> required <code>sep</code> <code>Optional[str]</code> <p>Separator when saving the Pandas dataframe</p> <code>','</code> <code>bucket_name</code> <code>Optional[str]</code> <p>Optional bucket name, otherwise defaults to BUCKET passed</p> <code>None</code> Source code in <code>msk_cdm/minio/_minio_api.py</code> <pre><code>def save_obj(\n    self,\n    df,\n    path_object: str,\n    sep: Optional[str] = \",\",\n    bucket_name: Optional[str] = None,\n):\n\"\"\"Save an object to minio\n\n    Args:\n        df: Pandas dataframe to be saved to Minio\n        path_object: Object filename for `df`\n        sep: Separator when saving the Pandas dataframe\n        bucket_name: Optional bucket name, otherwise defaults to BUCKET passed\n        via minio env fniame to constructor\n\n    \"\"\"\n\n    if (self._bucket is not None) &amp; (bucket_name is None):\n        bucket_name = self._bucket\n\n    csv_bytes = df.to_csv(index=False, sep=sep).encode(\"utf-8\")\n    csv_buffer = BytesIO(csv_bytes)\n\n    self._client.put_object(\n        bucket_name=bucket_name,\n        object_name=path_object,\n        data=csv_buffer,\n        length=len(csv_bytes),\n        content_type=\"application/csv\",\n    )\n\n    return None\n</code></pre>"},{"location":"reference/datasets/datasets/","title":"msk_cdm.datasets","text":"<p>Utilities to load various clinical datasets related to the MSK-IMPACT cohort (de-identified and PHI versions), and datasets derived from IDB queries.</p>"},{"location":"reference/datasets/datasets/#msk-impact-datasets-de-identified","title":"MSK-IMPACT Datasets (De-identified)","text":"Function Name Description <code>load_impact_data_clinical_patient</code> Load the clinical patient summary data from the IMPACT dataset. <code>load_impact_data_clinical_sample</code> Load the clinical sample summary data from the IMPACT dataset. <code>load_impact_data_timeline_surgery</code> Load the surgical timeline data from the IMPACT dataset. <code>load_impact_data_timeline_radiation</code> Load the radiation therapy timeline data from the IMPACT dataset. <code>load_impact_data_timeline_treatment</code> Load the treatment timeline data from the IMPACT dataset. <code>load_impact_data_timeline_diagnosis</code> Load the diagnosis timeline data from the IMPACT dataset. <code>load_impact_data_timeline_specimen</code> Load the specimen timeline data from the IMPACT dataset. <code>load_impact_data_timeline_specimen_surgery</code> Load the specimen surgery timeline data from the IMPACT dataset. <code>load_impact_data_timeline_gleason</code> Load the Gleason score timeline data from the IMPACT dataset. <code>load_impact_data_timeline_pdl1</code> Load the PD-L1 timeline data from the IMPACT dataset. <code>load_impact_data_timeline_mmr</code> Load the MMR timeline data from the IMPACT dataset. <code>load_impact_data_timeline_prior_meds</code> Load the prior medications timeline data from the IMPACT dataset. <code>load_impact_data_timeline_tumor_sites</code> Load the tumor sites timeline data from the IMPACT dataset. <code>load_impact_data_timeline_follow_up</code> Load the follow-up timeline data from the IMPACT dataset. <code>load_impact_data_timeline_progression</code> Load the progression timeline data from the IMPACT dataset. <code>load_impact_data_timeline_cancer_presence</code> Load the cancer presence timeline data from the IMPACT dataset. <code>load_impact_data_timeline_ecog_kps</code> Load the ECOG-KPS timeline data from the IMPACT dataset."},{"location":"reference/datasets/datasets/#msk-impact-datasets-contains-phi","title":"MSK-IMPACT Datasets (Contains PHI)","text":"Function Name Description <code>load_phi_impact_data_timeline_surgery</code> Load the surgical timeline data in the PHI IMPACT format. <code>load_phi_impact_data_timeline_radiation</code> Load the radiation therapy timeline data in the PHI IMPACT format. <code>load_phi_impact_data_timeline_treatment</code> Load the treatment timeline data in the PHI IMPACT format. <code>load_phi_impact_data_timeline_diagnosis</code> Load the diagnosis timeline data in the PHI IMPACT format. <code>load_phi_impact_data_timeline_specimen</code> Load the specimen timeline data in the PHI IMPACT format. <code>load_phi_impact_data_timeline_specimen_surgery</code> Load the specimen surgery timeline data in the PHI IMPACT format. <code>load_phi_impact_data_timeline_gleason</code> Load the Gleason score timeline data in the PHI IMPACT format. <code>load_phi_impact_data_timeline_pdl1</code> Load the PD-L1 timeline data in the PHI IMPACT format. <code>load_phi_impact_data_timeline_prior_meds</code> Load the prior medications timeline data in the PHI IMPACT format. <code>load_phi_impact_data_timeline_tumor_sites</code> Load the tumor sites timeline data in the PHI IMPACT format. <code>load_phi_impact_data_timeline_follow_up</code> Load the follow-up timeline data in the PHI IMPACT format. <code>load_phi_impact_data_timeline_progression</code> Load the progression timeline data in the PHI IMPACT format. <code>load_phi_impact_data_timeline_mmr</code> Load the MMR timeline data in the PHI IMPACT format. <code>load_phi_impact_data_timeline_cancer_presence</code> Load the cancer presence timeline data in the PHI IMPACT format. <code>load_phi_impact_data_timeline_ecog_kps</code> Load the ECOG-KPS timeline data in the PHI IMPACT format. <code>load_phi_impact_id_mapping</code> Load the ID mapping data in the PHI IMPACT format. <code>load_phi_impact_anchor_dates</code> Load the anchor dates data in the PHI IMPACT format."},{"location":"reference/datasets/datasets/#datasets-from-idb-queries","title":"Datasets from IDB Queries","text":"Function Name Description <code>load_phi_idb_demographics</code> Load the demographics data from IDB queries. <code>load_phi_idb_radiology_reports</code> Load the radiology reports data from IDB queries. <code>load_phi_idb_pathology_reports</code> Load the pathology reports data from IDB queries. <code>load_phi_idb_surgeries</code> Load the surgeries data from IDB queries. <code>load_phi_idb_diagnosis</code> Load the diagnosis data from IDB queries. <code>load_phi_idb_medications</code> Load the medications data from IDB queries. <code>load_phi_idb_radiation</code> Load the radiation data from IDB queries. <code>load_phi_idb_interventional_radiology</code> Load the interventional radiology data from IDB queries."},{"location":"reference/datasets/impact/load_data_clinical_patient/","title":"<code>load_data_clinical_patient</code>","text":"<p>Load and return the MSK-IMPACT clinical patient dataset (deidentified).</p> <p>Returns:</p> Name Type Description <code>data</code> <code>Bunch</code> <p>Dictionary-like object, with the following attributes.</p> <ul> <li>data : pandas DataFrame     The data matrix.</li> <li>description_columns : list     The names of the dataset columns. (Future release)</li> <li>description_dataset : str     The full description of the dataset. (Future release)</li> <li>filename : str     The path to the location of the data. (Future release)</li> </ul>"},{"location":"reference/datasets/impact/load_data_clinical_patient/#msk_cdm.datasets.impact.datasets_impact.load_data_clinical_patient--examples","title":"Examples","text":"<pre><code>from msk_cdm.datasets import connect_to_db\nfrom msk_cdm.datasets.impact import load_data_clinical_patient\n\n# Connect to the database\nauth_file = 'path/to/config.txt'\nconnect_to_db(auth_file=auth_file)\n\n# Load the dataset\ndf_clinical_patient = load_data_clinical_patient()\n\n# Access the data\ndf_clin_p = df_clinical_patient['data']\n\n# Display the first few rows of the data\nprint(df_clin_p.head())\n</code></pre> Source code in <code>msk_cdm/datasets/impact/datasets_impact.py</code> <pre><code>def load_data_clinical_patient() -&gt; Bunch:\n\"\"\"Load and return the MSK-IMPACT clinical patient dataset (deidentified).\n\n    Returns:\n        data : Dictionary-like object, with the following attributes.\n\n            - **data** : pandas DataFrame\n                The data matrix.\n            - **description_columns** : list\n                The names of the dataset columns. (Future release)\n            - **description_dataset** : str\n                The full description of the dataset. (Future release)\n            - **filename** : str\n                The path to the location of the data. (Future release)\n\n    Examples\n    --------\n    ```python\n    from msk_cdm.datasets import connect_to_db\n    from msk_cdm.datasets.impact import load_data_clinical_patient\n\n    # Connect to the database\n    auth_file = 'path/to/config.txt'\n    connect_to_db(auth_file=auth_file)\n\n    # Load the dataset\n    df_clinical_patient = load_data_clinical_patient()\n\n    # Access the data\n    df_clin_p = df_clinical_patient['data']\n\n    # Display the first few rows of the data\n    print(df_clin_p.head())\n    ```\n    \"\"\"\n    df = _loader._load_impact_data_clinical_patient()\n    output = Bunch(data=df)\n    return output\n</code></pre>"},{"location":"reference/datasets/impact/load_data_clinical_sample/","title":"<code>load_data_clinical_sample</code>","text":"<p>Load and return the MSK-IMPACT clinical sample dataset (deidentified).</p> <p>Returns:</p> Name Type Description <code>data</code> <code>Bunch</code> <p>Dictionary-like object, with the following attributes.</p> <ul> <li>data : pandas DataFrame     The data matrix.</li> <li>description_columns (Future release) : list     The names of the dataset columns.</li> <li>description_dataset (Future release) : str     The full description of the dataset.</li> <li>filename (Future release) : str     The path to the location of the data.</li> </ul>"},{"location":"reference/datasets/impact/load_data_clinical_sample/#msk_cdm.datasets.impact.datasets_impact.load_data_clinical_sample--examples","title":"Examples","text":"<pre><code>from msk_cdm.datasets import connect_to_db\nfrom msk_cdm.datasets.impact import load_data_clinical_sample\n\n# Connect to the database\nauth_file = 'path/to/config.txt'\nconnect_to_db(auth_file=auth_file)\n\n# Load the dataset\ndf_clinical_sample = load_data_clinical_sample()\n\n# Access the data\ndf_clin_s = df_clinical_sample['data']\n\n# Display the first few rows of the data\nprint(df_clin_s.head())\n</code></pre> Source code in <code>msk_cdm/datasets/impact/datasets_impact.py</code> <pre><code>def load_data_clinical_sample() -&gt; Bunch:\n\"\"\"Load and return the MSK-IMPACT clinical sample dataset (deidentified).\n\n    Returns:\n        data : Dictionary-like object, with the following attributes.\n\n            - **data** : pandas DataFrame\n                The data matrix.\n            - **description_columns** (Future release) : list\n                The names of the dataset columns.\n            - **description_dataset** (Future release) : str\n                The full description of the dataset.\n            - **filename** (Future release) : str\n                The path to the location of the data.\n\n    Examples\n    -\n    ```python\n    from msk_cdm.datasets import connect_to_db\n    from msk_cdm.datasets.impact import load_data_clinical_sample\n\n    # Connect to the database\n    auth_file = 'path/to/config.txt'\n    connect_to_db(auth_file=auth_file)\n\n    # Load the dataset\n    df_clinical_sample = load_data_clinical_sample()\n\n    # Access the data\n    df_clin_s = df_clinical_sample['data']\n\n    # Display the first few rows of the data\n    print(df_clin_s.head())\n    ```\n    \"\"\"\n    df = _loader._load_impact_data_clinical_sample()\n    output = Bunch(data=df)\n    return output\n</code></pre>"},{"location":"reference/datasets/impact/load_data_timeline_cancer_presence/","title":"<code>load_data_timeline_cancer_presence</code>","text":"<p>Load and return the MSK-IMPACT cancer presence timeline dataset (deidentified).</p> <p>Returns:</p> Name Type Description <code>data</code> <code>Bunch</code> <p>Dictionary-like object, with the following attributes.</p> <ul> <li>data : pandas DataFrame     The data matrix.</li> <li>description_columns (Future release) : list     The names of the dataset columns.</li> <li>description_dataset (Future release) : str     The full description of the dataset.</li> <li>filename (Future release) : str     The path to the location of the data.</li> </ul>"},{"location":"reference/datasets/impact/load_data_timeline_cancer_presence/#msk_cdm.datasets.impact.datasets_impact.load_data_timeline_cancer_presence--examples","title":"Examples","text":"<p>```python from msk_cdm.datasets import connect_to_db from msk_cdm.datasets.impact import load_data_timeline_cancer_presence</p>"},{"location":"reference/datasets/impact/load_data_timeline_cancer_presence/#msk_cdm.datasets.impact.datasets_impact.load_data_timeline_cancer_presence--connect-to-the-database","title":"Connect to the database","text":"<p>auth_file = 'path/to/config.txt' connect_to_db(auth_file=auth_file)</p>"},{"location":"reference/datasets/impact/load_data_timeline_cancer_presence/#msk_cdm.datasets.impact.datasets_impact.load_data_timeline_cancer_presence--load-the-dataset","title":"Load the dataset","text":"<p>df_timeline_cancer_presence = load_data_timeline_cancer_presence()</p>"},{"location":"reference/datasets/impact/load_data_timeline_cancer_presence/#msk_cdm.datasets.impact.datasets_impact.load_data_timeline_cancer_presence--access-the-data","title":"Access the data","text":"<p>df_cancer_presence = df_timeline_cancer_presence['data']</p>"},{"location":"reference/datasets/impact/load_data_timeline_cancer_presence/#msk_cdm.datasets.impact.datasets_impact.load_data_timeline_cancer_presence--display-the-first-few-rows-of-the-data","title":"Display the first few rows of the data","text":"<p>print(df_cancer_presence.head())</p> Source code in <code>msk_cdm/datasets/impact/datasets_impact.py</code> <pre><code>def load_data_timeline_cancer_presence() -&gt; Bunch:\n\"\"\"Load and return the MSK-IMPACT cancer presence timeline dataset (deidentified).\n\n    Returns:\n        data: Dictionary-like object, with the following attributes.\n\n            - **data** : pandas DataFrame\n                The data matrix.\n            - **description_columns** (Future release) : list\n                The names of the dataset columns.\n            - **description_dataset** (Future release) : str\n                The full description of the dataset.\n            - **filename** (Future release) : str\n                The path to the location of the data.\n\n    Examples\n    --------\n    ```python\n    from msk_cdm.datasets import connect_to_db\n    from msk_cdm.datasets.impact import load_data_timeline_cancer_presence\n\n    # Connect to the database\n    auth_file = 'path/to/config.txt'\n    connect_to_db(auth_file=auth_file)\n\n    # Load the dataset\n    df_timeline_cancer_presence = load_data_timeline_cancer_presence()\n\n    # Access the data\n    df_cancer_presence = df_timeline_cancer_presence['data']\n\n    # Display the first few rows of the data\n    print(df_cancer_presence.head())\n    \"\"\"\n    df = _loader._load_impact_data_timeline_cancer_presence()\n    output = Bunch(data=df)\n    return output\n</code></pre>"},{"location":"reference/datasets/impact/load_data_timeline_diagnosis/","title":"<code>load_data_timeline_diagnosis</code>","text":"<p>Load and return the MSK-IMPACT diagnosis timeline dataset (deidentified).</p> <p>Returns:</p> Name Type Description <code>data</code> <code>Bunch</code> <p>Dictionary-like object, with the following attributes.</p> <ul> <li>data : pandas DataFrame     The data matrix.</li> <li>description_columns (Future release) : list     The names of the dataset columns.</li> <li>description_dataset (Future release) : str     The full description of the dataset.</li> <li>filename (Future release) : str     The path to the location of the data.</li> </ul>"},{"location":"reference/datasets/impact/load_data_timeline_diagnosis/#msk_cdm.datasets.impact.datasets_impact.load_data_timeline_diagnosis--examples","title":"Examples","text":"<pre><code>from msk_cdm.datasets import connect_to_db\nfrom msk_cdm.datasets.impact import load_data_timeline_diagnosis\n\n# Connect to the database\nauth_file = 'path/to/config.txt'\nconnect_to_db(auth_file=auth_file)\n\n# Load the dataset\ndf_timeline_diagnosis = load_data_timeline_diagnosis()\n\n# Access the data\ndf_diag = df_timeline_diagnosis['data']\n\n# Display the first few rows of the data\nprint(df_diag.head())\n</code></pre> Source code in <code>msk_cdm/datasets/impact/datasets_impact.py</code> <pre><code>def load_data_timeline_diagnosis() -&gt; Bunch:\n\"\"\"Load and return the MSK-IMPACT diagnosis timeline dataset (deidentified).\n\n    Returns:\n        data : Dictionary-like object, with the following attributes.\n\n            - **data** : pandas DataFrame\n                The data matrix.\n            - **description_columns** (Future release) : list\n                The names of the dataset columns.\n            - **description_dataset** (Future release) : str\n                The full description of the dataset.\n            - **filename** (Future release) : str\n                The path to the location of the data.\n\n    Examples\n    --------\n    ```python\n    from msk_cdm.datasets import connect_to_db\n    from msk_cdm.datasets.impact import load_data_timeline_diagnosis\n\n    # Connect to the database\n    auth_file = 'path/to/config.txt'\n    connect_to_db(auth_file=auth_file)\n\n    # Load the dataset\n    df_timeline_diagnosis = load_data_timeline_diagnosis()\n\n    # Access the data\n    df_diag = df_timeline_diagnosis['data']\n\n    # Display the first few rows of the data\n    print(df_diag.head())\n    ```\n    \"\"\"\n    df = _loader._load_impact_data_timeline_diagnosis()\n    output = Bunch(data=df)\n    return output\n</code></pre>"},{"location":"reference/datasets/impact/load_data_timeline_ecog_kps/","title":"<code>load_data_timeline_ecog_kps</code>","text":"<p>Load and return the MSK-IMPACT ECOG-KPS timeline dataset (deidentified).</p> <p>Returns:</p> Name Type Description <code>data</code> <code>Bunch</code> <p>Dictionary-like object, with the following attributes.</p> <ul> <li>data : pandas DataFrame     The data matrix.</li> <li>description_columns (Future release) : list     The names of the dataset columns.</li> <li>description_dataset (Future release) : str     The full description of the dataset.</li> <li>filename (Future release) : str     The path to the location of the data.</li> </ul>"},{"location":"reference/datasets/impact/load_data_timeline_ecog_kps/#msk_cdm.datasets.impact.datasets_impact.load_data_timeline_ecog_kps--examples","title":"Examples","text":"<pre><code>from msk_cdm.datasets import connect_to_db\nfrom msk_cdm.datasets.impact import load_data_timeline_ecog_kps\n\n# Connect to the database\nauth_file = 'path/to/config.txt'\nconnect_to_db(auth_file=auth_file)\n\n# Load the dataset\ndf_timeline_ecog_kps = load_data_timeline_ecog_kps()\n\n# Access the data\ndf_ecog_kps = df_timeline_ecog_kps['data']\n\n# Display the first few rows of the data\nprint(df_ecog_kps.head())\n</code></pre> Source code in <code>msk_cdm/datasets/impact/datasets_impact.py</code> <pre><code>def load_data_timeline_ecog_kps() -&gt; Bunch:\n\"\"\"Load and return the MSK-IMPACT ECOG-KPS timeline dataset (deidentified).\n\n    Returns:\n        data: Dictionary-like object, with the following attributes.\n\n            - **data** : pandas DataFrame\n                The data matrix.\n            - **description_columns** (Future release) : list\n                The names of the dataset columns.\n            - **description_dataset** (Future release) : str\n                The full description of the dataset.\n            - **filename** (Future release) : str\n                The path to the location of the data.\n\n    Examples\n    --------\n    ```python\n    from msk_cdm.datasets import connect_to_db\n    from msk_cdm.datasets.impact import load_data_timeline_ecog_kps\n\n    # Connect to the database\n    auth_file = 'path/to/config.txt'\n    connect_to_db(auth_file=auth_file)\n\n    # Load the dataset\n    df_timeline_ecog_kps = load_data_timeline_ecog_kps()\n\n    # Access the data\n    df_ecog_kps = df_timeline_ecog_kps['data']\n\n    # Display the first few rows of the data\n    print(df_ecog_kps.head())\n    ```\n    \"\"\"\n    df = _loader._load_impact_data_timeline_ecog_kps()\n    output = Bunch(data=df)\n    return output\n</code></pre>"},{"location":"reference/datasets/impact/load_data_timeline_follow_up/","title":"<code>load_data_timeline_follow_up</code>","text":"<p>Load and return the MSK-IMPACT follow-up timeline dataset (deidentified).</p> <p>Returns:</p> Name Type Description <code>data</code> <code>Bunch</code> <p>Dictionary-like object, with the following attributes.</p> <ul> <li>data : pandas DataFrame     The data matrix.</li> <li>description_columns : list     The names of the dataset columns. (Future release)</li> <li>description_dataset : str     The full description of the dataset. (Future release)</li> <li>filename : str     The path to the location of the data. (Future release)</li> </ul>"},{"location":"reference/datasets/impact/load_data_timeline_follow_up/#msk_cdm.datasets.impact.datasets_impact.load_data_timeline_follow_up--examples","title":"Examples","text":"<pre><code>from msk_cdm.datasets import connect_to_db\nfrom msk_cdm.datasets.impact import load_data_timeline_follow_up\n\n# Connect to the database\nauth_file = 'path/to/config.txt'\nconnect_to_db(auth_file=auth_file)\n\n# Load the dataset\ndf_timeline_follow_up = load_data_timeline_follow_up()\n\n# Access the data\ndf_follow_up = df_timeline_follow_up['data']\n\n# Display the first few rows of the data\nprint(df_follow_up.head())\n</code></pre> Source code in <code>msk_cdm/datasets/impact/datasets_impact.py</code> <pre><code>def load_data_timeline_follow_up() -&gt; Bunch:\n\"\"\"Load and return the MSK-IMPACT follow-up timeline dataset (deidentified).\n\n    Returns:\n        data: Dictionary-like object, with the following attributes.\n\n            - **data** : pandas DataFrame\n                The data matrix.\n            - **description_columns** : list\n                The names of the dataset columns. (Future release)\n            - **description_dataset** : str\n                The full description of the dataset. (Future release)\n            - **filename** : str\n                The path to the location of the data. (Future release)\n\n    Examples\n    --------\n    ```python\n    from msk_cdm.datasets import connect_to_db\n    from msk_cdm.datasets.impact import load_data_timeline_follow_up\n\n    # Connect to the database\n    auth_file = 'path/to/config.txt'\n    connect_to_db(auth_file=auth_file)\n\n    # Load the dataset\n    df_timeline_follow_up = load_data_timeline_follow_up()\n\n    # Access the data\n    df_follow_up = df_timeline_follow_up['data']\n\n    # Display the first few rows of the data\n    print(df_follow_up.head())\n    ```\n    \"\"\"\n    df = _loader._load_impact_data_timeline_follow_up()\n    output = Bunch(data=df)\n    return output\n</code></pre>"},{"location":"reference/datasets/impact/load_data_timeline_gleason/","title":"<code>load_data_timeline_gleason</code>","text":"<p>Load and return the MSK-IMPACT Gleason score timeline dataset (deidentified).</p> <p>Returns:</p> Name Type Description <code>data</code> <code>Bunch</code> <p>Dictionary-like object, with the following attributes.</p> <ul> <li>data : pandas DataFrame     The data matrix.</li> <li>description_columns : list     The names of the dataset columns. (Future release)</li> <li>description_dataset : str     The full description of the dataset. (Future release)</li> <li>filename : str     The path to the location of the data. (Future release)</li> </ul>"},{"location":"reference/datasets/impact/load_data_timeline_gleason/#msk_cdm.datasets.impact.datasets_impact.load_data_timeline_gleason--examples","title":"Examples","text":"<pre><code>from msk_cdm.datasets import connect_to_db\nfrom msk_cdm.datasets.impact import load_data_timeline_gleason\n\n# Connect to the database\nauth_file = 'path/to/config.txt'\nconnect_to_db(auth_file=auth_file)\n\n# Load the dataset\ndf_timeline_gleason = load_data_timeline_gleason()\n\n# Access the data\ndf_gleason = df_timeline_gleason['data']\n\n# Display the first few rows of the data\nprint(df_gleason.head())\n</code></pre> Source code in <code>msk_cdm/datasets/impact/datasets_impact.py</code> <pre><code>def load_data_timeline_gleason() -&gt; Bunch:\n\"\"\"Load and return the MSK-IMPACT Gleason score timeline dataset (deidentified).\n\n    Returns:\n        data: Dictionary-like object, with the following attributes.\n\n            - **data** : pandas DataFrame\n                The data matrix.\n            - **description_columns** : list\n                The names of the dataset columns. (Future release)\n            - **description_dataset** : str\n                The full description of the dataset. (Future release)\n            - **filename** : str\n                The path to the location of the data. (Future release)\n\n    Examples\n    --------\n    ```python\n    from msk_cdm.datasets import connect_to_db\n    from msk_cdm.datasets.impact import load_data_timeline_gleason\n\n    # Connect to the database\n    auth_file = 'path/to/config.txt'\n    connect_to_db(auth_file=auth_file)\n\n    # Load the dataset\n    df_timeline_gleason = load_data_timeline_gleason()\n\n    # Access the data\n    df_gleason = df_timeline_gleason['data']\n\n    # Display the first few rows of the data\n    print(df_gleason.head())\n    ```\n    \"\"\"\n    df = _loader._load_impact_data_timeline_gleason()\n    output = Bunch(data=df)\n    return output\n</code></pre>"},{"location":"reference/datasets/impact/load_data_timeline_mmr/","title":"<code>load_data_timeline_mmr</code>","text":"<p>Load and return the MSK-IMPACT MMR timeline dataset (deidentified).</p> <p>Returns:</p> Name Type Description <code>data</code> <code>Bunch</code> <p>Dictionary-like object, with the following attributes.</p> <ul> <li>data : pandas DataFrame     The data matrix.</li> <li>description_columns : list     The names of the dataset columns. (Future release)</li> <li>description_dataset : str     The full description of the dataset. (Future release)</li> <li>filename : str     The path to the location of the data. (Future release)</li> </ul>"},{"location":"reference/datasets/impact/load_data_timeline_mmr/#msk_cdm.datasets.impact.datasets_impact.load_data_timeline_mmr--examples","title":"Examples","text":"<pre><code>from msk_cdm.datasets import connect_to_db\nfrom msk_cdm.datasets.impact import load_data_timeline_mmr\n\n# Connect to the database\nauth_file = 'path/to/config.txt'\nconnect_to_db(auth_file=auth_file)\n\n# Load the dataset\ndf_timeline_mmr = load_data_timeline_mmr()\n\n# Access the data\ndf_mmr = df_timeline_mmr['data']\n\n# Display the first few rows of the data\nprint(df_mmr.head())\n</code></pre> Source code in <code>msk_cdm/datasets/impact/datasets_impact.py</code> <pre><code>def load_data_timeline_mmr() -&gt; Bunch:\n\"\"\"Load and return the MSK-IMPACT MMR timeline dataset (deidentified).\n\n    Returns:\n        data: Dictionary-like object, with the following attributes.\n\n            - **data** : pandas DataFrame\n                The data matrix.\n            - **description_columns** : list\n                The names of the dataset columns. (Future release)\n            - **description_dataset** : str\n                The full description of the dataset. (Future release)\n            - **filename** : str\n                The path to the location of the data. (Future release)\n\n    Examples\n    --------\n    ```python\n    from msk_cdm.datasets import connect_to_db\n    from msk_cdm.datasets.impact import load_data_timeline_mmr\n\n    # Connect to the database\n    auth_file = 'path/to/config.txt'\n    connect_to_db(auth_file=auth_file)\n\n    # Load the dataset\n    df_timeline_mmr = load_data_timeline_mmr()\n\n    # Access the data\n    df_mmr = df_timeline_mmr['data']\n\n    # Display the first few rows of the data\n    print(df_mmr.head())\n    ```\n    \"\"\"\n    df = _loader._load_impact_data_timeline_mmr()\n    output = Bunch(data=df)\n    return output\n</code></pre>"},{"location":"reference/datasets/impact/load_data_timeline_pdl1/","title":"<code>load_data_timeline_pdl1</code>","text":"<p>Load and return the MSK-IMPACT PD-L1 timeline dataset (deidentified).</p> <p>Returns:</p> Name Type Description <code>data</code> <code>Bunch</code> <p>Dictionary-like object, with the following attributes.</p> <ul> <li>data : pandas DataFrame     The data matrix.</li> <li>description_columns : list     The names of the dataset columns. (Future release)</li> <li>description_dataset : str     The full description of the dataset. (Future release)</li> <li>filename : str     The path to the location of the data. (Future release)</li> </ul>"},{"location":"reference/datasets/impact/load_data_timeline_pdl1/#msk_cdm.datasets.impact.datasets_impact.load_data_timeline_pdl1--examples","title":"Examples","text":"<pre><code>from msk_cdm.datasets import connect_to_db\nfrom msk_cdm.datasets.impact import load_data_timeline_pdl1\n\n# Connect to the database\nauth_file = 'path/to/config.txt'\nconnect_to_db(auth_file=auth_file)\n\n# Load the dataset\ndf_timeline_pdl1 = load_data_timeline_pdl1()\n\n# Access the data\ndf_pdl1 = df_timeline_pdl1['data']\n\n# Display the first few rows of the data\nprint(df_pdl1.head())\n</code></pre> Source code in <code>msk_cdm/datasets/impact/datasets_impact.py</code> <pre><code>def load_data_timeline_pdl1() -&gt; Bunch:\n\"\"\"Load and return the MSK-IMPACT PD-L1 timeline dataset (deidentified).\n\n    Returns:\n        data: Dictionary-like object, with the following attributes.\n\n            - **data** : pandas DataFrame\n                The data matrix.\n            - **description_columns** : list\n                The names of the dataset columns. (Future release)\n            - **description_dataset** : str\n                The full description of the dataset. (Future release)\n            - **filename** : str\n                The path to the location of the data. (Future release)\n\n    Examples\n    --------\n    ```python\n    from msk_cdm.datasets import connect_to_db\n    from msk_cdm.datasets.impact import load_data_timeline_pdl1\n\n    # Connect to the database\n    auth_file = 'path/to/config.txt'\n    connect_to_db(auth_file=auth_file)\n\n    # Load the dataset\n    df_timeline_pdl1 = load_data_timeline_pdl1()\n\n    # Access the data\n    df_pdl1 = df_timeline_pdl1['data']\n\n    # Display the first few rows of the data\n    print(df_pdl1.head())\n    ```\n    \"\"\"\n    df = _loader._load_impact_data_timeline_pdl1()\n    output = Bunch(data=df)\n    return output\n</code></pre>"},{"location":"reference/datasets/impact/load_data_timeline_prior_meds/","title":"<code>load_data_timeline_prior_meds</code>","text":"<p>Load and return the MSK-IMPACT prior medications timeline dataset (deidentified).</p> <p>Returns:</p> Name Type Description <code>data</code> <code>Bunch</code> <p>Dictionary-like object, with the following attributes.</p> <ul> <li>data : pandas DataFrame     The data matrix.</li> <li>description_columns : list     The names of the dataset columns. (Future release)</li> <li>description_dataset : str     The full description of the dataset. (Future release)</li> <li>filename : str     The path to the location of the data. (Future release)</li> </ul>"},{"location":"reference/datasets/impact/load_data_timeline_prior_meds/#msk_cdm.datasets.impact.datasets_impact.load_data_timeline_prior_meds--examples","title":"Examples","text":"<pre><code>from msk_cdm.datasets import connect_to_db\nfrom msk_cdm.datasets.impact import load_data_timeline_prior_meds\n\n# Connect to the database\nauth_file = 'path/to/config.txt'\nconnect_to_db(auth_file=auth_file)\n\n# Load the dataset\ndf_timeline_prior_meds = load_data_timeline_prior_meds()\n\n# Access the data\ndf_prior_meds = df_timeline_prior_meds['data']\n\n# Display the first few rows of the data\nprint(df_prior_meds.head())\n</code></pre> Source code in <code>msk_cdm/datasets/impact/datasets_impact.py</code> <pre><code>def load_data_timeline_prior_meds() -&gt; Bunch:\n\"\"\"Load and return the MSK-IMPACT prior medications timeline dataset (deidentified).\n\n    Returns:\n        data: Dictionary-like object, with the following attributes.\n\n            - **data** : pandas DataFrame\n                The data matrix.\n            - **description_columns** : list\n                The names of the dataset columns. (Future release)\n            - **description_dataset** : str\n                The full description of the dataset. (Future release)\n            - **filename** : str\n                The path to the location of the data. (Future release)\n\n    Examples\n    --------\n    ```python\n    from msk_cdm.datasets import connect_to_db\n    from msk_cdm.datasets.impact import load_data_timeline_prior_meds\n\n    # Connect to the database\n    auth_file = 'path/to/config.txt'\n    connect_to_db(auth_file=auth_file)\n\n    # Load the dataset\n    df_timeline_prior_meds = load_data_timeline_prior_meds()\n\n    # Access the data\n    df_prior_meds = df_timeline_prior_meds['data']\n\n    # Display the first few rows of the data\n    print(df_prior_meds.head())\n    ```\n    \"\"\"\n    df = _loader._load_impact_data_timeline_prior_meds()\n    output = Bunch(data=df)\n    return output\n</code></pre>"},{"location":"reference/datasets/impact/load_data_timeline_progression/","title":"<code>load_data_timeline_progression</code>","text":"<p>Load and return the MSK-IMPACT progression timeline dataset (deidentified).</p> <p>Returns:</p> Name Type Description <code>data</code> <code>Bunch</code> <p>Dictionary-like object, with the following attributes.</p> <ul> <li>data : pandas DataFrame     The data matrix.</li> <li>description_columns : list     The names of the dataset columns. (Future release)</li> <li>description_dataset : str     The full description of the dataset. (Future release)</li> <li>filename : str     The path to the location of the data. (Future release)</li> </ul>"},{"location":"reference/datasets/impact/load_data_timeline_progression/#msk_cdm.datasets.impact.datasets_impact.load_data_timeline_progression--examples","title":"Examples","text":"<pre><code>from msk_cdm.datasets import connect_to_db\nfrom msk_cdm.datasets.impact import load_data_timeline_progression\n\n# Connect to the database\nauth_file = 'path/to/config.txt'\nconnect_to_db(auth_file=auth_file)\n\n# Load the dataset\ndf_timeline_progression = load_data_timeline_progression()\n\n# Access the data\ndf_progression = df_timeline_progression['data']\n\n# Display the first few rows of the data\nprint(df_progression.head())\n</code></pre> Source code in <code>msk_cdm/datasets/impact/datasets_impact.py</code> <pre><code>def load_data_timeline_progression() -&gt; Bunch:\n\"\"\"Load and return the MSK-IMPACT progression timeline dataset (deidentified).\n\n    Returns:\n        data: Dictionary-like object, with the following attributes.\n\n            - **data** : pandas DataFrame\n                The data matrix.\n            - **description_columns** : list\n                The names of the dataset columns. (Future release)\n            - **description_dataset** : str\n                The full description of the dataset. (Future release)\n            - **filename** : str\n                The path to the location of the data. (Future release)\n\n    Examples\n    --------\n    ```python\n    from msk_cdm.datasets import connect_to_db\n    from msk_cdm.datasets.impact import load_data_timeline_progression\n\n    # Connect to the database\n    auth_file = 'path/to/config.txt'\n    connect_to_db(auth_file=auth_file)\n\n    # Load the dataset\n    df_timeline_progression = load_data_timeline_progression()\n\n    # Access the data\n    df_progression = df_timeline_progression['data']\n\n    # Display the first few rows of the data\n    print(df_progression.head())\n    ```\n    \"\"\"\n    df = _loader._load_impact_data_timeline_progression()\n    output = Bunch(data=df)\n    return output\n</code></pre>"},{"location":"reference/datasets/impact/load_data_timeline_radiation/","title":"<code>load_data_timeline_radiation</code>","text":"<p>Load and return the MSK-IMPACT radiation therapy timeline dataset (deidentified).</p> <p>Returns:</p> Name Type Description <code>data</code> <code>Bunch</code> <p>Dictionary-like object, with the following attributes.</p> <ul> <li>data : pandas DataFrame     The data matrix.</li> <li>description_columns (Future release) : list     The names of the dataset columns.</li> <li>description_dataset (Future release) : str     The full description of the dataset.</li> <li>filename (Future release) : str     The path to the location of the data.</li> </ul>"},{"location":"reference/datasets/impact/load_data_timeline_radiation/#msk_cdm.datasets.impact.datasets_impact.load_data_timeline_radiation--examples","title":"Examples","text":"<pre><code>from msk_cdm.datasets import connect_to_db\nfrom msk_cdm.datasets.impact import load_data_timeline_radiation\n\n# Connect to the database\nauth_file = 'path/to/config.txt'\nconnect_to_db(auth_file=auth_file)\n\n# Load the dataset\ndf_timeline_radiation = load_data_timeline_radiation()\n\n# Access the data\ndf_rad = df_timeline_radiation['data']\n\n# Display the first few rows of the data\nprint(df_rad.head())\n</code></pre> Source code in <code>msk_cdm/datasets/impact/datasets_impact.py</code> <pre><code>def load_data_timeline_radiation() -&gt; Bunch:\n\"\"\"Load and return the MSK-IMPACT radiation therapy timeline dataset (deidentified).\n\n    Returns:\n        data: Dictionary-like object, with the following attributes.\n\n            - **data** : pandas DataFrame\n                The data matrix.\n            - **description_columns** (Future release) : list\n                The names of the dataset columns.\n            - **description_dataset** (Future release) : str\n                The full description of the dataset.\n            - **filename** (Future release) : str\n                The path to the location of the data.\n\n    Examples\n    --------\n    ```python\n    from msk_cdm.datasets import connect_to_db\n    from msk_cdm.datasets.impact import load_data_timeline_radiation\n\n    # Connect to the database\n    auth_file = 'path/to/config.txt'\n    connect_to_db(auth_file=auth_file)\n\n    # Load the dataset\n    df_timeline_radiation = load_data_timeline_radiation()\n\n    # Access the data\n    df_rad = df_timeline_radiation['data']\n\n    # Display the first few rows of the data\n    print(df_rad.head())\n    ```\n    \"\"\"\n    df = _loader._load_impact_data_timeline_radiation()\n    output = Bunch(data=df)\n    return output\n</code></pre>"},{"location":"reference/datasets/impact/load_data_timeline_specimen/","title":"<code>load_data_timeline_specimen</code>","text":"<p>Load and return the MSK-IMPACT specimen timeline dataset (deidentified).</p> <p>Returns:</p> Name Type Description <code>data</code> <code>Bunch</code> <p>Dictionary-like object, with the following attributes.</p> <ul> <li>data : pandas DataFrame     The data matrix.</li> <li>description_columns : list     The names of the dataset columns. (Future release)</li> <li>description_dataset : str     The full description of the dataset. (Future release)</li> <li>filename : str     The path to the location of the data. (Future release)</li> </ul>"},{"location":"reference/datasets/impact/load_data_timeline_specimen/#msk_cdm.datasets.impact.datasets_impact.load_data_timeline_specimen--examples","title":"Examples","text":"<pre><code>from msk_cdm.datasets import connect_to_db\nfrom msk_cdm.datasets.impact import load_data_timeline_specimen\n\n# Connect to the database\nauth_file = 'path/to/config.txt'\nconnect_to_db(auth_file=auth_file)\n\n# Load the dataset\ndf_timeline_specimen = load_data_timeline_specimen()\n\n# Access the data\ndf_spec = df_timeline_specimen['data']\n\n# Display the first few rows of the data\nprint(df_spec.head())\n</code></pre> Source code in <code>msk_cdm/datasets/impact/datasets_impact.py</code> <pre><code>def load_data_timeline_specimen() -&gt; Bunch:\n\"\"\"Load and return the MSK-IMPACT specimen timeline dataset (deidentified).\n\n    Returns:\n        data: Dictionary-like object, with the following attributes.\n\n            - **data** : pandas DataFrame\n                The data matrix.\n            - **description_columns** : list\n                The names of the dataset columns. (Future release)\n            - **description_dataset** : str\n                The full description of the dataset. (Future release)\n            - **filename** : str\n                The path to the location of the data. (Future release)\n\n    Examples\n    --------\n    ```python\n    from msk_cdm.datasets import connect_to_db\n    from msk_cdm.datasets.impact import load_data_timeline_specimen\n\n    # Connect to the database\n    auth_file = 'path/to/config.txt'\n    connect_to_db(auth_file=auth_file)\n\n    # Load the dataset\n    df_timeline_specimen = load_data_timeline_specimen()\n\n    # Access the data\n    df_spec = df_timeline_specimen['data']\n\n    # Display the first few rows of the data\n    print(df_spec.head())\n    ```\n    \"\"\"\n    df = _loader._load_impact_data_timeline_specimen()\n    output = Bunch(data=df)\n    return output\n</code></pre>"},{"location":"reference/datasets/impact/load_data_timeline_specimen_surgery/","title":"<code>load_data_timeline_specimen_surgery</code>","text":"<p>Load and return the MSK-IMPACT specimen surgery timeline dataset (deidentified).</p> <p>Returns:</p> Name Type Description <code>data</code> <code>Bunch</code> <p>Dictionary-like object, with the following attributes.</p> <ul> <li>data : pandas DataFrame     The data matrix.</li> <li>description_columns : list     The names of the dataset columns. (Future release)</li> <li>description_dataset : str     The full description of the dataset. (Future release)</li> <li>filename : str     The path to the location of the data. (Future release)</li> </ul>"},{"location":"reference/datasets/impact/load_data_timeline_specimen_surgery/#msk_cdm.datasets.impact.datasets_impact.load_data_timeline_specimen_surgery--examples","title":"Examples","text":"<pre><code>from msk_cdm.datasets import connect_to_db\nfrom msk_cdm.datasets.impact import load_data_timeline_specimen_surgery\n\n# Connect to the database\nauth_file = 'path/to/config.txt'\nconnect_to_db(auth_file=auth_file)\n\n# Load the dataset\ndf_timeline_specimen_surgery = load_data_timeline_specimen_surgery()\n\n# Access the data\ndf_spec_surg = df_timeline_specimen_surgery['data']\n\n# Display the first few rows of the data\nprint(df_spec_surg.head())\n</code></pre> Source code in <code>msk_cdm/datasets/impact/datasets_impact.py</code> <pre><code>def load_data_timeline_specimen_surgery() -&gt; Bunch:\n\"\"\"Load and return the MSK-IMPACT specimen surgery timeline dataset (deidentified).\n\n    Returns:\n        data: Dictionary-like object, with the following attributes.\n\n            - **data** : pandas DataFrame\n                The data matrix.\n            - **description_columns** : list\n                The names of the dataset columns. (Future release)\n            - **description_dataset** : str\n                The full description of the dataset. (Future release)\n            - **filename** : str\n                The path to the location of the data. (Future release)\n\n    Examples\n    --------\n    ```python\n    from msk_cdm.datasets import connect_to_db\n    from msk_cdm.datasets.impact import load_data_timeline_specimen_surgery\n\n    # Connect to the database\n    auth_file = 'path/to/config.txt'\n    connect_to_db(auth_file=auth_file)\n\n    # Load the dataset\n    df_timeline_specimen_surgery = load_data_timeline_specimen_surgery()\n\n    # Access the data\n    df_spec_surg = df_timeline_specimen_surgery['data']\n\n    # Display the first few rows of the data\n    print(df_spec_surg.head())\n    ```\n    \"\"\"\n    df = _loader._load_impact_data_timeline_specimen_surgery()\n    output = Bunch(data=df)\n    return output\n</code></pre>"},{"location":"reference/datasets/impact/load_data_timeline_surgery/","title":"<code>load_data_timeline_surgery</code>","text":"<p>Load and return the MSK-IMPACT surgical timeline dataset (deidentified).</p> <p>Returns:</p> Name Type Description <code>data</code> <code>Bunch</code> <p>Dictionary-like object, with the following attributes.</p> <ul> <li>data : pandas DataFrame     The data matrix.</li> <li>description_columns (Future release) : list     The names of the dataset columns.</li> <li>description_dataset (Future release) : str     The full description of the dataset.</li> <li>filename (Future release) : str     The path to the location of the data.</li> </ul>"},{"location":"reference/datasets/impact/load_data_timeline_surgery/#msk_cdm.datasets.impact.datasets_impact.load_data_timeline_surgery--examples","title":"Examples","text":"<pre><code>from msk_cdm.datasets import connect_to_db\nfrom msk_cdm.datasets.impact import load_data_timeline_surgery\n\n# Connect to the database\nauth_file = 'path/to/config.txt'\nconnect_to_db(auth_file=auth_file)\n\n# Load the dataset\ndf_timeline_surgery = load_data_timeline_surgery()\n\n# Access the data\ndf_surg = df_timeline_surgery['data']\n\n# Display the first few rows of the data\nprint(df_surg.head())\n</code></pre> Source code in <code>msk_cdm/datasets/impact/datasets_impact.py</code> <pre><code>def load_data_timeline_surgery() -&gt; Bunch:\n\"\"\"Load and return the MSK-IMPACT surgical timeline dataset (deidentified).\n\n    Returns:\n        data : Dictionary-like object, with the following attributes.\n\n            - **data** : pandas DataFrame\n                The data matrix.\n            - **description_columns** (Future release) : list\n                The names of the dataset columns.\n            - **description_dataset** (Future release) : str\n                The full description of the dataset.\n            - **filename** (Future release) : str\n                The path to the location of the data.\n\n    Examples\n    --------\n    ```python\n    from msk_cdm.datasets import connect_to_db\n    from msk_cdm.datasets.impact import load_data_timeline_surgery\n\n    # Connect to the database\n    auth_file = 'path/to/config.txt'\n    connect_to_db(auth_file=auth_file)\n\n    # Load the dataset\n    df_timeline_surgery = load_data_timeline_surgery()\n\n    # Access the data\n    df_surg = df_timeline_surgery['data']\n\n    # Display the first few rows of the data\n    print(df_surg.head())\n    ```\n    \"\"\"\n    df = _loader._load_impact_data_timeline_surgery()\n    output = Bunch(data=df)\n    return output\n</code></pre>"},{"location":"reference/datasets/impact/load_data_timeline_treatment/","title":"<code>load_data_timeline_treatment</code>","text":"<p>Load and return the MSK-IMPACT treatment timeline dataset (deidentified).</p> <p>Returns:</p> Name Type Description <code>data</code> <code>Bunch</code> <p>Dictionary-like object, with the following attributes.</p> <ul> <li>data : pandas DataFrame     The data matrix.</li> <li>description_columns (Future release) : list     The names of the dataset columns.</li> <li>description_dataset (Future release) : str     The full description of the dataset.</li> <li>filename (Future release) : str     The path to the location of the data.</li> </ul>"},{"location":"reference/datasets/impact/load_data_timeline_treatment/#msk_cdm.datasets.impact.datasets_impact.load_data_timeline_treatment--examples","title":"Examples","text":"<pre><code>from msk_cdm.datasets import connect_to_db\nfrom msk_cdm.datasets.impact import load_data_timeline_treatment\n\n# Connect to the database\nauth_file = 'path/to/config.txt'\nconnect_to_db(auth_file=auth_file)\n\n# Load the dataset\ndf_timeline_treatment = load_data_timeline_treatment()\n\n# Access the data\ndf_treat = df_timeline_treatment['data']\n\n# Display the first few rows of the data\nprint(df_treat.head())\n</code></pre> Source code in <code>msk_cdm/datasets/impact/datasets_impact.py</code> <pre><code>def load_data_timeline_treatment() -&gt; Bunch:\n\"\"\"Load and return the MSK-IMPACT treatment timeline dataset (deidentified).\n\n    Returns:\n        data: Dictionary-like object, with the following attributes.\n\n            - **data** : pandas DataFrame\n                The data matrix.\n            - **description_columns** (Future release) : list\n                The names of the dataset columns.\n            - **description_dataset** (Future release) : str\n                The full description of the dataset.\n            - **filename** (Future release) : str\n                The path to the location of the data.\n\n    Examples\n    --------\n    ```python\n    from msk_cdm.datasets import connect_to_db\n    from msk_cdm.datasets.impact import load_data_timeline_treatment\n\n    # Connect to the database\n    auth_file = 'path/to/config.txt'\n    connect_to_db(auth_file=auth_file)\n\n    # Load the dataset\n    df_timeline_treatment = load_data_timeline_treatment()\n\n    # Access the data\n    df_treat = df_timeline_treatment['data']\n\n    # Display the first few rows of the data\n    print(df_treat.head())\n    ```\n    \"\"\"\n    df = _loader._load_impact_data_timeline_treatment()\n    output = Bunch(data=df)\n    return output\n</code></pre>"},{"location":"reference/datasets/impact/load_data_timeline_tumor_sites/","title":"<code>load_data_timeline_tumor_sites</code>","text":"<p>Load and return the MSK-IMPACT tumor sites timeline dataset (deidentified).</p> <p>Returns:</p> Name Type Description <code>data</code> <code>Bunch</code> <p>Dictionary-like object, with the following attributes.</p> <ul> <li>data : pandas DataFrame     The data matrix.</li> <li>description_columns : list     The names of the dataset columns. (Future release)</li> <li>description_dataset : str     The full description of the dataset. (Future release)</li> <li>filename : str     The path to the location of the data. (Future release)</li> </ul>"},{"location":"reference/datasets/impact/load_data_timeline_tumor_sites/#msk_cdm.datasets.impact.datasets_impact.load_data_timeline_tumor_sites--examples","title":"Examples","text":"<pre><code>from msk_cdm.datasets import connect_to_db\nfrom msk_cdm.datasets.impact import load_data_timeline_tumor_sites\n\n# Connect to the database\nauth_file = 'path/to/config.txt'\nconnect_to_db(auth_file=auth_file)\n\n# Load the dataset\ndf_timeline_tumor_sites = load_data_timeline_tumor_sites()\n\n# Access the data\ndf_tumor_sites = df_timeline_tumor_sites['data']\n\n# Display the first few rows of the data\nprint(df_tumor_sites.head())\n</code></pre> Source code in <code>msk_cdm/datasets/impact/datasets_impact.py</code> <pre><code>def load_data_timeline_tumor_sites() -&gt; Bunch:\n\"\"\"Load and return the MSK-IMPACT tumor sites timeline dataset (deidentified).\n\n    Returns:\n        data: Dictionary-like object, with the following attributes.\n\n            - **data** : pandas DataFrame\n                The data matrix.\n            - **description_columns** : list\n                The names of the dataset columns. (Future release)\n            - **description_dataset** : str\n                The full description of the dataset. (Future release)\n            - **filename** : str\n                The path to the location of the data. (Future release)\n\n    Examples\n    --------\n    ```python\n    from msk_cdm.datasets import connect_to_db\n    from msk_cdm.datasets.impact import load_data_timeline_tumor_sites\n\n    # Connect to the database\n    auth_file = 'path/to/config.txt'\n    connect_to_db(auth_file=auth_file)\n\n    # Load the dataset\n    df_timeline_tumor_sites = load_data_timeline_tumor_sites()\n\n    # Access the data\n    df_tumor_sites = df_timeline_tumor_sites['data']\n\n    # Display the first few rows of the data\n    print(df_tumor_sites.head())\n    ```\n    \"\"\"\n    df = _loader._load_impact_data_timeline_tumor_sites()\n    output = Bunch(data=df)\n    return output\n</code></pre>"},{"location":"reference/datasets/impact/load_impact_data_clinical_patient/","title":"Load impact data clinical patient","text":"<p>Load and return the MSK-IMPACT clinical patient dataset (deidentified).</p> <p>Returns:</p> Name Type Description <code>data</code> <code>Bunch</code> <p>Dictionary-like object, with the following attributes.</p> <ul> <li>data : pandas DataFrame     The data matrix.</li> <li>description_columns : list     The names of the dataset columns. (Future release)</li> <li>description_dataset : str     The full description of the dataset. (Future release)</li> <li>filename : str     The path to the location of the data. (Future release)</li> </ul>"},{"location":"reference/datasets/impact/load_impact_data_clinical_patient/#msk_cdm.datasets.impact.datasets_impact.load_data_clinical_patient--examples","title":"Examples","text":"<pre><code>from msk_cdm.datasets import connect_to_db\nfrom msk_cdm.datasets.impact import load_data_clinical_patient\n\n# Connect to the database\nauth_file = 'path/to/config.txt'\nconnect_to_db(auth_file=auth_file)\n\n# Load the dataset\ndf_clinical_patient = load_data_clinical_patient()\n\n# Access the data\ndf_clin_p = df_clinical_patient['data']\n\n# Display the first few rows of the data\nprint(df_clin_p.head())\n</code></pre> Source code in <code>msk_cdm/datasets/impact/datasets_impact.py</code> <pre><code>def load_data_clinical_patient() -&gt; Bunch:\n\"\"\"Load and return the MSK-IMPACT clinical patient dataset (deidentified).\n\n    Returns:\n        data : Dictionary-like object, with the following attributes.\n\n            - **data** : pandas DataFrame\n                The data matrix.\n            - **description_columns** : list\n                The names of the dataset columns. (Future release)\n            - **description_dataset** : str\n                The full description of the dataset. (Future release)\n            - **filename** : str\n                The path to the location of the data. (Future release)\n\n    Examples\n    --------\n    ```python\n    from msk_cdm.datasets import connect_to_db\n    from msk_cdm.datasets.impact import load_data_clinical_patient\n\n    # Connect to the database\n    auth_file = 'path/to/config.txt'\n    connect_to_db(auth_file=auth_file)\n\n    # Load the dataset\n    df_clinical_patient = load_data_clinical_patient()\n\n    # Access the data\n    df_clin_p = df_clinical_patient['data']\n\n    # Display the first few rows of the data\n    print(df_clin_p.head())\n    ```\n    \"\"\"\n    df = _loader._load_impact_data_clinical_patient()\n    output = Bunch(data=df)\n    return output\n</code></pre>"},{"location":"reference/datasets/phi_idb/load_demographics_idb/","title":"<code>load_demographics_idb</code>","text":"<p>Load and return the IDB demographics dataset (PHI).</p> <p>Returns:</p> Name Type Description <code>data</code> <code>Bunch</code> <p>Dictionary-like object, with the following attributes.</p> <ul> <li>data : pandas DataFrame     The data matrix.</li> </ul>"},{"location":"reference/datasets/phi_idb/load_demographics_idb/#msk_cdm.datasets.phi.idb.datasets_phi_idb.load_demographics_idb--examples","title":"Examples","text":"<pre><code>from msk_cdm.datasets import connect_to_db\nfrom msk_cdm.datasets.impact import load_demographics_idb\n\n# Connect to the database\nauth_file = 'path/to/config.txt'\nconnect_to_db(auth_file=auth_file)\n\n# Load the dataset\ndf_demographics = load_demographics_idb()\n\n# Access the data\ndf = df_demographics['data']\n\n# Display the first few rows of the data\nprint(df.head())\n</code></pre> Source code in <code>msk_cdm/datasets/phi/idb/datasets_phi_idb.py</code> <pre><code>def load_demographics_idb() -&gt; Bunch:\n\"\"\"Load and return the IDB demographics dataset (PHI).\n\n    Returns:\n        data: Dictionary-like object, with the following attributes.\n\n            - **data** : pandas DataFrame\n                The data matrix.\n\n    Examples\n    --------\n    ```python\n    from msk_cdm.datasets import connect_to_db\n    from msk_cdm.datasets.impact import load_demographics_idb\n\n    # Connect to the database\n    auth_file = 'path/to/config.txt'\n    connect_to_db(auth_file=auth_file)\n\n    # Load the dataset\n    df_demographics = load_demographics_idb()\n\n    # Access the data\n    df = df_demographics['data']\n\n    # Display the first few rows of the data\n    print(df.head())\n    ```\n    \"\"\"\n    df = _loader._load_phi_idb_demographics()\n    data = Bunch(data=df)\n    return data\n</code></pre>"},{"location":"reference/datasets/phi_idb/load_diagnosis_idb/","title":"<code>load_diagnosis_idb</code>","text":"<p>Load and return the IDB diagnosis dataset (PHI).</p> <p>Returns:</p> Name Type Description <code>data</code> <code>Bunch</code> <p>Dictionary-like object, with the following attributes.</p> <ul> <li>data : pandas DataFrame     The data matrix.</li> </ul>"},{"location":"reference/datasets/phi_idb/load_diagnosis_idb/#msk_cdm.datasets.phi.idb.datasets_phi_idb.load_diagnosis_idb--examples","title":"Examples","text":"<pre><code>from msk_cdm.datasets import connect_to_db\nfrom msk_cdm.datasets.impact import load_diagnosis_idb\n\n# Connect to the database\nauth_file = 'path/to/config.txt'\nconnect_to_db(auth_file=auth_file)\n\n# Load the dataset\ndf_diagnosis = load_diagnosis_idb()\n\n# Access the data\ndf = df_diagnosis['data']\n\n# Display the first few rows of the data\nprint(df.head())\n</code></pre> Source code in <code>msk_cdm/datasets/phi/idb/datasets_phi_idb.py</code> <pre><code>def load_diagnosis_idb() -&gt; Bunch:\n\"\"\"Load and return the IDB diagnosis dataset (PHI).\n\n    Returns:\n        data: Dictionary-like object, with the following attributes.\n\n            - **data** : pandas DataFrame\n                The data matrix.\n\n    Examples\n    --------\n    ```python\n    from msk_cdm.datasets import connect_to_db\n    from msk_cdm.datasets.impact import load_diagnosis_idb\n\n    # Connect to the database\n    auth_file = 'path/to/config.txt'\n    connect_to_db(auth_file=auth_file)\n\n    # Load the dataset\n    df_diagnosis = load_diagnosis_idb()\n\n    # Access the data\n    df = df_diagnosis['data']\n\n    # Display the first few rows of the data\n    print(df.head())\n    ```\n    \"\"\"\n    df = _loader._load_phi_idb_diagnosis()\n    data = Bunch(data=df)\n    return data\n</code></pre>"},{"location":"reference/datasets/phi_idb/load_interventional_radiology_idb/","title":"<code>load_interventional_radiology_idb</code>","text":"<p>Load and return the IDB interventional radiology dataset (PHI).</p> <p>Returns:</p> Name Type Description <code>data</code> <code>Bunch</code> <p>Dictionary-like object, with the following attributes.</p> <ul> <li>data : pandas DataFrame     The data matrix.</li> </ul>"},{"location":"reference/datasets/phi_idb/load_interventional_radiology_idb/#msk_cdm.datasets.phi.idb.datasets_phi_idb.load_interventional_radiology_idb--examples","title":"Examples","text":"<pre><code>from msk_cdm.datasets import connect_to_db\nfrom msk_cdm.datasets.impact import load_interventional_radiology_idb\n\n# Connect to the database\nauth_file = 'path/to/config.txt'\nconnect_to_db(auth_file=auth_file)\n\n# Load the dataset\ndf_interventional_radiology = load_interventional_radiology_idb()\n\n# Access the data\ndf = df_interventional_radiology['data']\n\n# Display the first few rows of the data\nprint(df.head())\n</code></pre> Source code in <code>msk_cdm/datasets/phi/idb/datasets_phi_idb.py</code> <pre><code>def load_interventional_radiology_idb() -&gt; Bunch:\n\"\"\"Load and return the IDB interventional radiology dataset (PHI).\n\n    Returns:\n        data: Dictionary-like object, with the following attributes.\n\n            - **data** : pandas DataFrame\n                The data matrix.\n\n    Examples\n    --------\n    ```python\n    from msk_cdm.datasets import connect_to_db\n    from msk_cdm.datasets.impact import load_interventional_radiology_idb\n\n    # Connect to the database\n    auth_file = 'path/to/config.txt'\n    connect_to_db(auth_file=auth_file)\n\n    # Load the dataset\n    df_interventional_radiology = load_interventional_radiology_idb()\n\n    # Access the data\n    df = df_interventional_radiology['data']\n\n    # Display the first few rows of the data\n    print(df.head())\n    ```\n    \"\"\"\n    df = _loader._load_phi_idb_interventional_radiology()\n    data = Bunch(data=df)\n    return data\n</code></pre>"},{"location":"reference/datasets/phi_idb/load_medications_idb/","title":"<code>load_medications_idb</code>","text":"<p>Load and return the IDB medications dataset (PHI).</p> <p>Returns:</p> Name Type Description <code>data</code> <code>Bunch</code> <p>Dictionary-like object, with the following attributes.</p> <ul> <li>data : pandas DataFrame     The data matrix.</li> </ul>"},{"location":"reference/datasets/phi_idb/load_medications_idb/#msk_cdm.datasets.phi.idb.datasets_phi_idb.load_medications_idb--examples","title":"Examples","text":"<pre><code>from msk_cdm.datasets import connect_to_db\nfrom msk_cdm.datasets.impact import load_medications_idb\n\n# Connect to the database\nauth_file = 'path/to/config.txt'\nconnect_to_db(auth_file=auth_file)\n\n# Load the dataset\ndf_medications = load_medications_idb()\n\n# Access the data\ndf = df_medications['data']\n\n# Display the first few rows of the data\nprint(df.head())\n</code></pre> Source code in <code>msk_cdm/datasets/phi/idb/datasets_phi_idb.py</code> <pre><code>def load_medications_idb() -&gt; Bunch:\n\"\"\"Load and return the IDB medications dataset (PHI).\n\n    Returns:\n        data: Dictionary-like object, with the following attributes.\n\n            - **data** : pandas DataFrame\n                The data matrix.\n\n    Examples\n    --------\n    ```python\n    from msk_cdm.datasets import connect_to_db\n    from msk_cdm.datasets.impact import load_medications_idb\n\n    # Connect to the database\n    auth_file = 'path/to/config.txt'\n    connect_to_db(auth_file=auth_file)\n\n    # Load the dataset\n    df_medications = load_medications_idb()\n\n    # Access the data\n    df = df_medications['data']\n\n    # Display the first few rows of the data\n    print(df.head())\n    ```\n    \"\"\"\n    df = _loader._load_phi_idb_medications()\n    data = Bunch(data=df)\n    return data\n</code></pre>"},{"location":"reference/datasets/phi_idb/load_pathology_reports_idb/","title":"<code>load_pathology_reports_idb</code>","text":"<p>Load and return the IDB pathology reports dataset (PHI).</p> <p>Returns:</p> Name Type Description <code>data</code> <code>Bunch</code> <p>Dictionary-like object, with the following attributes.</p> <ul> <li>data : pandas DataFrame     The data matrix.</li> </ul>"},{"location":"reference/datasets/phi_idb/load_pathology_reports_idb/#msk_cdm.datasets.phi.idb.datasets_phi_idb.load_pathology_reports_idb--examples","title":"Examples","text":"<pre><code>from msk_cdm.datasets import connect_to_db\nfrom msk_cdm.datasets.impact import load_pathology_reports_idb\n\n# Connect to the database\nauth_file = 'path/to/config.txt'\nconnect_to_db(auth_file=auth_file)\n\n# Load the dataset\ndf_pathology_reports = load_pathology_reports_idb()\n\n# Access the data\ndf = df_pathology_reports['data']\n\n# Display the first few rows of the data\nprint(df.head())\n</code></pre> Source code in <code>msk_cdm/datasets/phi/idb/datasets_phi_idb.py</code> <pre><code>def load_pathology_reports_idb() -&gt; Bunch:\n\"\"\"Load and return the IDB pathology reports dataset (PHI).\n\n    Returns:\n        data: Dictionary-like object, with the following attributes.\n\n            - **data** : pandas DataFrame\n                The data matrix.\n\n    Examples\n    --------\n    ```python\n    from msk_cdm.datasets import connect_to_db\n    from msk_cdm.datasets.impact import load_pathology_reports_idb\n\n    # Connect to the database\n    auth_file = 'path/to/config.txt'\n    connect_to_db(auth_file=auth_file)\n\n    # Load the dataset\n    df_pathology_reports = load_pathology_reports_idb()\n\n    # Access the data\n    df = df_pathology_reports['data']\n\n    # Display the first few rows of the data\n    print(df.head())\n    ```\n    \"\"\"\n    df = _loader._load_phi_idb_pathology_reports()\n    data = Bunch(data=df)\n    return data\n</code></pre>"},{"location":"reference/datasets/phi_idb/load_radiation_idb/","title":"<code>load_radiation_idb</code>","text":"<p>Load and return the IDB radiation dataset (PHI).</p> <p>Returns:</p> Name Type Description <code>data</code> <code>Bunch</code> <p>Dictionary-like object, with the following attributes.</p> <ul> <li>data : pandas DataFrame     The data matrix.</li> </ul>"},{"location":"reference/datasets/phi_idb/load_radiation_idb/#msk_cdm.datasets.phi.idb.datasets_phi_idb.load_radiation_idb--examples","title":"Examples","text":"<pre><code>from msk_cdm.datasets import connect_to_db\nfrom msk_cdm.datasets.impact import load_radiation_idb\n\n# Connect to the database\nauth_file = 'path/to/config.txt'\nconnect_to_db(auth_file=auth_file)\n\n# Load the dataset\ndf_radiation = load_radiation_idb()\n\n# Access the data\ndf = df_radiation['data']\n\n# Display the first few rows of the data\nprint(df.head())\n</code></pre> Source code in <code>msk_cdm/datasets/phi/idb/datasets_phi_idb.py</code> <pre><code>def load_radiation_idb() -&gt; Bunch:\n\"\"\"Load and return the IDB radiation dataset (PHI).\n\n    Returns:\n        data: Dictionary-like object, with the following attributes.\n\n            - **data** : pandas DataFrame\n                The data matrix.\n\n    Examples\n    --------\n    ```python\n    from msk_cdm.datasets import connect_to_db\n    from msk_cdm.datasets.impact import load_radiation_idb\n\n    # Connect to the database\n    auth_file = 'path/to/config.txt'\n    connect_to_db(auth_file=auth_file)\n\n    # Load the dataset\n    df_radiation = load_radiation_idb()\n\n    # Access the data\n    df = df_radiation['data']\n\n    # Display the first few rows of the data\n    print(df.head())\n    ```\n    \"\"\"\n    df = _loader._load_phi_idb_radiation()\n    data = Bunch(data=df)\n    return data\n</code></pre>"},{"location":"reference/datasets/phi_idb/load_radiology_reports_idb/","title":"<code>load_radiology_reports_idb</code>","text":"<p>Load and return the IDB radiology reports dataset (PHI).</p> <p>Returns:</p> Name Type Description <code>data</code> <code>Bunch</code> <p>Dictionary-like object, with the following attributes.</p> <ul> <li>data : pandas DataFrame     The data matrix.</li> </ul>"},{"location":"reference/datasets/phi_idb/load_radiology_reports_idb/#msk_cdm.datasets.phi.idb.datasets_phi_idb.load_radiology_reports_idb--examples","title":"Examples","text":"<pre><code>from msk_cdm.datasets import connect_to_db\nfrom msk_cdm.datasets.impact import load_radiology_reports_idb\n\n# Connect to the database\nauth_file = 'path/to/config.txt'\nconnect_to_db(auth_file=auth_file)\n\n# Load the dataset\ndf_radiology_reports = load_radiology_reports_idb()\n\n# Access the data\ndf = df_radiology_reports['data']\n\n# Display the first few rows of the data\nprint(df.head())\n</code></pre> Source code in <code>msk_cdm/datasets/phi/idb/datasets_phi_idb.py</code> <pre><code>def load_radiology_reports_idb() -&gt; Bunch:\n\"\"\"Load and return the IDB radiology reports dataset (PHI).\n\n    Returns:\n        data: Dictionary-like object, with the following attributes.\n\n            - **data** : pandas DataFrame\n                The data matrix.\n\n    Examples\n    --------\n    ```python\n    from msk_cdm.datasets import connect_to_db\n    from msk_cdm.datasets.impact import load_radiology_reports_idb\n\n    # Connect to the database\n    auth_file = 'path/to/config.txt'\n    connect_to_db(auth_file=auth_file)\n\n    # Load the dataset\n    df_radiology_reports = load_radiology_reports_idb()\n\n    # Access the data\n    df = df_radiology_reports['data']\n\n    # Display the first few rows of the data\n    print(df.head())\n    ```\n    \"\"\"\n    df = _loader._load_phi_idb_radiology_reports()\n    data = Bunch(data=df)\n    return data\n</code></pre>"},{"location":"reference/datasets/phi_idb/load_surgeries_idb/","title":"<code>load_surgeries_idb</code>","text":"<p>Load and return the IDB surgeries dataset (PHI).</p> <p>Returns:</p> Name Type Description <code>data</code> <code>Bunch</code> <p>Dictionary-like object, with the following attributes.</p> <ul> <li>data : pandas DataFrame     The data matrix.</li> </ul>"},{"location":"reference/datasets/phi_idb/load_surgeries_idb/#msk_cdm.datasets.phi.idb.datasets_phi_idb.load_surgeries_idb--examples","title":"Examples","text":"<pre><code>from msk_cdm.datasets import connect_to_db\nfrom msk_cdm.datasets.impact import load_surgeries_idb\n\n# Connect to the database\nauth_file = 'path/to/config.txt'\nconnect_to_db(auth_file=auth_file)\n\n# Load the dataset\ndf_surgeries = load_surgeries_idb()\n\n# Access the data\ndf = df_surgeries['data']\n\n# Display the first few rows of the data\nprint(df.head())\n</code></pre> Source code in <code>msk_cdm/datasets/phi/idb/datasets_phi_idb.py</code> <pre><code>def load_surgeries_idb() -&gt; Bunch:\n\"\"\"Load and return the IDB surgeries dataset (PHI).\n\n    Returns:\n        data: Dictionary-like object, with the following attributes.\n\n            - **data** : pandas DataFrame\n                The data matrix.\n\n    Examples\n    --------\n    ```python\n    from msk_cdm.datasets import connect_to_db\n    from msk_cdm.datasets.impact import load_surgeries_idb\n\n    # Connect to the database\n    auth_file = 'path/to/config.txt'\n    connect_to_db(auth_file=auth_file)\n\n    # Load the dataset\n    df_surgeries = load_surgeries_idb()\n\n    # Access the data\n    df = df_surgeries['data']\n\n    # Display the first few rows of the data\n    print(df.head())\n    ```\n    \"\"\"\n    df = _loader._load_phi_idb_surgeries()\n    data = Bunch(data=df)\n    return data\n</code></pre>"},{"location":"reference/datasets/phi_impact/load_data_anchor_dates_phi/","title":"<code>load_data_anchor_dates_phi</code>","text":"<p>Load and return the anchor dates dataset (PHI).</p> <p>Returns:</p> Name Type Description <code>data</code> <code>Bunch</code> <p>Dictionary-like object, with the following attributes.</p> <ul> <li>data : pandas DataFrame     The data matrix.</li> </ul>"},{"location":"reference/datasets/phi_impact/load_data_anchor_dates_phi/#msk_cdm.datasets.phi.impact.datasets_phi_impact.load_data_anchor_dates_phi--examples","title":"Examples","text":"<pre><code>from msk_cdm.datasets import connect_to_db\nfrom msk_cdm.datasets.impact import load_data_anchor_dates_phi\n\nauth_file = 'path/to/config.txt'\nconnect_to_db(auth_file=auth_file)\n\ndf_anchor_dates = load_data_anchor_dates_phi()\ndf = df_anchor_dates['data']\n\nprint(df.head())\n</code></pre> Source code in <code>msk_cdm/datasets/phi/impact/datasets_phi_impact.py</code> <pre><code>def load_data_anchor_dates_phi() -&gt; Bunch:\n\"\"\"Load and return the anchor dates dataset (PHI).\n\n    Returns:\n        data: Dictionary-like object, with the following attributes.\n\n            - **data** : pandas DataFrame\n                The data matrix.\n\n    Examples\n    --------\n    ```python\n    from msk_cdm.datasets import connect_to_db\n    from msk_cdm.datasets.impact import load_data_anchor_dates_phi\n\n    auth_file = 'path/to/config.txt'\n    connect_to_db(auth_file=auth_file)\n\n    df_anchor_dates = load_data_anchor_dates_phi()\n    df = df_anchor_dates['data']\n\n    print(df.head())\n    ```\n    \"\"\"\n    df = _loader._load_phi_impact_anchor_dates()\n    data = Bunch(data=df)\n    return data\n</code></pre>"},{"location":"reference/datasets/phi_impact/load_data_id_mapping_phi/","title":"<code>load_data_id_mapping_phi</code>","text":"<p>Load and return the ID mapping dataset (PHI).</p> <p>Returns:</p> Name Type Description <code>data</code> <code>Bunch</code> <p>Dictionary-like object, with the following attributes.</p> <ul> <li>data : pandas DataFrame     The data matrix.</li> </ul>"},{"location":"reference/datasets/phi_impact/load_data_id_mapping_phi/#msk_cdm.datasets.phi.impact.datasets_phi_impact.load_data_id_mapping_phi--examples","title":"Examples","text":"<pre><code>from msk_cdm.datasets import connect_to_db\nfrom msk_cdm.datasets.impact import load_data_id_mapping_phi\n\nauth_file = 'path/to/config.txt'\nconnect_to_db(auth_file=auth_file)\n\ndf_id_mapping = load_data_id_mapping_phi()\ndf = df_id_mapping['data']\n\nprint(df.head())\n</code></pre> Source code in <code>msk_cdm/datasets/phi/impact/datasets_phi_impact.py</code> <pre><code>def load_data_id_mapping_phi() -&gt; Bunch:\n\"\"\"Load and return the ID mapping dataset (PHI).\n\n    Returns:\n        data: Dictionary-like object, with the following attributes.\n\n            - **data** : pandas DataFrame\n                The data matrix.\n\n    Examples\n    --------\n    ```python\n    from msk_cdm.datasets import connect_to_db\n    from msk_cdm.datasets.impact import load_data_id_mapping_phi\n\n    auth_file = 'path/to/config.txt'\n    connect_to_db(auth_file=auth_file)\n\n    df_id_mapping = load_data_id_mapping_phi()\n    df = df_id_mapping['data']\n\n    print(df.head())\n    ```\n    \"\"\"\n    df = _loader._load_phi_impact_id_mapping()\n    data = Bunch(data=df)\n    return data\n</code></pre>"},{"location":"reference/datasets/phi_impact/load_data_timeline_cancer_presence_phi/","title":"<code>load_data_timeline_cancer_presence_phi</code>","text":"<p>Load and return the cancer presence timeline dataset (PHI).</p> <p>Returns:</p> Name Type Description <code>data</code> <code>Bunch</code> <p>Dictionary-like object, with the following attributes.</p> <ul> <li>data : pandas DataFrame     The data matrix.</li> </ul>"},{"location":"reference/datasets/phi_impact/load_data_timeline_cancer_presence_phi/#msk_cdm.datasets.phi.impact.datasets_phi_impact.load_data_timeline_cancer_presence_phi--examples","title":"Examples","text":"<pre><code>from msk_cdm.datasets import connect_to_db\nfrom msk_cdm.datasets.impact import load_data_timeline_cancer_presence_phi\n\nauth_file = 'path/to/config.txt'\nconnect_to_db(auth_file=auth_file)\n\ndf_timeline_cancer_presence = load_data_timeline_cancer_presence_phi()\ndf = df_timeline_cancer_presence['data']\n\nprint(df.head())\n</code></pre> Source code in <code>msk_cdm/datasets/phi/impact/datasets_phi_impact.py</code> <pre><code>def load_data_timeline_cancer_presence_phi() -&gt; Bunch:\n\"\"\"Load and return the cancer presence timeline dataset (PHI).\n\n    Returns:\n        data: Dictionary-like object, with the following attributes.\n\n            - **data** : pandas DataFrame\n                The data matrix.\n\n    Examples\n    --------\n    ```python\n    from msk_cdm.datasets import connect_to_db\n    from msk_cdm.datasets.impact import load_data_timeline_cancer_presence_phi\n\n    auth_file = 'path/to/config.txt'\n    connect_to_db(auth_file=auth_file)\n\n    df_timeline_cancer_presence = load_data_timeline_cancer_presence_phi()\n    df = df_timeline_cancer_presence['data']\n\n    print(df.head())\n    ```\n    \"\"\"\n    df = _loader._load_phi_impact_data_timeline_cancer_presence()\n    data = Bunch(data=df)\n    return data\n</code></pre>"},{"location":"reference/datasets/phi_impact/load_data_timeline_diagnosis_phi/","title":"<code>load_data_timeline_diagnosis_phi</code>","text":"<p>Load and return the diagnosis timeline dataset (PHI).</p> <p>Returns:</p> Name Type Description <code>data</code> <code>Bunch</code> <p>Dictionary-like object, with the following attributes.</p> <ul> <li>data : pandas DataFrame     The data matrix.</li> </ul>"},{"location":"reference/datasets/phi_impact/load_data_timeline_diagnosis_phi/#msk_cdm.datasets.phi.impact.datasets_phi_impact.load_data_timeline_diagnosis_phi--examples","title":"Examples","text":"<pre><code>from msk_cdm.datasets import connect_to_db\nfrom msk_cdm.datasets.impact import load_data_timeline_diagnosis_phi\n\nauth_file = 'path/to/config.txt'\nconnect_to_db(auth_file=auth_file)\n\ndf_timeline_diagnosis = load_data_timeline_diagnosis_phi()\ndf = df_timeline_diagnosis['data']\n\nprint(df.head())\n</code></pre> Source code in <code>msk_cdm/datasets/phi/impact/datasets_phi_impact.py</code> <pre><code>def load_data_timeline_diagnosis_phi() -&gt; Bunch:\n\"\"\"Load and return the diagnosis timeline dataset (PHI).\n\n    Returns:\n        data: Dictionary-like object, with the following attributes.\n\n            - **data** : pandas DataFrame\n                The data matrix.\n\n    Examples\n    --------\n    ```python\n    from msk_cdm.datasets import connect_to_db\n    from msk_cdm.datasets.impact import load_data_timeline_diagnosis_phi\n\n    auth_file = 'path/to/config.txt'\n    connect_to_db(auth_file=auth_file)\n\n    df_timeline_diagnosis = load_data_timeline_diagnosis_phi()\n    df = df_timeline_diagnosis['data']\n\n    print(df.head())\n    ```\n    \"\"\"\n    df = _loader._load_phi_impact_data_timeline_diagnosis()\n    data = Bunch(data=df)\n    return data\n</code></pre>"},{"location":"reference/datasets/phi_impact/load_data_timeline_ecog_kps_phi/","title":"<code>load_data_timeline_ecog_kps_phi</code>","text":"<p>Load and return the ECOG/KPS timeline dataset (PHI).</p> <p>Returns:</p> Name Type Description <code>data</code> <code>Bunch</code> <p>Dictionary-like object, with the following attributes.</p> <ul> <li>data : pandas DataFrame     The data matrix.</li> </ul>"},{"location":"reference/datasets/phi_impact/load_data_timeline_ecog_kps_phi/#msk_cdm.datasets.phi.impact.datasets_phi_impact.load_data_timeline_ecog_kps_phi--examples","title":"Examples","text":"<pre><code>from msk_cdm.datasets import connect_to_db\nfrom msk_cdm.datasets.impact import load_data_timeline_ecog_kps_phi\n\nauth_file = 'path/to/config.txt'\nconnect_to_db(auth_file=auth_file)\n\ndf_timeline_ecog_kps = load_data_timeline_ecog_kps_phi()\ndf = df_timeline_ecog_kps['data']\n\nprint(df.head())\n</code></pre> Source code in <code>msk_cdm/datasets/phi/impact/datasets_phi_impact.py</code> <pre><code>def load_data_timeline_ecog_kps_phi() -&gt; Bunch:\n\"\"\"Load and return the ECOG/KPS timeline dataset (PHI).\n\n    Returns:\n        data: Dictionary-like object, with the following attributes.\n\n            - **data** : pandas DataFrame\n                The data matrix.\n\n    Examples\n    --------\n    ```python\n    from msk_cdm.datasets import connect_to_db\n    from msk_cdm.datasets.impact import load_data_timeline_ecog_kps_phi\n\n    auth_file = 'path/to/config.txt'\n    connect_to_db(auth_file=auth_file)\n\n    df_timeline_ecog_kps = load_data_timeline_ecog_kps_phi()\n    df = df_timeline_ecog_kps['data']\n\n    print(df.head())\n    ```\n    \"\"\"\n    df = _loader._load_phi_impact_data_timeline_ecog_kps()\n    data = Bunch(data=df)\n    return data\n</code></pre>"},{"location":"reference/datasets/phi_impact/load_data_timeline_follow_up_phi/","title":"<code>load_data_timeline_follow_up_phi</code>","text":"<p>Load and return the follow-up timeline dataset (PHI).</p> <p>Returns:</p> Name Type Description <code>data</code> <code>Bunch</code> <p>Dictionary-like object, with the following attributes.</p> <ul> <li>data : pandas DataFrame     The data matrix.</li> </ul>"},{"location":"reference/datasets/phi_impact/load_data_timeline_follow_up_phi/#msk_cdm.datasets.phi.impact.datasets_phi_impact.load_data_timeline_follow_up_phi--examples","title":"Examples","text":"<pre><code>from msk_cdm.datasets import connect_to_db\nfrom msk_cdm.datasets.impact import load_data_timeline_follow_up_phi\n\nauth_file = 'path/to/config.txt'\nconnect_to_db(auth_file=auth_file)\n\ndf_timeline_follow_up = load_data_timeline_follow_up_phi()\ndf = df_timeline_follow_up['data']\n\nprint(df.head())\n</code></pre> Source code in <code>msk_cdm/datasets/phi/impact/datasets_phi_impact.py</code> <pre><code>def load_data_timeline_follow_up_phi() -&gt; Bunch:\n\"\"\"Load and return the follow-up timeline dataset (PHI).\n\n    Returns:\n        data: Dictionary-like object, with the following attributes.\n\n            - **data** : pandas DataFrame\n                The data matrix.\n\n    Examples\n    --------\n    ```python\n    from msk_cdm.datasets import connect_to_db\n    from msk_cdm.datasets.impact import load_data_timeline_follow_up_phi\n\n    auth_file = 'path/to/config.txt'\n    connect_to_db(auth_file=auth_file)\n\n    df_timeline_follow_up = load_data_timeline_follow_up_phi()\n    df = df_timeline_follow_up['data']\n\n    print(df.head())\n    ```\n    \"\"\"\n    df = _loader._load_phi_impact_data_timeline_follow_up()\n    data = Bunch(data=df)\n    return data\n</code></pre>"},{"location":"reference/datasets/phi_impact/load_data_timeline_gleason_phi/","title":"<code>load_data_timeline_gleason_phi</code>","text":"<p>Load and return the Gleason score timeline dataset (PHI).</p> <p>Returns:</p> Name Type Description <code>data</code> <code>Bunch</code> <p>Dictionary-like object, with the following attributes.</p> <ul> <li>data : pandas DataFrame     The data matrix.</li> </ul>"},{"location":"reference/datasets/phi_impact/load_data_timeline_gleason_phi/#msk_cdm.datasets.phi.impact.datasets_phi_impact.load_data_timeline_gleason_phi--examples","title":"Examples","text":"<pre><code>from msk_cdm.datasets import connect_to_db\nfrom msk_cdm.datasets.impact import load_data_timeline_gleason_phi\n\nauth_file = 'path/to/config.txt'\nconnect_to_db(auth_file=auth_file)\n\ndf_timeline_gleason = load_data_timeline_gleason_phi()\ndf = df_timeline_gleason['data']\n\nprint(df.head())\n</code></pre> Source code in <code>msk_cdm/datasets/phi/impact/datasets_phi_impact.py</code> <pre><code>def load_data_timeline_gleason_phi() -&gt; Bunch:\n\"\"\"Load and return the Gleason score timeline dataset (PHI).\n\n    Returns:\n        data: Dictionary-like object, with the following attributes.\n\n            - **data** : pandas DataFrame\n                The data matrix.\n\n    Examples\n    --------\n    ```python\n    from msk_cdm.datasets import connect_to_db\n    from msk_cdm.datasets.impact import load_data_timeline_gleason_phi\n\n    auth_file = 'path/to/config.txt'\n    connect_to_db(auth_file=auth_file)\n\n    df_timeline_gleason = load_data_timeline_gleason_phi()\n    df = df_timeline_gleason['data']\n\n    print(df.head())\n    ```\n    \"\"\"\n    df = _loader._load_phi_impact_data_timeline_gleason()\n    data = Bunch(data=df)\n    return data\n</code></pre>"},{"location":"reference/datasets/phi_impact/load_data_timeline_mmr_phi/","title":"<code>load_data_timeline_mmr_phi</code>","text":"<p>Load and return the MMR timeline dataset (PHI).</p> <p>Returns:</p> Name Type Description <code>data</code> <code>Bunch</code> <p>Dictionary-like object, with the following attributes.</p> <ul> <li>data : pandas DataFrame     The data matrix.</li> </ul>"},{"location":"reference/datasets/phi_impact/load_data_timeline_mmr_phi/#msk_cdm.datasets.phi.impact.datasets_phi_impact.load_data_timeline_mmr_phi--examples","title":"Examples","text":"<pre><code>from msk_cdm.datasets import connect_to_db\nfrom msk_cdm.datasets.impact import load_data_timeline_mmr_phi\n\nauth_file = 'path/to/config.txt'\nconnect_to_db(auth_file=auth_file)\n\ndf_timeline_mmr = load_data_timeline_mmr_phi()\ndf = df_timeline_mmr['data']\n\nprint(df.head())\n</code></pre> Source code in <code>msk_cdm/datasets/phi/impact/datasets_phi_impact.py</code> <pre><code>def load_data_timeline_mmr_phi() -&gt; Bunch:\n\"\"\"Load and return the MMR timeline dataset (PHI).\n\n    Returns:\n        data: Dictionary-like object, with the following attributes.\n\n            - **data** : pandas DataFrame\n                The data matrix.\n\n    Examples\n    --------\n    ```python\n    from msk_cdm.datasets import connect_to_db\n    from msk_cdm.datasets.impact import load_data_timeline_mmr_phi\n\n    auth_file = 'path/to/config.txt'\n    connect_to_db(auth_file=auth_file)\n\n    df_timeline_mmr = load_data_timeline_mmr_phi()\n    df = df_timeline_mmr['data']\n\n    print(df.head())\n    ```\n    \"\"\"\n    df = _loader._load_phi_impact_data_timeline_mmr()\n    data = Bunch(data=df)\n    return data\n</code></pre>"},{"location":"reference/datasets/phi_impact/load_data_timeline_pdl1_phi/","title":"<code>load_data_timeline_pdl1_phi</code>","text":"<p>Load and return the PD-L1 timeline dataset (PHI).</p> <p>Returns:</p> Name Type Description <code>data</code> <code>Bunch</code> <p>Dictionary-like object, with the following attributes.</p> <ul> <li>data : pandas DataFrame     The data matrix.</li> </ul>"},{"location":"reference/datasets/phi_impact/load_data_timeline_pdl1_phi/#msk_cdm.datasets.phi.impact.datasets_phi_impact.load_data_timeline_pdl1_phi--examples","title":"Examples","text":"<pre><code>from msk_cdm.datasets import connect_to_db\nfrom msk_cdm.datasets.impact import load_data_timeline_pdl1_phi\n\nauth_file = 'path/to/config.txt'\nconnect_to_db(auth_file=auth_file)\n\ndf_timeline_pdl1 = load_data_timeline_pdl1_phi()\ndf = df_timeline_pdl1['data']\n\nprint(df.head())\n</code></pre> Source code in <code>msk_cdm/datasets/phi/impact/datasets_phi_impact.py</code> <pre><code>def load_data_timeline_pdl1_phi() -&gt; Bunch:\n\"\"\"Load and return the PD-L1 timeline dataset (PHI).\n\n    Returns:\n        data: Dictionary-like object, with the following attributes.\n\n            - **data** : pandas DataFrame\n                The data matrix.\n\n    Examples\n    --------\n    ```python\n    from msk_cdm.datasets import connect_to_db\n    from msk_cdm.datasets.impact import load_data_timeline_pdl1_phi\n\n    auth_file = 'path/to/config.txt'\n    connect_to_db(auth_file=auth_file)\n\n    df_timeline_pdl1 = load_data_timeline_pdl1_phi()\n    df = df_timeline_pdl1['data']\n\n    print(df.head())\n    ```\n    \"\"\"\n    df = _loader._load_phi_impact_data_timeline_pdl1()\n    data = Bunch(data=df)\n    return data\n</code></pre>"},{"location":"reference/datasets/phi_impact/load_data_timeline_prior_meds_phi/","title":"<code>load_data_timeline_prior_meds_phi</code>","text":"<p>Load and return the prior medications timeline dataset (PHI).</p> <p>Returns:</p> Name Type Description <code>data</code> <code>Bunch</code> <p>Dictionary-like object, with the following attributes.</p> <ul> <li>data : pandas DataFrame     The data matrix.</li> </ul>"},{"location":"reference/datasets/phi_impact/load_data_timeline_prior_meds_phi/#msk_cdm.datasets.phi.impact.datasets_phi_impact.load_data_timeline_prior_meds_phi--examples","title":"Examples","text":"<pre><code>from msk_cdm.datasets import connect_to_db\nfrom msk_cdm.datasets.impact import load_data_timeline_prior_meds_phi\n\nauth_file = 'path/to/config.txt'\nconnect_to_db(auth_file=auth_file)\n\ndf_timeline_prior_meds = load_data_timeline_prior_meds_phi()\ndf = df_timeline_prior_meds['data']\n\nprint(df.head())\n</code></pre> Source code in <code>msk_cdm/datasets/phi/impact/datasets_phi_impact.py</code> <pre><code>def load_data_timeline_prior_meds_phi() -&gt; Bunch:\n\"\"\"Load and return the prior medications timeline dataset (PHI).\n\n    Returns:\n        data: Dictionary-like object, with the following attributes.\n\n            - **data** : pandas DataFrame\n                The data matrix.\n\n    Examples\n    --------\n    ```python\n    from msk_cdm.datasets import connect_to_db\n    from msk_cdm.datasets.impact import load_data_timeline_prior_meds_phi\n\n    auth_file = 'path/to/config.txt'\n    connect_to_db(auth_file=auth_file)\n\n    df_timeline_prior_meds = load_data_timeline_prior_meds_phi()\n    df = df_timeline_prior_meds['data']\n\n    print(df.head())\n    ```\n    \"\"\"\n    df = _loader._load_phi_impact_data_timeline_prior_meds()\n    data = Bunch(data=df)\n    return data\n</code></pre>"},{"location":"reference/datasets/phi_impact/load_data_timeline_progression_phi/","title":"<code>load_data_timeline_progression_phi</code>","text":"<p>Load and return the progression timeline dataset (PHI).</p> <p>Returns:</p> Name Type Description <code>data</code> <code>Bunch</code> <p>Dictionary-like object, with the following attributes.</p> <ul> <li>data : pandas DataFrame     The data matrix.</li> </ul>"},{"location":"reference/datasets/phi_impact/load_data_timeline_progression_phi/#msk_cdm.datasets.phi.impact.datasets_phi_impact.load_data_timeline_progression_phi--examples","title":"Examples","text":"<pre><code>from msk_cdm.datasets import connect_to_db\nfrom msk_cdm.datasets.impact import load_data_timeline_progression_phi\n\nauth_file = 'path/to/config.txt'\nconnect_to_db(auth_file=auth_file)\n\ndf_timeline_progression = load_data_timeline_progression_phi()\ndf = df_timeline_progression['data']\n\nprint(df.head())\n</code></pre> Source code in <code>msk_cdm/datasets/phi/impact/datasets_phi_impact.py</code> <pre><code>def load_data_timeline_progression_phi() -&gt; Bunch:\n\"\"\"Load and return the progression timeline dataset (PHI).\n\n    Returns:\n        data: Dictionary-like object, with the following attributes.\n\n            - **data** : pandas DataFrame\n                The data matrix.\n\n    Examples\n    --------\n    ```python\n    from msk_cdm.datasets import connect_to_db\n    from msk_cdm.datasets.impact import load_data_timeline_progression_phi\n\n    auth_file = 'path/to/config.txt'\n    connect_to_db(auth_file=auth_file)\n\n    df_timeline_progression = load_data_timeline_progression_phi()\n    df = df_timeline_progression['data']\n\n    print(df.head())\n    ```\n    \"\"\"\n    df = _loader._load_phi_impact_data_timeline_progression()\n    data = Bunch(data=df)\n    return data\n</code></pre>"},{"location":"reference/datasets/phi_impact/load_data_timeline_radiation_phi/","title":"<code>load_data_timeline_radiation_phi</code>","text":"<p>Load and return the radiation timeline dataset (PHI).</p> <p>Returns:</p> Name Type Description <code>data</code> <code>Bunch</code> <p>Dictionary-like object, with the following attributes.</p> <ul> <li>data : pandas DataFrame     The data matrix.</li> </ul>"},{"location":"reference/datasets/phi_impact/load_data_timeline_radiation_phi/#msk_cdm.datasets.phi.impact.datasets_phi_impact.load_data_timeline_radiation_phi--examples","title":"Examples","text":"<pre><code>from msk_cdm.datasets import connect_to_db\nfrom msk_cdm.datasets.impact import load_data_timeline_radiation_phi\n\nauth_file = 'path/to/config.txt'\nconnect_to_db(auth_file=auth_file)\n\ndf_timeline_radiation = load_data_timeline_radiation_phi()\ndf = df_timeline_radiation['data']\n\nprint(df.head())\n</code></pre> Source code in <code>msk_cdm/datasets/phi/impact/datasets_phi_impact.py</code> <pre><code>def load_data_timeline_radiation_phi() -&gt; Bunch:\n\"\"\"Load and return the radiation timeline dataset (PHI).\n\n    Returns:\n        data: Dictionary-like object, with the following attributes.\n\n            - **data** : pandas DataFrame\n                The data matrix.\n\n    Examples\n    --------\n    ```python\n    from msk_cdm.datasets import connect_to_db\n    from msk_cdm.datasets.impact import load_data_timeline_radiation_phi\n\n    auth_file = 'path/to/config.txt'\n    connect_to_db(auth_file=auth_file)\n\n    df_timeline_radiation = load_data_timeline_radiation_phi()\n    df = df_timeline_radiation['data']\n\n    print(df.head())\n    ```\n    \"\"\"\n    df = _loader._load_phi_impact_data_timeline_radiation()\n    data = Bunch(data=df)\n    return data\n</code></pre>"},{"location":"reference/datasets/phi_impact/load_data_timeline_specimen_phi/","title":"<code>load_data_timeline_specimen_phi</code>","text":"<p>Load and return the specimen timeline dataset (PHI).</p> <p>Returns:</p> Name Type Description <code>data</code> <code>Bunch</code> <p>Dictionary-like object, with the following attributes.</p> <ul> <li>data : pandas DataFrame     The data matrix.</li> </ul>"},{"location":"reference/datasets/phi_impact/load_data_timeline_specimen_phi/#msk_cdm.datasets.phi.impact.datasets_phi_impact.load_data_timeline_specimen_phi--examples","title":"Examples","text":"<pre><code>from msk_cdm.datasets import connect_to_db\nfrom msk_cdm.datasets.impact import load_data_timeline_specimen_phi\n\nauth_file = 'path/to/config.txt'\nconnect_to_db(auth_file=auth_file)\n\ndf_timeline_specimen = load_data_timeline_specimen_phi()\ndf = df_timeline_specimen['data']\n\nprint(df.head())\n</code></pre> Source code in <code>msk_cdm/datasets/phi/impact/datasets_phi_impact.py</code> <pre><code>def load_data_timeline_specimen_phi() -&gt; Bunch:\n\"\"\"Load and return the specimen timeline dataset (PHI).\n\n    Returns:\n        data: Dictionary-like object, with the following attributes.\n\n            - **data** : pandas DataFrame\n                The data matrix.\n\n    Examples\n    --------\n    ```python\n    from msk_cdm.datasets import connect_to_db\n    from msk_cdm.datasets.impact import load_data_timeline_specimen_phi\n\n    auth_file = 'path/to/config.txt'\n    connect_to_db(auth_file=auth_file)\n\n    df_timeline_specimen = load_data_timeline_specimen_phi()\n    df = df_timeline_specimen['data']\n\n    print(df.head())\n    ```\n    \"\"\"\n    df = _loader._load_phi_impact_data_timeline_specimen()\n    data = Bunch(data=df)\n    return data\n</code></pre>"},{"location":"reference/datasets/phi_impact/load_data_timeline_specimen_surgery_phi/","title":"<code>load_data_timeline_specimen_surgery_phi</code>","text":"<p>Load and return the specimen-surgery timeline dataset (PHI).</p> <p>Returns:</p> Name Type Description <code>data</code> <code>Bunch</code> <p>Dictionary-like object, with the following attributes.</p> <ul> <li>data : pandas DataFrame     The data matrix.</li> </ul>"},{"location":"reference/datasets/phi_impact/load_data_timeline_specimen_surgery_phi/#msk_cdm.datasets.phi.impact.datasets_phi_impact.load_data_timeline_specimen_surgery_phi--examples","title":"Examples","text":"<pre><code>from msk_cdm.datasets import connect_to_db\nfrom msk_cdm.datasets.impact import load_data_timeline_specimen_surgery_phi\n\nauth_file = 'path/to/config.txt'\nconnect_to_db(auth_file=auth_file)\n\ndf_timeline_specimen_surgery = load_data_timeline_specimen_surgery_phi()\ndf = df_timeline_specimen_surgery['data']\n\nprint(df.head())\n</code></pre> Source code in <code>msk_cdm/datasets/phi/impact/datasets_phi_impact.py</code> <pre><code>def load_data_timeline_specimen_surgery_phi() -&gt; Bunch:\n\"\"\"Load and return the specimen-surgery timeline dataset (PHI).\n\n    Returns:\n        data: Dictionary-like object, with the following attributes.\n\n            - **data** : pandas DataFrame\n                The data matrix.\n\n    Examples\n    --------\n    ```python\n    from msk_cdm.datasets import connect_to_db\n    from msk_cdm.datasets.impact import load_data_timeline_specimen_surgery_phi\n\n    auth_file = 'path/to/config.txt'\n    connect_to_db(auth_file=auth_file)\n\n    df_timeline_specimen_surgery = load_data_timeline_specimen_surgery_phi()\n    df = df_timeline_specimen_surgery['data']\n\n    print(df.head())\n    ```\n    \"\"\"\n    df = _loader._load_phi_impact_data_timeline_specimen_surgery()\n    data = Bunch(data=df)\n    return data\n</code></pre>"},{"location":"reference/datasets/phi_impact/load_data_timeline_surgery_phi/","title":"<code>load_data_timeline_surgery_phi</code>","text":"<p>Load and return the surgery timeline dataset (PHI).</p> <p>Returns:</p> Name Type Description <code>data</code> <code>Bunch</code> <p>Dictionary-like object, with the following attributes.</p> <ul> <li>data : pandas DataFrame     The data matrix.</li> </ul>"},{"location":"reference/datasets/phi_impact/load_data_timeline_surgery_phi/#msk_cdm.datasets.phi.impact.datasets_phi_impact.load_data_timeline_surgery_phi--examples","title":"Examples","text":"<pre><code>from msk_cdm.datasets import connect_to_db\nfrom msk_cdm.datasets.impact import load_data_timeline_surgery_phi\n\nauth_file = 'path/to/config.txt'\nconnect_to_db(auth_file=auth_file)\n\ndf_timeline_surgery = load_data_timeline_surgery_phi()\ndf = df_timeline_surgery['data']\n\nprint(df.head())\n</code></pre> Source code in <code>msk_cdm/datasets/phi/impact/datasets_phi_impact.py</code> <pre><code>def load_data_timeline_surgery_phi() -&gt; Bunch:\n\"\"\"Load and return the surgery timeline dataset (PHI).\n\n    Returns:\n        data: Dictionary-like object, with the following attributes.\n\n            - **data** : pandas DataFrame\n                The data matrix.\n\n    Examples\n    --------\n    ```python\n    from msk_cdm.datasets import connect_to_db\n    from msk_cdm.datasets.impact import load_data_timeline_surgery_phi\n\n    auth_file = 'path/to/config.txt'\n    connect_to_db(auth_file=auth_file)\n\n    df_timeline_surgery = load_data_timeline_surgery_phi()\n    df = df_timeline_surgery['data']\n\n    print(df.head())\n    ```\n    \"\"\"\n    df = _loader._load_phi_impact_data_timeline_surgery()\n    data = Bunch(data=df)\n    return data\n</code></pre>"},{"location":"reference/datasets/phi_impact/load_data_timeline_treatment_phi/","title":"<code>load_data_timeline_treatment_phi</code>","text":"<p>Load and return the treatment timeline dataset (PHI).</p> <p>Returns:</p> Name Type Description <code>data</code> <code>Bunch</code> <p>Dictionary-like object, with the following attributes.</p> <ul> <li>data : pandas DataFrame     The data matrix.</li> </ul>"},{"location":"reference/datasets/phi_impact/load_data_timeline_treatment_phi/#msk_cdm.datasets.phi.impact.datasets_phi_impact.load_data_timeline_treatment_phi--examples","title":"Examples","text":"<pre><code>from msk_cdm.datasets import connect_to_db\nfrom msk_cdm.datasets.impact import load_data_timeline_treatment_phi\n\nauth_file = 'path/to/config.txt'\nconnect_to_db(auth_file=auth_file)\n\ndf_timeline_treatment = load_data_timeline_treatment_phi()\ndf = df_timeline_treatment['data']\n\nprint(df.head())\n</code></pre> Source code in <code>msk_cdm/datasets/phi/impact/datasets_phi_impact.py</code> <pre><code>def load_data_timeline_treatment_phi() -&gt; Bunch:\n\"\"\"Load and return the treatment timeline dataset (PHI).\n\n    Returns:\n        data: Dictionary-like object, with the following attributes.\n\n            - **data** : pandas DataFrame\n                The data matrix.\n\n    Examples\n    --------\n    ```python\n    from msk_cdm.datasets import connect_to_db\n    from msk_cdm.datasets.impact import load_data_timeline_treatment_phi\n\n    auth_file = 'path/to/config.txt'\n    connect_to_db(auth_file=auth_file)\n\n    df_timeline_treatment = load_data_timeline_treatment_phi()\n    df = df_timeline_treatment['data']\n\n    print(df.head())\n    ```\n    \"\"\"\n    df = _loader._load_phi_impact_data_timeline_treatment()\n    data = Bunch(data=df)\n    return data\n</code></pre>"},{"location":"reference/datasets/phi_impact/load_data_timeline_tumor_sites_phi/","title":"<code>load_data_timeline_tumor_sites_phi</code>","text":"<p>Load and return the tumor sites timeline dataset (PHI).</p> <p>Returns:</p> Name Type Description <code>data</code> <code>Bunch</code> <p>Dictionary-like object, with the following attributes.</p> <ul> <li>data : pandas DataFrame     The data matrix.</li> </ul>"},{"location":"reference/datasets/phi_impact/load_data_timeline_tumor_sites_phi/#msk_cdm.datasets.phi.impact.datasets_phi_impact.load_data_timeline_tumor_sites_phi--examples","title":"Examples","text":"<pre><code>from msk_cdm.datasets import connect_to_db\nfrom msk_cdm.datasets.impact import load_data_timeline_tumor_sites_phi\n\nauth_file = 'path/to/config.txt'\nconnect_to_db(auth_file=auth_file)\n\ndf_timeline_tumor_sites = load_data_timeline_tumor_sites_phi()\ndf = df_timeline_tumor_sites['data']\n\nprint(df.head())\n</code></pre> Source code in <code>msk_cdm/datasets/phi/impact/datasets_phi_impact.py</code> <pre><code>def load_data_timeline_tumor_sites_phi() -&gt; Bunch:\n\"\"\"Load and return the tumor sites timeline dataset (PHI).\n\n    Returns:\n        data: Dictionary-like object, with the following attributes.\n\n            - **data** : pandas DataFrame\n                The data matrix.\n\n    Examples\n    --------\n    ```python\n    from msk_cdm.datasets import connect_to_db\n    from msk_cdm.datasets.impact import load_data_timeline_tumor_sites_phi\n\n    auth_file = 'path/to/config.txt'\n    connect_to_db(auth_file=auth_file)\n\n    df_timeline_tumor_sites = load_data_timeline_tumor_sites_phi()\n    df = df_timeline_tumor_sites['data']\n\n    print(df.head())\n    ```\n    \"\"\"\n    df = _loader._load_phi_impact_data_timeline_tumor_sites()\n    data = Bunch(data=df)\n    return data\n</code></pre>"},{"location":"reference/user-guide/airflow/","title":"Airflow","text":"<p>A DAG (Directed Acyclic Graph) is the core concept of Airflow, collecting Tasks together, organized with dependencies and relationships to say how they should run. Airflow DAGs can be deployed on tllihpcmind6 and other MSK-MIND servers.</p>"},{"location":"reference/user-guide/airflow/#steps-to-running-a-dag","title":"Steps to Running a DAG","text":"<ul> <li>Clone the DAG repo from cdm-dags and create a new branch.</li> <li>Duplicate the DAG template file <code>cdm_TEMPLATE.py</code> and change the filename (no spaces)</li> <li>Within your new DAG file, change <code>&lt;&lt;&lt;&lt;&lt;CREATE DAG ID&gt;&gt;&gt;&gt;&gt;</code> to the exact filename, minus .py</li> <li>Change <code>&lt;&lt;&lt;&lt;&lt;YOUR EMAIL&gt;&gt;&gt;&gt;&gt;</code> to your email to receive updates on errors</li> <li>Write your DAG</li> <li>From the command line, perform chmod 777 on your DAG file <code>chmod 777 /mind_data/&lt;my_workspace&gt;/cdm-dags/my_dag.py</code></li> <li>Copy the DAG you created in your workspace, and paste into the shared Airflow folders (ex. <code>cp /mind_data/&lt;my_workspace&gt;/cdm-dags/my_dag.py /mind_data/airflow/dags/curation/my_dag.py</code>) </li> <li>AVOID MODIFYING FILES WITHIN THE SHARED AIRFLOW FOLDER. THIS CAN RESULT IN WORK STOPPAGE FOR ALL USERS.</li> <li>After a few moments, the DAG should appear in the Airflow UI</li> <li>Once you have a functioning dag, commit the dag changes back to the cdm-dag repo. Merge your branch if needed.</li> </ul>"},{"location":"reference/user-guide/conda-cheatsheet/","title":"Conda Cheatsheet","text":"<p>Command line package and environment manager Learn to use conda in 30 minutes at bit.ly/tryconda</p>"},{"location":"reference/user-guide/conda-cheatsheet/#getting-started","title":"Getting Started","text":""},{"location":"reference/user-guide/conda-cheatsheet/#setting-up-conda-on-your-machine","title":"Setting up Conda on your machine","text":"<p>When using <code>tllihpcmind6</code>, initialize Conda by <code>source ~/.bashrc</code> <pre><code>$ source ~/.bashrc\n\n$ which conda\n/gpfs/mskmind_ess/&lt;YOUR_USER_NAME&gt;/miniconda/bin/conda\n\n$ conda update conda\n\n$ conda --version\n\n$ conda --help\n\n$ conda list\n</code></pre></p>"},{"location":"reference/user-guide/conda-cheatsheet/#creating-a-virtual-environment","title":"Creating a Virtual Environment","text":"<p>A virtual environment can be defined and created through an <code>environment.yml</code> file. For example, the Conda environment <code>conda-env-cdm</code> can be created with this snippet included in <code>environment.yml</code> <pre><code>name: conda-env-cdm\nchannels:\n  - pytorch\n  - nvidia\n  - conda-forge\ndependencies:\n   - python == 3.10\n   - pandas\n   - pyyaml\n   - requests\n   - setuptools_scm\n   - setuptools\n   - pip\n   - pip: \n       - git+https://github.com/clinical-data-mining/msk_cdm.git\n   - minio\n</code></pre></p>"},{"location":"reference/user-guide/conda-cheatsheet/#conda-basics","title":"Conda Basics","text":"<ul> <li><code>conda info</code>: Verify conda is installed, check version number</li> <li><code>conda update conda</code>: Update conda to the current version</li> <li><code>conda install PACKAGENAME</code>: Install a package included in Anaconda</li> <li><code>spyder</code>: Run a package after install, example Spyder*</li> <li><code>conda update PACKAGENAME</code>: Update any installed program</li> <li><code>COMMANDNAME --help</code>: Command line help<ul> <li><code>conda install --help</code></li> </ul> </li> </ul>"},{"location":"reference/user-guide/conda-cheatsheet/#using-environments","title":"Using Environments","text":"<ul> <li><code>conda create --name py35 python=3.5</code>: Create a new environment named py35, install Python 3.5</li> <li><code>conda env create -f environment.yaml</code>: Create a new environment with specifications in <code>environment.yaml</code> (See Start-up)</li> <li><code>conda activate py35</code>: Activate the new environment to use it</li> <li><code>conda env list</code>: Get a list of all my environments, active environment is shown with *</li> <li><code>conda list</code>: List all packages and versions installed in active environment</li> <li><code>conda env remove --name bio-env</code>: Delete an environment and everything in it</li> <li><code>conda deactivate</code>: Deactivate the current environment</li> </ul>"},{"location":"reference/user-guide/conda-cheatsheet/#installing-packages","title":"Installing Packages","text":"<p>Anaconda includes both the Python and R programming languages, most of the common Python libraries used in science and engineering (including NumPy, SciPy, Matplotlib, and pandas), and many commonly used R packages (https://anaconda.org/).</p> <ul> <li><code>conda install -c anaconda pandas</code>: Install Pandas into your activate environment</li> </ul>"},{"location":"reference/user-guide/data-query-quick-start/","title":"CDM Data Query Quick Start Guide","text":"<p>This repo offers several Jupyter Notebooks that will help query and transform data. Here's how to get started!</p>"},{"location":"reference/user-guide/data-query-quick-start/#create-virtual-environment-from-requirementstxt-file","title":"Create virtual environment from requirements.txt file","text":"<p>At the command line, create a virtual envirnoment called <code>env</code> <pre><code>python3 -m venv env\n</code></pre></p> <p>Then, activate the virtual env: <pre><code>source env/bin/activate\n</code></pre></p> <p>If it's been awhile (or the first time), update pip: <pre><code>&lt;path-to-repo&gt;/cdm-utilities/env/bin/python3 -m pip install --upgrade pip\n</code></pre></p> <p>Once activated, install the packages listed in the <code>requirements.txt</code> file <pre><code>python3 -m pip install -r requirements.txt\n</code></pre></p> <p>After packages have been installed, run <code>jupyter-lab</code> (make sure a web browser is open) and start prototyping!</p>"},{"location":"reference/user-guide/data-query-quick-start/#query-data","title":"Query Data","text":"<p>With your environment set up, this Jupyter Notebook will show you how to query data from our Dremio instance.</p> <p>To understand how to use our data (better), check out the data dictionary.</p>"},{"location":"reference/user-guide/git-branch-management/","title":"Git Cheatsheet","text":""},{"location":"reference/user-guide/git-branch-management/#basics","title":"Basics","text":"<ul> <li><code>git help &lt;command&gt;</code>: get help for a git command</li> <li><code>git init</code>: creates a new git repo, with data stored in the .git directory</li> <li><code>git status</code>: tells you what\u2019s going on</li> <li><code>git add &lt;filename&gt;</code>: adds files to staging area</li> <li><code>git commit</code>: creates a new commit<ul> <li>`Write good commit messages!</li> <li>`Even more reasons to write good commit messages!</li> </ul> </li> <li><code>git log</code>: shows a flattened log of history</li> <li><code>git log --all --graph --decorate</code>: visualizes history as a DAG</li> <li><code>git diff &lt;filename&gt;</code>: show changes you made relative to the staging area</li> <li><code>git diff &lt;revision&gt; &lt;filename&gt;</code>: shows differences in a file between snapshots</li> <li><code>git checkout &lt;revision&gt;</code>: updates HEAD and current branch</li> </ul>"},{"location":"reference/user-guide/git-branch-management/#branching-and-merging","title":"Branching and merging","text":"<ul> <li><code>git branch</code>: shows branches</li> <li><code>git branch &lt;name&gt;</code>: creates a branch</li> <li><code>$ git branch -d [name_of_your_new_branch]</code>: Delete a branch on your local filesystem</li> <li><code>git checkout -b &lt;name&gt;</code>: creates a branch and switches to it</li> <li><code>git merge &lt;revision&gt;</code>: merges into current branch</li> <li><code>git mergetool</code>: use a fancy tool to help resolve merge conflicts</li> <li><code>git rebase</code>: rebase set of patches onto a new base</li> </ul>"},{"location":"reference/user-guide/git-branch-management/#remotes","title":"Remotes","text":"<ul> <li><code>git remote</code>: list remotes</li> <li><code>git remote add &lt;name_of_your_remote&gt; &lt;name_of_your_new_branch&gt;</code>: add a remote</li> <li><code>git push &lt;remote&gt; &lt;local branch&gt;:&lt;remote branch&gt;</code>: send objects to remote, and update remote reference<ul> <li><code>$ git push origin [name_of_your_new_branch]</code></li> </ul> </li> <li><code>git branch --set-upstream-to=&lt;remote&gt;/&lt;remote branch&gt;</code>: set up correspondence between local and remote branch</li> <li><code>git fetch</code>: retrieve objects/references from a remote</li> <li><code>git pull</code>: same as git fetch; git merge</li> <li><code>git clone</code>: download repository from remote</li> </ul>"},{"location":"reference/user-guide/git-branch-management/#undo","title":"Undo","text":"<ul> <li><code>git commit --amend</code>: edit a commit\u2019s contents/message</li> <li><code>git reset HEAD &lt;file&gt;</code>: unstage a file</li> <li><code>git checkout -- &lt;file&gt;</code>: discard changes</li> </ul>"},{"location":"reference/user-guide/git-branch-management/#advanced-git","title":"Advanced Git","text":"<ul> <li><code>git config</code>: Git is highly customizable</li> <li><code>git clone --depth=1</code>: shallow clone, without entire version history</li> <li><code>git add -p</code>: interactive staging</li> <li><code>git rebase -i</code>: interactive rebasing</li> <li><code>git blame</code>: show who last edited which line</li> <li><code>git stash</code>: temporarily remove modifications to working directory</li> <li><code>git bisect</code>: binary search history (e.g. for regressions)</li> <li><code>.gitignore</code>: specify intentionally untracked files to ignore</li> </ul>"},{"location":"reference/user-guide/git-branch-management/#references","title":"References","text":"<ol> <li>MIT lecture on version control</li> </ol>"},{"location":"reference/user-guide/htcondor/","title":"HTCondor","text":""},{"location":"reference/user-guide/htcondor/#htcondor-guide","title":"HTCondor Guide","text":"<p>This guide provides instructions for using HTCondor, including cluster setup, job submission, troubleshooting, and useful commands.</p>"},{"location":"reference/user-guide/htcondor/#cluster-setup","title":"Cluster Setup","text":""},{"location":"reference/user-guide/htcondor/#pool-1-research","title":"Pool 1 - Research","text":"<ul> <li>Central manager node: <code>pllimskhpc1</code></li> <li>Submit and Execute nodes: <code>pllimskhpc123</code></li> </ul>"},{"location":"reference/user-guide/htcondor/#pool-2-engineering","title":"Pool 2 - Engineering","text":"<ul> <li>Central manager node: <code>pllimsksparky1</code></li> <li>Submit and Execute nodes: <code>pllimsksparky1234</code></li> </ul>"},{"location":"reference/user-guide/htcondor/#example-submit-file","title":"Example Submit File","text":"<p>Below is an example <code>.sub</code> file for submitting a training job with HTCondor:</p> <pre><code># HTCondor submit file to train RoBERTa\nuniverse = vanilla\nexecutable = run_irae_nlp_clinical_longformer.sh\narguments = $(Process)\n\n# Specify job length: \"short\" (~12 hrs), \"medium\" (~24 hrs), \"long\" (~7 days)\n+GPUJobLength = \"short\"\n\n# GPU and memory requirements\nrequest_gpus = 1\nrequest_memory = 10GB\n\n# Output, log, and error file paths\noutput = logs/log.infer.$(Cluster)_$(Process).out\nlog = logs/log.infer.$(Cluster)_$(Process).log\nerror = logs/log.infer.$(Cluster)_$(Process).err\n\n# Submit job\nqueue 1\n</code></pre>"},{"location":"reference/user-guide/htcondor/#example-workflow","title":"Example Workflow","text":"<ol> <li>Prepare an executable (e.g., shell script, Python script, or Docker container) and a <code>.sub</code> file.</li> <li>Submit your job:    <pre><code>condor_submit submit.sub\n</code></pre></li> <li>Monitor job status:    <pre><code>condor_q\n</code></pre></li> <li>View detailed job analysis:    <pre><code>condor_q -better-analyze &lt;job_id&gt;\n</code></pre></li> <li>Check output and logs (specified in the <code>.sub</code> file).</li> <li>Terminate a job:    <pre><code>condor_rm &lt;job_id&gt;\n</code></pre></li> </ol>"},{"location":"reference/user-guide/htcondor/#common-htcondor-commands","title":"Common HTCondor Commands","text":""},{"location":"reference/user-guide/htcondor/#check-nodes","title":"Check Nodes","text":"<pre><code>condor_status\ncondor_status -compact\ncondor_status -compact -constraint 'TotalSlotGpus &gt; 0'\ncondor_status -claimed\n</code></pre>"},{"location":"reference/user-guide/htcondor/#check-jobs","title":"Check Jobs","text":"<pre><code>condor_q\ncondor_q -all\ncondor_history\n</code></pre>"},{"location":"reference/user-guide/htcondor/#debugging-jobs","title":"Debugging Jobs","text":"<pre><code>condor_q -better-analyze &lt;job_id&gt;\n</code></pre>"},{"location":"reference/user-guide/htcondor/#job-submission-notes","title":"Job Submission Notes","text":"<ul> <li> <p>To prevent overloading the cluster, set the following properties in your submit file:</p> <ul> <li><code>max_idle</code>: Maximum number of idle jobs in the queue.</li> <li><code>max_materialize</code>: Maximum number of jobs to execute simultaneously.</li> </ul> </li> <li> <p>To ensure compatibility with shared filesystems: <pre><code>Requirements = TARGET.UidDomain == \"mskcc.org\" &amp;&amp; \\\n               TARGET.FileSystemDomain == \"mskcc.org\"\n</code></pre></p> </li> <li> <p>To disable specific nodes: <pre><code>Requirements = (Machine != \"pllimsksparky2.mskcc.org\")\n</code></pre></p> </li> </ul>"},{"location":"reference/user-guide/htcondor/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Exec format error: Ensure the script starts with the correct shebang (e.g., <code>#!/bin/bash</code> for Bash scripts).</li> <li>MemoryError: Increase memory allocation in the submit file:   <pre><code>request_memory = 16GB\n</code></pre></li> <li>Job stuck in hold: Use the following command to find the issue:   <pre><code>condor_q -l\n</code></pre>   Check the <code>HoldReason</code> key for details.</li> </ul>"},{"location":"reference/user-guide/htcondor/#useful-guides","title":"Useful Guides","text":"<ul> <li>HTCondor Full Manual</li> <li>Running Jobs Steps</li> <li>Submitting Multiple Jobs</li> <li>GPU Jobs</li> <li>Using a Shared Filesystem</li> </ul> <p>Caution: Always test your submit file and program with a small number of jobs before scaling up to prevent resource waste and low submission priority. Use <code>condor_submit -dry-run</code> for debugging.</p>"},{"location":"reference/user-guide/jupyterhub/","title":"JupyterHub","text":""},{"location":"reference/user-guide/jupyterhub/#location","title":"Location","text":"<p>JupyterHub URL for CDM Development</p> <p>Login: MSK credentials. If you are unable to log in, please contact one of the MIND engineers to have you added as a linux user of the pllimsksparky2 machine and you must log into this machine once using the terminal in order to set up your home directory. The first time you log in, jupyterhub may take a minute or so to spawn the user server and jupyter lab environment. Since an MSK self-signed certificate is being used, your browser may warn you that the connection is insecure, but the connection is secure (https encrpyted) and only lacks trust verification with a third-party certificate authority which is not needed for security in the internal MSK network. On Chrome click on the 'Advanced' button to continue to proceed to using the web application.\u00a0</p> <p>Base Directory: The directory under which jupyterhub's file view launches is <code>/mind_data/shared_data_folder</code>. You may choose to create a sub-directory under this directory with your username and store your notebooks in there. You may control access to your notebooks via linux file permissions. Please contact one of the MIND engineers to set up the sub-directory for you.</p>"},{"location":"reference/user-guide/jupyterhub/#creating-python-virtual-environments","title":"Creating Python Virtual Environments","text":"<p>Note: These instructures are for creating virtual environments in Jupyterhub only. For Python environments on the terminal, please see Python.</p> <p>Virtual environments [ref1] help you to isolate your execution environment from other notebooks and other users. You may use virtual environment to select specific version of python and its of libraries without having to worry about collisions. Follow these instructions below to setup, test and teardown your virtual environments for notebooks on Jupyterhub.</p>"},{"location":"reference/user-guide/jupyterhub/#setup","title":"Setup","text":"<p>Open a terminal in your Jupyter environment by selecting File -&gt; New -&gt; Terminal and execute the following commands.\u00a0</p> <p>$ cd [LOCATION-WHERE-YOU-WANT-TO-CREATE-THE-VIRTUAL-ENV]</p>"},{"location":"reference/user-guide/jupyterhub/#creating-a-virtual-environment","title":"Creating a Virtual Environment","text":"<pre><code># create the virtual environment\npython3 -m venv my-test-venv\n\n# activate the virtual environment\n$ source my-test-venv/bin/activate\n\n# upgrade pip\n$ python3 -m pip install --upgrade pip\n\n# install ipykernel\n$ pip install ipykernel\n\n# Register this env with jupyter lab. It will now show up in the\n# launcher &amp; kernels list once you refresh the page\n$ python3 -m ipykernel install --user --name my-test-venv --display-name \"my test virtual env\"\n\n# List kernels to ensure it was created successfully\n$ jupyter kernelspec list\n\nmy-test-venv    /home/pashaa/.local/share/jupyter/kernels/my-test-venv\npython3      /gpfs/mskmindhdp_emc/sw/env/share/jupyter/kernels/python3\n\n# for testing purposes, install a specific package into this environment\n$ pip install fire\n\n# ensure the package installed successfully\n$ pip list | grep fire\n\nfire                0.4.0\n\n# deactivate the virtual environment in the terminal\n$ deactivate\n</code></pre>"},{"location":"reference/user-guide/jupyterhub/#test","title":"Test","text":"<p>Now, apply the new kernel to your notebook by first selecting the default kernel (which is typically \"Python 3\") from the top right corner of your notebook and then selecting your new kernel \"my test virtual env\".\u00a0NOTE:\u00a0It may take a minute for the drop-down list to update.\u00a0</p> <p>Test your new environment by running the following commands in your notebook code cells.\u00a0</p> <p>Verify that you are using python from your virtual env <pre><code>import sys\nprint(sys.executable)\n\n/gpfs/mskmindhdp_emc/user/pashaa/my-test-venv/bin/python\n</code></pre> Verify you are able to import the test package and that it belongs to your virtual env <pre><code>import fire\nprint(fire.__file__)\nprint(fire.__version__)\n\n'/gpfs/mskmindhdp_emc/user/pashaa/my-test-venv/lib64/python3.6/site-packages/fire/__init__.py'\n 0.4.0\n</code></pre> Alternatively, you can check your dependencies by listing the installed libraries, or checking the details of a specific library. <pre><code>%pip list\nor\n%pip show pip\n</code></pre> You can install libraries in your virtual env within your notebook by using the % operator. <pre><code>%pip install &lt;new-lib&gt;\n</code></pre></p>"},{"location":"reference/user-guide/jupyterhub/#teardown","title":"Teardown","text":"<p>Execute the following commands in your jupyter terminal.</p>"},{"location":"reference/user-guide/jupyterhub/#uninstall-virtual-environment","title":"Uninstall Virtual Environment","text":"<pre><code>$ jupyter kernelspec uninstall my-test-venv\n$ rm -rf my-test-venv\n</code></pre>"},{"location":"reference/user-guide/jupyterhub/#creating-r-virtual-environments","title":"Creating R Virtual Environments","text":""},{"location":"reference/user-guide/jupyterhub/#setup_1","title":"Setup","text":"<p>R packages are installed into libraries, which are directories in the file system containing a subdirectory for each package installed there [ref 2]. Users can have one or more libraries, normally specified by the environment variable R_LIBS_USER, if the corresponding directory actually exists (which by default it will not).</p> <p>Users may set up separate directories for these libraries for separate projects and install project specific libraries into these directories as shown below.\u00a0</p> <p>First, use a terminal to create a project directory in your user space. <pre><code>$ mkdir /gpfs/mskmind_ess/pashaa/my_R_libs_prj1\n</code></pre> Next, open a Jupyter notebook in Jupyterhub, select the system-wide R kernel and configure R to use this directory as a user libs directory. \u00a0</p>"},{"location":"reference/user-guide/jupyterhub/#r-script-in-jupyter-irkernel","title":"R Script in Jupyter IRKernel","text":"<pre><code>[1] Sys.getenv('R_LIBS_USER')\n\n     '~/R/x86_64-pc-linux-gnu-library/4.0'\n\n[2] .libPaths()\n\n     '/opt/R/4.0.5/lib/R/library'\n\n[3] Sys.setenv(R_LIBS_USER=\"/gpfs/mskmind_ess/pashaa/my_R_libs_prj1\")\n[4] .libPaths(c( Sys.getenv('R_LIBS_USER'), .libPaths()  ))\n</code></pre>"},{"location":"reference/user-guide/jupyterhub/#test_1","title":"Test","text":"<p>Test your IRKernel environment. </p> <p>NOTE: You may change the order of the paths to make R pick libraries from the system-wide project directory first by changing the order of the libraries in the .libPaths() setup call above. But, in general you may find it beneficial to give precedence to the libraries that you have installed for your project.\u00a0 <pre><code>[5] Sys.getenv('R_LIBS_USER')\n\n'/gpfs/mskmind_ess/pashaa/my_R_libs_prj1'\n\n[6] .libPaths()\n\n     '/gpfs/mskmind_ess/pashaa/my_R_libs_prj1''/opt/R/4.0.5/lib/R/library'\n</code></pre> List the current set of packages, then install a package and list packages again to check if the package has been installed. Note when loading the packages, you must specify the \"lib.loc\" library location to ensure you are loading from your library.\u00a0 <pre><code>[7] my_packages &lt;- library()$results[,1]\n[8] print(my_packages)\n\n        [1] \"bslib\"         \"cachem\"        \"commonmark\"    \"httpuv\"       \n        [5] \"jquerylib\"     \"later\"         \"promises\"      \"sass\"         \n        [9] \"sourcetools\"   \"xtable\"        \"askpass\"       \"assertthat\"   \n        ...\n        [125] \"utf8\"          \"utils\"         \"uuid\"          \"vctrs\"        \n        [129] \"viridisLite\"   \"vroom\"         \"withr\"         \"xfun\"         \n        [133] \"yaml\"\n\n[9] install.packages('shiny', lib=\"/gpfs/mskmind_ess/pashaa/my_R_libs_prj1\")\n\n        Installing package into \u2018/gpfs/mskmind_ess/pashaa/my_R_libs_prj1\u2019\n        (as \u2018lib\u2019 is unspecified)\n\n[10] my_packages &lt;- library()$results[,1]\n[11] print(my_packages)\n\n        [1] \"bslib\"         \"cachem\"        \"commonmark\"    \"httpuv\"       \n        [5] \"jquerylib\"     \"later\"         \"promises\"      \"sass\"         \n        [9] \"sourcetools\"   \"xtable\"        \"askpass\"       \"assertthat\"   \n        ...\n        [125] \"utf8\"          \"utils\"         \"uuid\"          \"vctrs\"        \n        [129] \"viridisLite\"   \"vroom\"         \"withr\"         \"xfun\"         \n        [133] \"yaml\"          \"shiny\"\n\n[12] library(\"shiny\", lib.loc=\"/gpfs/mskmind_ess/pashaa/my_R_libs_prj1\")\n</code></pre> On the terminal, you may check to see if the package and its dependencies have been installed in your project location.\u00a0</p>"},{"location":"reference/user-guide/jupyterhub/#terminal","title":"Terminal","text":"<p><pre><code>$ ls /gpfs/mskmind_ess/pashaa/my_R_libs_prj1\nbslib  cachem  commonmark  httpuv  jquerylib  later  promises  sass  shiny  sourcetools  xtable\n</code></pre> More than 1 project directory may be set up in this manner. However, it is advisable to configure R to use one user lib directory at a time so there is no confusion as to where libraries are being installed or uninstalled from.\u00a0</p>"},{"location":"reference/user-guide/jupyterhub/#teardown_1","title":"Teardown","text":"<p>You may uninstall a package as follows.</p>"},{"location":"reference/user-guide/jupyterhub/#r-script-in-jupyter-irkernel_1","title":"R Script in Jupyter IRKernel","text":"<p><pre><code>[12] remove.packages('shiny')\n</code></pre> For a full teardown, reset the R configuration and remove the project directory.\u00a0 <pre><code>[13] Sys.setenv(R_LIBS_USER=\"~/R/x86_64-pc-linux-gnu-library/4.0\")\n[14] .libPaths(.libPaths()[2])\n</code></pre> <pre><code>$ rm -rf /gpfs/mskmind_ess/pashaa/my_R_libs_prj1\n</code></pre></p>"},{"location":"reference/user-guide/jupyterhub/#pre-installed-r-packages","title":"Pre-installed R Packages","text":"<p>For ease of use there is already a set of commonly used packages installed in the base R installation on pllimsksparky2. In addition to the libraries that come with R, we have explicitly installed the following:</p> <ul> <li>tidyverse</li> <li>tidyr</li> <li>ggplot2</li> <li>tidymodels</li> <li>mgcv</li> <li>nlme</li> <li>car</li> <li>randomForest</li> <li>multcomp</li> <li>glmnet</li> <li>survival</li> <li>caret</li> <li>shiny</li> <li>rmarkdown</li> <li>BiocManager</li> <li>flowCore</li> </ul> <p>Chances are that even if the package you're looking for isn't on the above list, it was probably installed as a dependency. To see a complete list of all installed libraries, open an R terminal and run the following:</p>"},{"location":"reference/user-guide/jupyterhub/#r-script-in-jupyter-irkernel_2","title":"R script in Jupyter IRKernel","text":"<p><pre><code>&gt; print(library()$results[,1])\n</code></pre> If you're interested in using a package and can't install it in a virtual environment due to core system dependencies or any other issues, let the MIND Engineering team know.</p>"},{"location":"reference/user-guide/jupyterhub/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Notebook cells are not executing or are hanging: Simply restart the kernel by selecting menu item Kernel \u2192 Restart Kernel</li> <li>Notbook and terminal not working in Safari: Make sure you have Safari version 15.4 or higher, and add untrusted cert to keychain to make it trusted (https://github.com/jupyterhub/jupyterhub/issues/292)</li> <li>\"Unexpected error while saving file: .ipynb attempt to write a readonly database\": Restart your server by going to File\u2192Hub Control Panel.\u00a0 <li>Trouble initiating Jupyterhub in Safari: Try Google Chrome</li> <li>Trouble loading, saving, or creating new jupyter notebooks: Try restarting the Jupyterhub service by: Going to File -&gt; Hub Control Panel Click Start My Server</li>"},{"location":"reference/user-guide/jupyterhub/#references","title":"References","text":"<ol> <li>Creating a kernel for virtual environments</li> <li>R Admin - Managing Libraries</li> <li>List R packages</li> <li>Using Virtual Environments in Jupyter Notebook and Python</li> </ol> <p>Documentation adapted from MSK-MIND</p>"},{"location":"reference/user-guide/minio/","title":"Minio","text":"<p>Minio is an object store very similar to AWS S3. It allows your data to be remotely accessible via the S3 protocol.</p>"},{"location":"reference/user-guide/minio/#python-api","title":"Python API","text":"<p>Steps to access data using Python Minio client API</p>"},{"location":"reference/user-guide/minio/#1-log-into-minio","title":"1) Log into Minio.","text":"<p>If you get an error \"Expecting a policy to be set for user <code>X</code> or one of their groups\", contact Pasha, Arfath or Kohli, Armaan with the 'X' String and the names of the buckets you need access to so we can give you access to these buckets.</p>"},{"location":"reference/user-guide/minio/#2-create-a-service-account-with-a-simple-access-key-and-the-recommended-secret-key","title":"2) Create a \"Service Account\" with a simple access key and the recommended secret key.","text":"<p>Copy the secret key to clipboard on creation.</p> <p></p>"},{"location":"reference/user-guide/minio/#3-store-your-credentials-in-an-env-file","title":"3) Store your credentials in an env file.","text":""},{"location":"reference/user-guide/minio/#create-env","title":"Create .env","text":"<pre><code># create and populate `env.txt` file\nACCESS_KEY=&lt;ACCESS_KEY&gt;\nSECRET_KEY=&lt;SECRET_KEY&gt;\nCA_CERTS=&lt;PATH_TO&gt;/certificate.crt\nURL_PORT=pllimsksparky3:9000\nBUCKET=cdm-data\n\n\n$ ls -la\ndrwxr-xr-x    4 xxx  xxx   128 Jun  1 11:20 .\ndrwxr-xr-x  121 xxx  xxx  3872 May 26 13:50 ..\n-rw-r--r--    1 xxx  xxx    42 Jun  1 11:20 env.txt\n</code></pre>"},{"location":"reference/user-guide/minio/#4-change-permissions-to-the-dotenv-file-so-only-you-can-read-and-write-to-it","title":"4) Change permissions to the dotenv file so only you can read and write to it.","text":""},{"location":"reference/user-guide/minio/#permissions","title":"Permissions","text":"<pre><code>$ chmod 600 env.txt\n$ ls -la-rw------- 1 xxx xxx 42 Jun 1 11:20 env.txt\n</code></pre>"},{"location":"reference/user-guide/minio/#5-install-the-msk_cdm-python-package","title":"5) Install the <code>msk_cdm</code> python package","text":"<pre><code>$ conda activate &lt;YOUR_CONDA_ENV&gt;\n$ git clone https://github.com/clinical-data-mining/msk_cdm.git\n$ cd msk_cdm\n$ pip install .\n</code></pre>"},{"location":"reference/user-guide/minio/#6-get-the-full-ssl-certifcate-chain-for-the-minio-instance","title":"6) Get the full SSL certifcate chain for the Minio instance","text":""},{"location":"reference/user-guide/minio/#ssl-cert","title":"SSL Cert","text":"<pre><code>openssl s_client -showcerts -verify 5 -connect HOST:PORT &gt; certificate.crt\n\nexample:\nopenssl s_client -showcerts -verify 5 -connect tllihpcmind6:9000 &gt; certificate.crt\n</code></pre> URL HOST PORT https://pllimsksparky3:9001/ pllimsksparky3 9000"},{"location":"reference/user-guide/minio/#7-demo","title":"7) Demo","text":"<p>Try our Jupyter notebook demonstrating how download and upload data to MinIO</p>"},{"location":"reference/user-guide/minio/#access-via-r","title":"Access via R","text":"<p>Data stored in minio can be accessed in R in the following fashion: <pre><code># sample r/minio interface via aws.s3\n\n# load external libs\nlibrary(aws.s3)\nlibrary(httr)\nlibrary(dplyr)\n\n# disable ssl verification (required for the time being)\nhttr::set_config(config( ssl_verifypeer = 0L ))\n\n# params need to be written to environment\nSys.setenv(AWS_SECRET_ACCESS_KEY=\"***\",\nAWS_ACCESS_KEY_ID=\"***\",\nAWS_S3_ENDPOINT=\"tllihpcmind6:9000\",  # server name\nAWS_DEFAULT_REGION=\"\") # region env variable doesn't work must be specified with each call\n\n# list all buckets\nbucketlist(region=\"\")\n\n# get bucket\nb &lt;- aws.s3::get_bucket(bucket='sample-bucket', region=\"\")\nprint(b)\n\n# read csv from minio bucket\nobj &lt;- aws.s3::get_object(object=\"test.csv\",\nbucket='sample-bucket', region=\"\")\ncsvcharobj &lt;- rawToChar(obj)  \ncon &lt;- textConnection(csvcharobj)  \ndata &lt;- read.csv(file = con)\nclose(con)\n# head(data, n=2)\n\n\n# write csv file back to minio\ncon &lt;- rawConnection(raw(0), \"r+\")\nwrite.csv(data, con)\naws.s3::put_object(file = rawConnectionValue(con),\nbucket = \"sample-bucket\", object = \"test_write.csv\", region=\"\")\nclose(con)\n</code></pre></p>"},{"location":"reference/user-guide/minio/#troubleshooting","title":"Troubleshooting","text":"<ol> <li> <p>If while making a minio client call (mc) you get a certificate error, use '\u2013insecure' in your call to disable certificate verification. The connection will still be encrypted, ony that the certificate verification process with a certificate authority will be skipped in the SSL protocol. This is sometimes necessary for self-signed certificates. Our certificate is a self-signed certificate issued by MSK Open Systems .</p> </li> <li> <p>mc:  Unable to initialize new alias from the provided credentials. Get \"https://pllimsksparky3:9006\": dial tcp: lookup pllimsksparky3 on 140.163.135.19:53: server misbehaving. <li>Solution: Ping the server to get its IP address and add it to you '/etc/hosts' file as '10.254.130.16 pllimsksparky3'.</li> <p>Documentation adapted from MSK-MIND</p>"},{"location":"reference/user-guide/python-environment/","title":"Python","text":"<p>These instructures are for terminal access only. For Python environments in JupyterHub, please see JupyterHub.</p>"},{"location":"reference/user-guide/python-environment/#virtualenv","title":"Virtualenv","text":"<p>You may create a virtualenv by pointing to a particular version of python. Different versions of Python interpreters are available,</p>"},{"location":"reference/user-guide/python-environment/#python-versions","title":"Python Versions","text":"<p><pre><code>On the phi server:\n\n/mind_data/sw\n\u251c\u2500\u2500 python3.7\n\u251c\u2500\u2500 python3.8\n\u2514\u2500\u2500 python3.9\n\nOn the de-identified servers:\n\n/gpfs/mskmind_ess/sw/\n\u251c\u2500\u2500 python3.7\n\u251c\u2500\u2500 python3.8\n\u2514\u2500\u2500 python3.9\n</code></pre> So, for example you may create a virtual environment with python3.7 as shown below.</p> <p>Note: LD_LIBRARY_PATH needs to be set to include the lib of the version you choose.\u00a0</p>"},{"location":"reference/user-guide/python-environment/#virtualenv_1","title":"virtualenv","text":"<p><pre><code>$ export LD_LIBRARY_PATH=/usr/lib64:/gpfs/mskmind_ess/sw/python3.7/lib\n$ /gpfs/mskmind_ess/sw/python3.7/bin/python3.7 -m venv venv\n$ source venv/bin/activate\n(venv) $ which python\n~/venv/bin/python\n(venv) $ python --version\nPython 3.7.11   \n(venv) $ deactivate\n</code></pre> Install packages with pip : <pre><code>(venv) $ pip install numpy\n</code></pre> N.B., do not use the --user\u00a0 option to pip (otherwise it will install to your home directory).</p>"},{"location":"reference/user-guide/python-environment/#miniconda","title":"Miniconda","text":"<p>Miniconda gives you the Python interpreter itself, along with a command-line tool called conda which operates as a cross-platform package manager geared toward Python packages. You may use Miniconda on the compute servers to manage your own python environments and packages for your projects. This means you do not have to depend on any centralized or system-wide python environment for your projects.\u00a0</p>"},{"location":"reference/user-guide/python-environment/#setup","title":"Setup","text":"<p>Install Miniconda using the instructions below in your ESS sub-directory your mind_data sub-directory</p>"},{"location":"reference/user-guide/python-environment/#miniconda-installation","title":"Miniconda Installation","text":"<p><pre><code># or `cd` into any directory of your choice (e.g. /mind_data/&lt;YOUR_USER_NAME&gt;)\n$ cd /gpfs/mskmind_ess/&lt;YOUR_USER_NAME&gt;/\n\n# Download the python 3.9 (or latest) installer (See Ref 1)\n# NOTE: You can choose different versions of python for different projects later\n# using the conda package manager. This is described in the Test section below.\n# If having issues w/ insecure warnings, add --no-check-certificate flag\n$ wget https://repo.anaconda.com/miniconda/Miniconda3-py39_4.10.3-Linux-x86_64.sh\n\n# Verify Hash\n$ sha256sum Miniconda3-py39_4.10.3-Linux-x86_64.sh\n1ea2f885b4dbc3098662845560bc64271eb17085387a70c2ba3f29fff6f8d52f\n\n$ chmod +x Miniconda3-py39_4.10.3-Linux-x86_64.sh\n\n$ ./Miniconda3-py39_4.10.3-Linux-x86_64.sh\n</code></pre> When prompted for location of installation, point the installer to <code>/gpfs/mskmind_ess/&lt;YOUR_USER_NAME&gt;/miniconda</code>. Make sure the directory does not already exist.</p> <p>Allow the installer to run conda init and select defaults for the rest of the installation.\u00a0</p> <p>Log out of your shell and log back in.\u00a0</p>"},{"location":"reference/user-guide/python-environment/#test","title":"Test","text":"<p>Test your miniconda installation by following the steps below. If these commands execute, your conda environment is set up correctly.\u00a0 <pre><code>$ source ~/.bashrc\n\n$ which conda\n/gpfs/mskmind_ess/&lt;YOUR_USER_NAME&gt;/miniconda/bin/conda\n\n$ conda update conda\n\n$ conda --version\n\n$ conda --help\n\n$ conda list\n</code></pre> You may now create separate python environments and install separate versions of python packages in these environments following the instructions below.\u00a0</p> <p>Create an environment file with the following contents. Note that you can specify the version of python you want to use that is independent of the version of python chosen when installing miniconda.\u00a0</p>"},{"location":"reference/user-guide/python-environment/#conda-environment-file","title":"conda environment file","text":"<pre><code>name: project_1\ndependencies:\n- python=3.9 # or 3.7, 3.10, etc.\n- pip\n- pip:\n    - numpy==1.23.5\n</code></pre> <p>Now create and activate your environment.\u00a0 <pre><code>$ conda env create -f project_1.yml\n\n$ conda activate project_1\n</code></pre> Install additional packages with pip or conda: <pre><code>$ pip install pandas\n\n$ conda install poetry\n</code></pre> N.B., do not <code>pip install</code>\u00a0 packages with the <code>--user</code>\u00a0 option.</p> <p>To deactivate, remove an environment and clean conda cache,\u00a0</p>"},{"location":"reference/user-guide/python-environment/#cleanup-environment","title":"cleanup environment","text":"<pre><code>$ conda deactivate\n\n$ conda remove --name project_1 --all\n\n$ conda clean --all\n</code></pre>"},{"location":"reference/user-guide/python-environment/#teardown","title":"Teardown","text":"<p>To remove miniconda altogether, follow these instructions below. NOTE: With many environment installs and uninstalls, the miniconda dir tends to grow in size, so it would be good to re-install it from time to time.\u00a0</p>"},{"location":"reference/user-guide/python-environment/#uninstall-miniconda","title":"uninstall miniconda","text":"<p><pre><code>$ rm -rf /gpfs/mskmind_ess/&lt;YOUR_USER_NAME&gt;/miniconda\n\n$ rm -rf ~/.conda\n</code></pre> Edit your ~/.bashrc (or .bash_profile) file and remove all lines between, and including these lines.\u00a0 <pre><code># &gt;&gt;&gt; conda initialize &gt;&gt;&gt;\n...\n# &lt;&lt;&lt; conda initialize &lt;&lt;&lt;\n</code></pre></p>"},{"location":"reference/user-guide/python-environment/#references","title":"References","text":"<ol> <li>Miniconda installers</li> <li>Conda Tasks</li> </ol> <p>Documentation adapted from MSK-MIND</p>"},{"location":"reference/user-guide/r-guide/","title":"R User Guide","text":"<p>In a conda env, do</p> <p><code>conda install -c conda-forge r-base</code></p> <p>using default <code>r-base</code> from conda will install old R version (3.6)</p>"},{"location":"reference/user-guide/r-guide/#package-installation","title":"Package Installation","text":"<pre><code># list installed packages\n$ R -e \"installed.packages()\"\n\n# list all packages where an update is available\n$ R -e \"old.packages()\"\n\n# to update only a specific package use install.packages()\n$ R -e \"install.packages(\"plotly\")\"\n\n# to update all packages, without prompts for permission/clarification\n$ R -e \"update.packages(ask = FALSE)\"\n</code></pre> <p>Documentation adapted from MSK-MIND </p>"},{"location":"reference/user-guide/redcap_etl/","title":"Redcap to cBioPortal ETL","text":""},{"location":"reference/user-guide/redcap_etl/#-","title":"---","text":"<p>Step-by-step break down of setting up a Redcap study for data to be exposed on cBioPortal, pulling data from Redcap, and creating configuration files for transforming data into a cBioPortal data format</p> <p>There are three (3) configuration files that can be used for exporting data from Redcap and transforming the data into the cBioPortal format: - API mapping file (Required) - Key variable map file (Required) - Data transformation file (Optional)</p>"},{"location":"reference/user-guide/redcap_etl/#setting-up-redcap-for-the-etl","title":"Setting up Redcap for the ETL","text":""},{"location":"reference/user-guide/redcap_etl/#required-from-redcap-before-starting","title":"Required from Redcap before starting","text":"<ul> <li>A Redcap API token for the study</li> <li>At least one (1) Redcap report</li> <li>These variables in the study codebook:<ul> <li>DOB column</li> <li>record id column</li> <li>de-identified ID column</li> <li>IMPACT sample id column</li> </ul> </li> </ul>"},{"location":"reference/user-guide/redcap_etl/#requirements-for-a-redcap-report","title":"Requirements for a Redcap report","text":"<p>Data on cBioPortal cannot contain PHI such as dates, accession numbers, or identifiable patient IDs. </p> <p>Therefore, the following is needed for each Redcap reports: - The date of birth column, or another reference date. This is only used when the report contains a date attribute that needs to be de-identified. - The de-identified patient ID column. This will map to the IDs used when initially creating a cBioPortal study. - The IMPACT sample id column. Only required for sample-level data.</p>"},{"location":"reference/user-guide/redcap_etl/#ideal-setup-of-redcap-reports","title":"Ideal setup of Redcap reports","text":"<p>Data that will be imported into cBioPortal will need to fit these three categories: - Patient-level summaries (data_clinical_patient.txt) - Sample-level summaries (data_clinical_sample.txt) - Timeline files (data_timeline_specimen.txt)</p> <p>Therefore, Redcap reports will ideally be constructed in a similar way: - Non-repeating instruments, patients (Every row contains data for each patient) - Repeating instruments, samples (Every row contains data for an sample id) - Repeating instruments, events (Every row contains data for a date/time stamp)</p> <p>If Redcap report data does not fit this ideal setup, that's okay! Steps below can be taken to format the data. Once your Redcap study is setup, the following configuration files need to be created. </p>"},{"location":"reference/user-guide/redcap_etl/#configuration-files-for-exporting-data-from-redcap","title":"Configuration files for exporting data from Redcap","text":""},{"location":"reference/user-guide/redcap_etl/#api-mapping-file","title":"API mapping file","text":""},{"location":"reference/user-guide/redcap_etl/#purpose","title":"Purpose","text":"<p>This file is used as high-level instructions for code to perform these functions: - Export the Redcap reports of interest onto your server.  - Decide which reports are needed for transfer and which are to be imported onto cbioPortal - Indicate how Redcap report data should be transformed to fit the cBioPortal format</p>"},{"location":"reference/user-guide/redcap_etl/#columns","title":"Columns","text":"<ul> <li>REPORT_NAME<ul> <li>Name of report on redcap, but can be named anything.</li> </ul> </li> <li>API_ID<ul> <li>API ID number located in the Redcap report summary/edit page</li> </ul> </li> <li>FOR_CBIOPORTAL<ul> <li>Indication if Redcap report will be loaded onto cbioportal or if left as a csv at destination</li> </ul> </li> <li>TIMELINE_LEVEL<ul> <li>Column indicates that the Redcap report will be converted into a cBioPortal timeline file according to the specifications in the data transformation files. Entry for this column should be the filename for the data transformation file.</li> </ul> </li> <li>PATIENT_LEVEL<ul> <li>Column indicates that the Redcap report will be converted into a cBioPortal patient summary file. For direct mapping into a summary file, enter (x). Otherwise, entry for this column should be the filename for the data transformation file. </li> </ul> </li> <li>SAMPLE_LEVEL<ul> <li>Column indicates that the Redcap report will be converted into a cBioPortal sample summary file. For direct mapping into a summary file, enter (x). Otherwise, entry for this column should be the filename for the data transformation file. </li> </ul> </li> <li>TIMELINE_TYPE<ul> <li>Label for the expected cBioPortl timeline file type that the Redcap report will be converted to. Each cBioPortal format file will have to adhere to a specific format. The format types are: treatment, toxicity, and diagnosis (please see the cbioportal docs for more info: cBioPortal Docs)</li> </ul> </li> </ul>"},{"location":"reference/user-guide/redcap_etl/#redcap-key-variable-map-file","title":"Redcap key variable map file","text":""},{"location":"reference/user-guide/redcap_etl/#purpose_1","title":"Purpose","text":"<p>This file is used to identify the patient and sample ID mapping (record_id to a de-identified id), while also using the DOB as a marker for de-identifying dates to age, in days</p>"},{"location":"reference/user-guide/redcap_etl/#columns_1","title":"Columns","text":"<ul> <li>variable<ul> <li>DOB column (col_dte_birth)</li> <li>record id column (col_darwin_id)</li> <li>de-identified ID column (col_sample_id)</li> <li>IMPACT sample id column (col_record_id)</li> </ul> </li> <li>redcap_field<ul> <li>Entries for each attribute defined in Redcap study</li> </ul> </li> </ul>"},{"location":"reference/user-guide/redcap_etl/#data-transformation-files","title":"Data transformation files","text":""},{"location":"reference/user-guide/redcap_etl/#purpose_2","title":"Purpose","text":"<p>This file is used to determine the specifications for how data will be transformed and aggregated to fit the patient-, sample-, or timeline-level data files. </p>"},{"location":"reference/user-guide/redcap_etl/#columns_2","title":"Columns","text":"<ul> <li>COLUMN_NAME_REDCAP<ul> <li>Column name used in Redcap</li> </ul> </li> <li>COLUMN_NAME_CBIOPORTAL<ul> <li>Column name that will be used in cbioportal (\"heading\" name). This name should be all CAPS. For timeline transformations, use START_DATE and END_DATE</li> </ul> </li> <li>KEEP_FOR_CBIOPORTAL<ul> <li>(This column will be deprecated) should be all (x).</li> </ul> </li> <li>TIMELINE<ul> <li>Marker if column shoud be included in a timeline file type. Mark as (x) to include.</li> </ul> </li> <li>AGGREGATE_PATIENT<ul> <li>Marker if column shoud be included in a patient-level file type. Mark as (x) to include. If report transformed with this file is a repeated instrument, notes can be used here to aggregate by other functions (max, min, mean, etc).</li> </ul> </li> <li>AGGREGATE_SAMPLE<ul> <li>Marker if column shoud be included in a sample-level file type. Mark as (x) to include. If report transformed with this file is a repeated instrument, unaligned with sample-level data, notes can be used here to aggregate by other functions (max, min, mean, etc).</li> </ul> </li> <li>MELT_ID_VARS<ul> <li>When melting a Redcap report, (ie. transforming data from patient-level to timeline values) mark the row (x) that indicates the patient ID that will be used.</li> </ul> </li> <li>MELT_VAL_VARS<ul> <li>When melting data, indicate the Redcap report columns to be used as date events. Note that COLUMN_NAME_CBIOPORTAL must be labeled as START_DATE or END_DATE when using.</li> </ul> </li> <li>PIVOT_IDX<ul> <li>(This column will be deprecated) When pivoting a Redcap report (ie. transforming data from repeated-instrument to patient-level values), mark the row (x) that indicates the patient ID that will be used.</li> </ul> </li> <li>PIVOT_SUMMARY<ul> <li>(This column will be deprecated) When pivoting data, mark the rows (x) that will be converted from multiple selection (dropdown, checkbox) into a pivot table of multiple columns of Yes/No</li> </ul> </li> </ul>"},{"location":"reference/user-guide/redcap_etl/#exporting-reports-from-your-redcap-study","title":"Exporting reports from your Redcap study","text":"<p>Once the API mapping configuration file has been created, Redcap reports can be pulled to a destination using:</p> <p><code>data-curation/redcap_tools/redcap_api_report_pull.py</code></p>"},{"location":"reference/user-guide/redcap_etl/#requirements","title":"Requirements","text":"<p>Create a virtual environment containing these packages: - redcap - pandas - numpy - re - argparse</p> <p>Alternatively, after creating a virtual environment, install the required packages using the requirements.txt file using </p> <p><code>pip install -r /path/to/requirements.txt</code></p>"},{"location":"reference/user-guide/redcap_etl/#command-line","title":"Command Line","text":"<pre><code>redcap_api_report_pull.py  \\\n-t REDCAP_API_TOKEN \\\n-u REDCAP_URL \\\n-map API_MAPPING_FILE \\\n-vars KEY_VARIABLE_MAP_FILE \\\n-dest PATH_TO_REDCAP_REPORT_EXPORTS\n</code></pre>"},{"location":"reference/user-guide/redcap_etl/#example","title":"Example","text":"<pre><code>/path_to/venv/bin/python /path_to/redcap_tools/redcap_api_report_pull.py  \\\n-t your_redcap_token \\\n-u https://redcap.mskcc.org/api/ \\\n-map /path/to/mapping_file.csv \\\n-vars /path/to/key_variables_file.csv \\\n-dest /another/path/to/exported/data\n</code></pre>"},{"location":"reference/user-guide/redcap_etl/#formatting-redcap-reports-in-the-cbioportal-format","title":"Formatting Redcap reports in the cBioPortal format","text":"<p>Once data and codebook is exported from Redcap, data must be transformed and formatted into the cBioPortal format before importing. The cBioPortal format consists of three file types: - Patient-level summary - Sample-level summary - Timeline (event) data</p> <p>Depending on the complexity of the Redcap reports being exported (i.e. reports from one instrument [simple] vs. reports from multiple instruments [complex]), built-in or custom made functions can be used. </p>"},{"location":"reference/user-guide/redcap_etl/#built-in-functionality","title":"Built-in functionality","text":""},{"location":"reference/user-guide/redcap_etl/#requirements_1","title":"Requirements","text":"<p>Create a virtual environment containing these packages: - pandas - numpy</p> <p>Alternatively, after creating a virtual environment, install the required packages using the requirements.txt file using </p> <p><code>pip install -r /path/to/requirements.txt</code></p>"},{"location":"reference/user-guide/redcap_etl/#python-function","title":"Python Function","text":"<p><pre><code>class RedcapToCbioportalFormat(fname_report_api, \n                               path_config, \n                               fname_report_map, \n                               fname_variables,\n                               path_save)\n</code></pre> Source Code</p>"},{"location":"reference/user-guide/redcap_etl/#parameters","title":"Parameters:","text":"<pre><code>path_config: (str) Pathname to timeline and summary data transformation files. (TODO: Deprecate. Move this info directly into the API mapping file [fname_report_api])\n\nfname_report_api: (str) Filename for the API mapping file. This file is used as high-level instructions for export Redcap reports and pointing to transformation files.\n\nfname_report_map: (str) Filename for table mapping the Redcap report name to the export Redcap data filename. This typically generatea a file named `*_redcap_report_mapping.tsv`\n\nfname_variables: (str) Filename to the Redcap key variable map file. This is created when setting up the Redcap report export.\n\npath_save: (str) Pathname to save formatted data and cBioPortal header files to be merged.\n</code></pre>"},{"location":"reference/user-guide/redcap_etl/#attributes","title":"Attributes:","text":"<pre><code>create_summary_default(patient_or_sample): \nCreates cBioPortal data and header summary files defined in the the data transformation configuration files. Options for patient_or_sample include `patient` or `sample`. \n\ncreate_timeline_files()\nCreate cBioPortal timeline files that can be directly imported into \"datahub\" \n</code></pre>"},{"location":"reference/user-guide/redcap_etl/#returns","title":"Returns:","text":"<pre><code>summary_manifest_patient.csv\nsummary_manifest_sample.csv\n\nThese manifest files contain a table mapping all patient or sample summary data and cBioPortal headers required to be merged. \n</code></pre>"},{"location":"reference/user-guide/redcap_etl/#example_1","title":"Example","text":"<pre><code>from create_summary_from_redcap_reports import RedcapToCbioportalFormat\n\n\n# Create Redcap formatting object from configuration files and pathnames\nobj_redcap_to_cbio = RedcapToCbioportalFormat(fname_report_api='/path/to/api_map.csv', \n                                              path_config='/path/to/transformation/config/files',\n                                              fname_report_map='/path/to/report_map.csv', \n                                              fname_variables='/path/to/variables.csv', \n                                              path_save='/path/to/headers/and/data')\n# Create out-of-box summary tables from Redcap reports\nobj_redcap_to_cbio.create_summary_default(patient_or_sample='patient')\n\n# Create out-of-box timeline files\nobj_redcap_to_cbio.create_timeline_files()\n</code></pre>"},{"location":"reference/user-guide/redcap_etl/#custom-summary-data-formatting","title":"Custom Summary Data Formatting","text":"<p>If Redcap report data, or any data file requires custom aggregation, cBioPortal formatting can be done manually.  Two files need to be generated:</p> <p>1) The datafile containing summary data 2) The cBioPortal summary header file </p> <p>In addition, one other file needs to be modified or created: 3) summary_manifest_patient.csv and/or summary_manifest_sample.csv</p>"},{"location":"reference/user-guide/redcap_etl/#header-file","title":"Header file","text":"<p>The header file is a dataframe containing 5 columns: 1) <code>label</code>: The text shown on the portal summary page 2) <code>comment</code>: The \"hover-over\" text shown when mouse hovers over widet 3) <code>data_type</code>: Data type of columns (STRING or NUMBER) 4) <code>visible</code>: Mark 0 or 1 to make visible on summary page 5) <code>heading</code>: Column names contained in the datafile (below). These columns MUST match the datafile!</p> <p>For summary and header files, the <code>heading</code> column must be <code>PATIENT_ID</code> or <code>SAMPLE_ID</code>. In addition, the <code>label</code> column must contain <code>#Patient Identifier</code> or <code>#Sample Identifier</code>.</p>"},{"location":"reference/user-guide/redcap_etl/#datafile","title":"Datafile","text":"<p>The datafile contains data to be imported. The column header (first row) must match the <code>heading</code> values in the header file.</p> <p>For summary files, the first column must be <code>PATIENT_ID</code> or <code>SAMPLE_ID</code></p>"},{"location":"reference/user-guide/redcap_etl/#modifying-the-manifest-file","title":"Modifying the manifest file","text":"<p>The patient and sample manifest files contain three columns: 1) <code>REPORT_NAME</code>: The name of the summary file created. Typically taken from API map file, but can be named anything. 2) <code>SUMMARY_FILENAME</code>: Complete path, filename of the summary datafile. 3) <code>SUMMARY_HEADER_FILENAME</code>: Complete path, filename of the corresponding header file.</p> <p>For each set of data and header files, a new row must be entered into the manifest file. This data is required when merging summary data!!</p>"},{"location":"reference/user-guide/redcap_etl/#merging-formatted-files-for-datahub-import","title":"Merging formatted files for datahub import","text":"<p>Once summary data and header files are created, a Python script is required to merge the summary files. </p>"},{"location":"reference/user-guide/redcap_etl/#built-in-merge-functionality","title":"Built-in merge functionality","text":""},{"location":"reference/user-guide/redcap_etl/#requirements_2","title":"Requirements","text":"<p>Create a virtual environment containing these packages: - pandas</p> <p>Alternatively, after creating a virtual environment, install the required packages using the requirements.txt file using </p> <p><code>pip install -r /path/to/requirements.txt</code></p>"},{"location":"reference/user-guide/redcap_etl/#python-function_1","title":"Python Function","text":"<p><pre><code>class cbioportalSummaryFileCombiner(fname_manifest, \n                                    fname_current_summary, \n                                    patient_or_sample)\n</code></pre> Source Code</p>"},{"location":"reference/user-guide/redcap_etl/#parameters_1","title":"Parameters:","text":"<pre><code>fname_manifest: (str) Filename to patient or sample manifest files. Should be either `summary_manifest_patient.csv` or `summary_manifest_sample.csv`\n\nfname_current_summary: (str) Filename of the current patient or sample summary file (i.e. data_clinical_patient.txt)\n\npatient_or_sample: (str) Options for patient_or_sample include `patient` or `sample` for patient or sample level summary merging.\n</code></pre>"},{"location":"reference/user-guide/redcap_etl/#attributes_1","title":"Attributes:","text":"<pre><code>return_orig(): (tuple)\nReturns the header and data in a tuple (header, data)\n\nsave_update(fname): (None)\nSaves updated/merged summary file to `fname`\n</code></pre>"},{"location":"reference/user-guide/redcap_etl/#example_2","title":"Example","text":"<pre><code>import sys\nimport pandas as pd\nsys.path.insert(0, '/path_to/data-curation/cbioportal-study-merge-tools')\nfrom cbioportal_summary_file_combiner import cbioportalSummaryFileCombiner\n\n\nfname_manifest_sample = 'summary_manifest_sample.csv'\nfname_s_sum = '/datahub/you_study/data_clinical_sample_template.txt'\n\n# Sample updates\nobj_s_combiner = cbioportalSummaryFileCombiner(fname_manifest=fname_manifest_sample, \n                                               fname_current_summary=fname_s_sum, \n                                               patient_or_sample='sample')\n\n# Return original summary dataframe and header\norig_header_s, orig_summary_s = obj_s_combiner.return_orig()\n\n# Save updated/merged data\nobj_s_combiner.save_update(fname=fname_cbio_s_save)\n</code></pre>"},{"location":"reference/user-guide/redcap_etl/#manual-summary-merging","title":"Manual summary merging","text":"<p>Manual merging can be done using the class <code>cBioPortalSummaryMergeTool</code> Source</p> <p>More under construction...</p>"},{"location":"reference/user-guide/slurm/","title":"Slurm GPU Job Scheduler","text":""},{"location":"reference/user-guide/slurm/#introduction","title":"Introduction","text":"<p>SLURM (Simple Linux Utility for Resource Management) is a powerful and flexible workload manager and job scheduler. It is used to allocate resources, submit, monitor, and manage jobs on high-performance computing clusters.</p> <p>This guide covers the basics of using SLURM, including submitting jobs, requesting resources, and monitoring their execution.</p>"},{"location":"reference/user-guide/slurm/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Submitting a Job with SLURM</li> <li>Basic SLURM Directives</li> <li>Example SLURM Job Script</li> <li>Running Array Jobs</li> <li>Monitoring Jobs</li> <li>Canceling Jobs</li> <li>Common SLURM Commands</li> </ul>"},{"location":"reference/user-guide/slurm/#submitting-a-job-with-slurm","title":"Submitting a Job with SLURM","text":"<p>To submit a job in SLURM, you create a job script that includes directives telling SLURM what resources your job needs, how long it will take, where to write output, etc. This script is submitted using the <code>sbatch</code> command.</p> <pre><code>sbatch job_script.slurm\n</code></pre>"},{"location":"reference/user-guide/slurm/#basic-slurm-directives","title":"Basic SLURM Directives","text":"<p>In the job script, directives are defined using the <code>#SBATCH</code> prefix, followed by the resource requests or configurations you need for your job.</p> <p>Here are some common SLURM directives:</p> Directive Description <code>--job-name=&lt;name&gt;</code> Sets the job name for easier identification <code>--output=&lt;file&gt;</code> File to store standard output (use <code>%j</code> for job ID) <code>--error=&lt;file&gt;</code> File to store standard error (use <code>%j</code> for job ID) <code>--ntasks=&lt;num&gt;</code> Number of tasks (CPU cores) required <code>--mem=&lt;size&gt;</code> Memory required for the job (e.g., 4G, 10G, etc.) <code>--time=&lt;time&gt;</code> Maximum run time (format: <code>days-hours:minutes:seconds</code>) <code>--partition=&lt;name&gt;</code> Specify the partition or queue to use <code>--gpus=&lt;num&gt;</code> Number of GPUs required <code>--array=&lt;range&gt;</code> Job array (e.g., <code>0-10</code>, creates 11 tasks)"},{"location":"reference/user-guide/slurm/#example-slurm-job-script","title":"Example SLURM Job Script","text":"<p>Below is a simple example of a SLURM job script.</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=train_RoBERTa_infer  # Job name\n#SBATCH --output=/gpfs/mindphidata/cdm_repos/github/progression-predict/slurm/logs/log.infer.%j.out  # Output file\n#SBATCH --error=/gpfs/mindphidata/cdm_repos/github/progression-predict/slurm/logs/log.infer.%j.err   # Error file\n#SBATCH --ntasks=1                      # Run on a single CPU\n#SBATCH --mem=10G                       # Memory request\n#SBATCH --gpus=1                        # Number of GPUs\n\n# Run the executable with the provided arguments (you may need to adapt this if different arguments are required)\nsrun ./run_infer_mlflow.sh $SLURM_ARRAY_TASK_ID\n</code></pre> <p>In this script: - The <code>#SBATCH</code> directives configure the job's resources. - The <code>srun</code> command launches the program, which in this case runs a Python script.</p>"},{"location":"reference/user-guide/slurm/#running-array-jobs","title":"Running Array Jobs","text":"<p>Array jobs allow you to submit multiple similar jobs with one submission. You can specify an array with the <code>--array</code> directive.</p> <pre><code>#SBATCH --array=0-10  # Submits 11 tasks, with IDs ranging from 0 to 10\n</code></pre> <p>In your script, you can use the environment variable <code>$SLURM_ARRAY_TASK_ID</code> to differentiate tasks in the array.</p> <p>Example:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=array_job\n#SBATCH --array=0-10\n#SBATCH --output=logs/job_%A_%a.out  # %A is the job ID, %a is the array index\n\n# Command that varies based on the array task ID\nsrun ./process_data.sh input_file_$SLURM_ARRAY_TASK_ID.txt\n</code></pre>"},{"location":"reference/user-guide/slurm/#monitoring-jobs","title":"Monitoring Jobs","text":"<p>To monitor your submitted jobs, you can use the following commands:</p> <ul> <li> <p><code>squeue</code>: Shows the status of all jobs in the queue.   <pre><code>squeue -u &lt;username&gt;\n</code></pre></p> </li> <li> <p><code>scontrol show job &lt;job_id&gt;</code>: Shows detailed information about a specific job.</p> </li> <li> <p><code>sacct</code>: Displays accounting information for your completed jobs.   <pre><code>sacct -j &lt;job_id&gt;\n</code></pre></p> </li> </ul>"},{"location":"reference/user-guide/slurm/#canceling-jobs","title":"Canceling Jobs","text":"<p>You can cancel a running or pending job using the <code>scancel</code> command:</p> <pre><code>scancel &lt;job_id&gt;\n</code></pre> <p>To cancel an entire job array, you can omit the task ID, or use the specific task ID to cancel only one task:</p> <pre><code>scancel &lt;job_id&gt;               # Cancels the entire array\nscancel &lt;job_id&gt;_&lt;task_id&gt;      # Cancels a specific task in the array\n</code></pre>"},{"location":"reference/user-guide/slurm/#common-slurm-commands","title":"Common SLURM Commands","text":"<ul> <li> <p><code>sbatch</code>: Submits a job script.   <pre><code>sbatch my_job_script.slurm\n</code></pre></p> </li> <li> <p><code>squeue</code>: Displays information about jobs in the queue.   <pre><code>squeue -u &lt;username&gt;\n</code></pre></p> </li> <li> <p><code>scancel</code>: Cancels a job or set of jobs.   <pre><code>scancel &lt;job_id&gt;\n</code></pre></p> </li> <li> <p><code>sinfo</code>: Shows the status of partitions and nodes.   <pre><code>sinfo\n</code></pre></p> </li> <li> <p><code>scontrol</code>: Allows you to manage jobs and resources (e.g., show job details).   <pre><code>scontrol show job &lt;job_id&gt;\n</code></pre></p> </li> <li> <p><code>srun</code>: Runs parallel tasks within a SLURM job (not typically needed for single-node jobs).</p> </li> </ul>"},{"location":"reference/user-guide/slurm/#additional-guides","title":"Additional Guides","text":"<p>For further details and advanced usage, consult the official SLURM documentation.</p>"},{"location":"reference/user-guide/training/","title":"Required Training for Data Access","text":""},{"location":"reference/user-guide/training/#citi-program","title":"CITI Program","text":"<p>Here\u2019s the link to register for a CITI account. Once you have made an account, you can select the applicable training modules you are required to complete.</p> <p>You need to complete both Human Subjects Protection (HSP) and Good Clinical Practice (GCP) regardless of you being a wetlab or a drylab memeber.</p>"},{"location":"reference/user-guide/training/#additional-resource-related-to-training-on-internal-websites","title":"Additional resource related to training on internal websites","text":"<p>These resources provide an overview of the 12-245 patient consent process and the role of CRA.</p> <ul> <li>12-245 Toolkit</li> <li>Using Specimens in Research (the Genetics link has info about GAP)</li> <li>DUA</li> </ul>"}]}