{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the documentation for Clinical Data Mining @ MSKCC!","text":""},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>CDM Data on cBioPortal</li> <li>CDM Data on Dremio</li> <li>Github Repos</li> <li>CDM Airflow</li> </ul>"},{"location":"reference/","title":"Directory","text":""},{"location":"reference/#data-access-tools","title":"Data Access Tools","text":"<ul> <li>Databricks</li> <li>Minio</li> <li>Dremio</li> </ul>"},{"location":"reference/About-Us/","title":"About Us","text":"<p>The Clinical Data Mining (CDM) team extracts notes, treatment records, and health measurements from the electronic health record generated during the process of cancer care and transforms it into usable, real-world data that can fuel scientific insights.</p> <p>Retrieval of clinical annotation of tumor samples and patients presents a major challenge for data integration, as the current approach of manual abstraction from largely unstructured electronic medical records (EMR) is difficult to scale. However, the use of clinical text classification by means of natural language processing (NLP) and advanced machine learning methods has the potential to unlock information embedded in clinical narratives. The CDM team is creating a hybrid NLP system to leverage against structured and unstructured EMR to identify patient and sample-specific attributes. The development of this system will lead to a robust, large-scale system for enhanced clinical integration with genomic databases that can be used to predict outcomes and treatment responses of individual cancer patients.</p>"},{"location":"reference/databricks/","title":"msk_cdm.databricks","text":""},{"location":"reference/databricks/#msk_cdm.databricks.databricks_api.DatabricksAPI","title":"<code>DatabricksAPI</code>","text":"<p>               Bases: <code>object</code></p> <p>A class to interact with Databricks through its SQL API. This class allows connecting to a Databricks cluster, executing queries, and retrieving the results as pandas DataFrames.</p> Source code in <code>msk_cdm/databricks/databricks_api.py</code> <pre><code>class DatabricksAPI(object):\n    \"\"\"A class to interact with Databricks through its SQL API. This class allows\n    connecting to a Databricks cluster, executing queries, and retrieving\n    the results as pandas DataFrames.\"\"\"\n    def __init__(\n            self,\n            token=None,\n            hostname=None,\n            http_path=None,\n            fname_databricks_env=None\n    ):\n        \"\"\"\n        Initializes the DatabricksAPI class and establishes a connection to the\n        Databricks cluster.\n\n        Parameters:\n        -----------\n        token : str, optional\n            The access token for authentication with Databricks (default is None).\n        hostname : str, optional\n            The hostname of the Databricks server (default is None).\n        http_path : str, optional\n            The HTTP path for the Databricks SQL endpoint (default is None).\n        fname_databricks_env : str, optional\n            The file name of the environment file containing connection parameters\n            (default is None).\n        \"\"\"\n        self._TOKEN = token\n        self._HOSTNAME = hostname\n        self._HTTP_PATH = http_path\n        self._client = None\n        self._URL = None\n\n        if fname_databricks_env is not None:\n            print('Parsing env file')\n            self._process_env(fname_databricks_env=fname_databricks_env)\n\n        self._connect(\n            token=self._TOKEN,\n            hostname=self._HOSTNAME,\n            http_path=self._HTTP_PATH\n        )\n\n    def _connect(\n        self,\n        token,\n        hostname,\n        http_path\n    ):\n        \"\"\"\n        Establishes a connection to the Databricks cluster using the provided\n        access token, hostname, and HTTP path.\n\n        Parameters:\n        -----------\n        token : str\n            The access token for authentication with Databricks.\n        hostname : str\n            The hostname of the Databricks server.\n        http_path : str\n            The HTTP path for the Databricks SQL endpoint.\n\n        Returns:\n        --------\n        None\n        \"\"\"\n        print('Making databricks connection')\n        connection = sql.connect(\n            server_hostname=hostname,\n            http_path=http_path,\n            access_token=token\n        )\n        print('Connected.')\n\n        self._client = connection\n\n        return None\n\n    def _process_env(\n            self,\n            fname_databricks_env\n    ):\n        \"\"\"\n        Processes the environment file to extract connection parameters such as\n        the access token, hostname, HTTP path, and URL.\n\n        Parameters:\n        -----------\n        fname_databricks_env : str\n            The file name of the environment file containing connection parameters.\n\n        Returns:\n        --------\n        None\n        \"\"\"\n\n        dict_config = dotenv_values(fname_databricks_env)\n\n        if not self._TOKEN:\n            self._TOKEN = dict_config.get(\"TOKEN\", None)\n        if not self._HOSTNAME:\n            self._HOSTNAME = dict_config.get(\"HOSTNAME\", None)\n        if not self._URL:\n            self._URL = dict_config.get(\"URL\", None)\n        if not self._HTTP_PATH:\n            self._HTTP_PATH = dict_config.get(\"HTTP_PATH\", None)\n\n        return None\n\n\n    def query_from_file(\n            self,\n            *,\n            fname_sql\n    ):\n        \"\"\"\n        Executes a Spark SQL query from a file and returns the result as a pandas\n        DataFrame.\n\n        Parameters:\n        -----------\n        fname_sql : str\n            The file name of the SQL file containing the query.\n\n        Returns:\n        --------\n        df : pandas.DataFrame\n            A DataFrame containing the results of the query.\n        \"\"\"\n        # open SQL file\n        fd = open(fname_sql, 'r')\n        sqlFile = fd.read()\n        fd.close()\n\n        print('Preview of SQL in %s:' % fname_sql)\n        print(sqlFile[:50])\n\n        df = self.query_from_sql(sql=sqlFile)\n\n        return df\n\n    def query_from_sql(\n            self,\n            *,\n            sql: str\n    ):\n        \"\"\"\n        Executes a Spark SQL query from a string and returns the result as a pandas\n        DataFrame.\n\n        Parameters:\n        -----------\n        sql : str\n           The Spark SQL query string to be executed.\n\n        Returns:\n        --------\n        df : pandas.DataFrame\n           A DataFrame containing the results of the query.\n        \"\"\"\n\n        cursor = self._client.cursor()\n        for i,query in enumerate(sql.split(';')):\n            cursor.execute(query)\n\n        # Gather column names from query\n        column_names = [desc[0] for desc in cursor.description]\n        data = cursor.fetchall()\n\n        # Convert to pandas dataframe\n        df = pd.DataFrame(\n            data,\n            columns=column_names\n        )\n\n        return df\n\n    def close_connection(self):\n        \"\"\"\n        Closes the connection to the Databricks cluster.\n\n        Returns:\n        --------\n        None\n        \"\"\"\n        cursor = self._client.cursor()\n        cursor.close()\n        self._client.close()\n        print('Databricks connection closed')\n\n        return None\n</code></pre>"},{"location":"reference/databricks/#msk_cdm.databricks.databricks_api.DatabricksAPI.__init__","title":"<code>__init__(token=None, hostname=None, http_path=None, fname_databricks_env=None)</code>","text":"<p>Initializes the DatabricksAPI class and establishes a connection to the Databricks cluster.</p>"},{"location":"reference/databricks/#msk_cdm.databricks.databricks_api.DatabricksAPI.__init__--parameters","title":"Parameters:","text":"<p>token : str, optional     The access token for authentication with Databricks (default is None). hostname : str, optional     The hostname of the Databricks server (default is None). http_path : str, optional     The HTTP path for the Databricks SQL endpoint (default is None). fname_databricks_env : str, optional     The file name of the environment file containing connection parameters     (default is None).</p> Source code in <code>msk_cdm/databricks/databricks_api.py</code> <pre><code>def __init__(\n        self,\n        token=None,\n        hostname=None,\n        http_path=None,\n        fname_databricks_env=None\n):\n    \"\"\"\n    Initializes the DatabricksAPI class and establishes a connection to the\n    Databricks cluster.\n\n    Parameters:\n    -----------\n    token : str, optional\n        The access token for authentication with Databricks (default is None).\n    hostname : str, optional\n        The hostname of the Databricks server (default is None).\n    http_path : str, optional\n        The HTTP path for the Databricks SQL endpoint (default is None).\n    fname_databricks_env : str, optional\n        The file name of the environment file containing connection parameters\n        (default is None).\n    \"\"\"\n    self._TOKEN = token\n    self._HOSTNAME = hostname\n    self._HTTP_PATH = http_path\n    self._client = None\n    self._URL = None\n\n    if fname_databricks_env is not None:\n        print('Parsing env file')\n        self._process_env(fname_databricks_env=fname_databricks_env)\n\n    self._connect(\n        token=self._TOKEN,\n        hostname=self._HOSTNAME,\n        http_path=self._HTTP_PATH\n    )\n</code></pre>"},{"location":"reference/databricks/#msk_cdm.databricks.databricks_api.DatabricksAPI.close_connection","title":"<code>close_connection()</code>","text":"<p>Closes the connection to the Databricks cluster.</p>"},{"location":"reference/databricks/#msk_cdm.databricks.databricks_api.DatabricksAPI.close_connection--returns","title":"Returns:","text":"<p>None</p> Source code in <code>msk_cdm/databricks/databricks_api.py</code> <pre><code>def close_connection(self):\n    \"\"\"\n    Closes the connection to the Databricks cluster.\n\n    Returns:\n    --------\n    None\n    \"\"\"\n    cursor = self._client.cursor()\n    cursor.close()\n    self._client.close()\n    print('Databricks connection closed')\n\n    return None\n</code></pre>"},{"location":"reference/databricks/#msk_cdm.databricks.databricks_api.DatabricksAPI.query_from_file","title":"<code>query_from_file(*, fname_sql)</code>","text":"<p>Executes a Spark SQL query from a file and returns the result as a pandas DataFrame.</p>"},{"location":"reference/databricks/#msk_cdm.databricks.databricks_api.DatabricksAPI.query_from_file--parameters","title":"Parameters:","text":"<p>fname_sql : str     The file name of the SQL file containing the query.</p>"},{"location":"reference/databricks/#msk_cdm.databricks.databricks_api.DatabricksAPI.query_from_file--returns","title":"Returns:","text":"<p>df : pandas.DataFrame     A DataFrame containing the results of the query.</p> Source code in <code>msk_cdm/databricks/databricks_api.py</code> <pre><code>def query_from_file(\n        self,\n        *,\n        fname_sql\n):\n    \"\"\"\n    Executes a Spark SQL query from a file and returns the result as a pandas\n    DataFrame.\n\n    Parameters:\n    -----------\n    fname_sql : str\n        The file name of the SQL file containing the query.\n\n    Returns:\n    --------\n    df : pandas.DataFrame\n        A DataFrame containing the results of the query.\n    \"\"\"\n    # open SQL file\n    fd = open(fname_sql, 'r')\n    sqlFile = fd.read()\n    fd.close()\n\n    print('Preview of SQL in %s:' % fname_sql)\n    print(sqlFile[:50])\n\n    df = self.query_from_sql(sql=sqlFile)\n\n    return df\n</code></pre>"},{"location":"reference/databricks/#msk_cdm.databricks.databricks_api.DatabricksAPI.query_from_sql","title":"<code>query_from_sql(*, sql)</code>","text":"<p>Executes a Spark SQL query from a string and returns the result as a pandas DataFrame.</p>"},{"location":"reference/databricks/#msk_cdm.databricks.databricks_api.DatabricksAPI.query_from_sql--parameters","title":"Parameters:","text":"<p>sql : str    The Spark SQL query string to be executed.</p>"},{"location":"reference/databricks/#msk_cdm.databricks.databricks_api.DatabricksAPI.query_from_sql--returns","title":"Returns:","text":"<p>df : pandas.DataFrame    A DataFrame containing the results of the query.</p> Source code in <code>msk_cdm/databricks/databricks_api.py</code> <pre><code>def query_from_sql(\n        self,\n        *,\n        sql: str\n):\n    \"\"\"\n    Executes a Spark SQL query from a string and returns the result as a pandas\n    DataFrame.\n\n    Parameters:\n    -----------\n    sql : str\n       The Spark SQL query string to be executed.\n\n    Returns:\n    --------\n    df : pandas.DataFrame\n       A DataFrame containing the results of the query.\n    \"\"\"\n\n    cursor = self._client.cursor()\n    for i,query in enumerate(sql.split(';')):\n        cursor.execute(query)\n\n    # Gather column names from query\n    column_names = [desc[0] for desc in cursor.description]\n    data = cursor.fetchall()\n\n    # Convert to pandas dataframe\n    df = pd.DataFrame(\n        data,\n        columns=column_names\n    )\n\n    return df\n</code></pre>"},{"location":"reference/dremio/","title":"msk_cdm.dremio","text":""},{"location":"reference/dremio/#msk_cdm.dremio._dremio_api.DremioAPI","title":"<code>DremioAPI</code>","text":"<p>               Bases: <code>object</code></p> <p>Object to simplify reading from Dremio (CDSI's SQL engine).</p> Source code in <code>msk_cdm/dremio/_dremio_api.py</code> <pre><code>class DremioAPI(object):\n    \"\"\"Object to simplify reading from Dremio (CDSI's SQL engine).\"\"\"\n\n    def __init__(\n        self,\n        *,\n        fname_env: str,\n        env_key_user: Optional[str] = \"USER\",\n        env_key_pw: Optional[str] = \"PW\",\n        scheme: Optional[str] = \"grpc+tcp\",\n        hostname: Optional[str] = \"tlvidreamcord1\",\n        flightport: Optional[int] = 32010,\n    ):\n        \"\"\"Initialization\n\n        Args:\n            fname_env: Environment file with username and pw\n            env_key_user: Key term to identify the username in fname_env\n            env_key_pw: Key term to identify the password in fname_env\n            scheme: The connection scheme used\n            hostname: Server hostname\n            flightport: Port number\n\n        \"\"\"\n\n        self._df = None\n        self._scheme = scheme\n        self._hostname = hostname\n        self._flightport = flightport\n\n        load_dotenv(fname_env)\n        self._authenticate(user=os.getenv(env_key_user), pw=os.getenv(env_key_pw))\n\n    def return_data(self):\n        \"\"\"Return data queried from Dremio in a Pandas dataframe\n\n        Returns:\n            df\n\n        \"\"\"\n        df = self._df\n\n        return df\n\n    def _authenticate(self, user, pw):\n        scheme = self._scheme\n        hostname = self._hostname\n        flightport = self._flightport\n        connection_args = {}\n        # Two WLM settings can be provided upon initial authentication\n        # with the Dremio Server Flight Endpoint:\n        # - routing-tag\n        # - routing queue\n        initial_options = flight.FlightCallOptions(\n            headers=[\n                (b\"routing-tag\", b\"test-routing-tag\"),\n                (b\"routing-queue\", b\"Low Cost User Queries\"),\n            ]\n        )\n        client_auth_middleware = DremioClientAuthMiddlewareFactory()\n        client = flight.FlightClient(\n            f\"{scheme}://{hostname}:{flightport}\",\n            middleware=[client_auth_middleware],\n            **connection_args,\n        )\n        bearer_token = client.authenticate_basic_token(user, pw, initial_options)\n        print(\"[INFO] Authentication was successful\")\n\n        self._client = client\n        self._bearer_token = bearer_token\n\n    def query_data(self, sql):\n        \"\"\"Query Dremio with SQL string\n\n        Args:\n            sql: SQL string used to query Dremio\n\n        Returns:\n            df_output\n\n        \"\"\"\n        client = self._client\n        bearer_token = self._bearer_token\n        # Get table from our dicom segments\n        flight_desc = flight.FlightDescriptor.for_command(sql)\n        options = flight.FlightCallOptions(headers=[bearer_token])\n        schema = client.get_schema(flight_desc, options)\n\n        flight_info = client.get_flight_info(\n            flight.FlightDescriptor.for_command(sql), options\n        )\n        reader = client.do_get(flight_info.endpoints[0].ticket, options)\n\n        df_output = reader.read_pandas()\n\n        self._df = df_output\n\n        return df_output\n</code></pre>"},{"location":"reference/dremio/#msk_cdm.dremio._dremio_api.DremioAPI.__init__","title":"<code>__init__(*, fname_env, env_key_user='USER', env_key_pw='PW', scheme='grpc+tcp', hostname='tlvidreamcord1', flightport=32010)</code>","text":"<p>Initialization</p> <p>Parameters:</p> Name Type Description Default <code>fname_env</code> <code>str</code> <p>Environment file with username and pw</p> required <code>env_key_user</code> <code>Optional[str]</code> <p>Key term to identify the username in fname_env</p> <code>'USER'</code> <code>env_key_pw</code> <code>Optional[str]</code> <p>Key term to identify the password in fname_env</p> <code>'PW'</code> <code>scheme</code> <code>Optional[str]</code> <p>The connection scheme used</p> <code>'grpc+tcp'</code> <code>hostname</code> <code>Optional[str]</code> <p>Server hostname</p> <code>'tlvidreamcord1'</code> <code>flightport</code> <code>Optional[int]</code> <p>Port number</p> <code>32010</code> Source code in <code>msk_cdm/dremio/_dremio_api.py</code> <pre><code>def __init__(\n    self,\n    *,\n    fname_env: str,\n    env_key_user: Optional[str] = \"USER\",\n    env_key_pw: Optional[str] = \"PW\",\n    scheme: Optional[str] = \"grpc+tcp\",\n    hostname: Optional[str] = \"tlvidreamcord1\",\n    flightport: Optional[int] = 32010,\n):\n    \"\"\"Initialization\n\n    Args:\n        fname_env: Environment file with username and pw\n        env_key_user: Key term to identify the username in fname_env\n        env_key_pw: Key term to identify the password in fname_env\n        scheme: The connection scheme used\n        hostname: Server hostname\n        flightport: Port number\n\n    \"\"\"\n\n    self._df = None\n    self._scheme = scheme\n    self._hostname = hostname\n    self._flightport = flightport\n\n    load_dotenv(fname_env)\n    self._authenticate(user=os.getenv(env_key_user), pw=os.getenv(env_key_pw))\n</code></pre>"},{"location":"reference/dremio/#msk_cdm.dremio._dremio_api.DremioAPI.query_data","title":"<code>query_data(sql)</code>","text":"<p>Query Dremio with SQL string</p> <p>Parameters:</p> Name Type Description Default <code>sql</code> <p>SQL string used to query Dremio</p> required <p>Returns:</p> Type Description <p>df_output</p> Source code in <code>msk_cdm/dremio/_dremio_api.py</code> <pre><code>def query_data(self, sql):\n    \"\"\"Query Dremio with SQL string\n\n    Args:\n        sql: SQL string used to query Dremio\n\n    Returns:\n        df_output\n\n    \"\"\"\n    client = self._client\n    bearer_token = self._bearer_token\n    # Get table from our dicom segments\n    flight_desc = flight.FlightDescriptor.for_command(sql)\n    options = flight.FlightCallOptions(headers=[bearer_token])\n    schema = client.get_schema(flight_desc, options)\n\n    flight_info = client.get_flight_info(\n        flight.FlightDescriptor.for_command(sql), options\n    )\n    reader = client.do_get(flight_info.endpoints[0].ticket, options)\n\n    df_output = reader.read_pandas()\n\n    self._df = df_output\n\n    return df_output\n</code></pre>"},{"location":"reference/dremio/#msk_cdm.dremio._dremio_api.DremioAPI.return_data","title":"<code>return_data()</code>","text":"<p>Return data queried from Dremio in a Pandas dataframe</p> <p>Returns:</p> Type Description <p>df</p> Source code in <code>msk_cdm/dremio/_dremio_api.py</code> <pre><code>def return_data(self):\n    \"\"\"Return data queried from Dremio in a Pandas dataframe\n\n    Returns:\n        df\n\n    \"\"\"\n    df = self._df\n\n    return df\n</code></pre>"},{"location":"reference/dremio/#msk_cdm.dremio._dremio_api.DremioClientAuthMiddleware","title":"<code>DremioClientAuthMiddleware</code>","text":"<p>               Bases: <code>ClientMiddleware</code></p> <p>A ClientMiddleware that extracts the bearer token from the authorization header returned by the Dremio Flight Server Endpoint. Parameters</p> <p>factory : ClientHeaderAuthMiddlewareFactory     The factory to set call credentials if an     authorization header with bearer token is     returned by the Dremio server.</p> Source code in <code>msk_cdm/dremio/_dremio_api.py</code> <pre><code>class DremioClientAuthMiddleware(flight.ClientMiddleware):\n    \"\"\"\n    A ClientMiddleware that extracts the bearer token from\n    the authorization header returned by the Dremio\n    Flight Server Endpoint.\n    Parameters\n    ----------\n    factory : ClientHeaderAuthMiddlewareFactory\n        The factory to set call credentials if an\n        authorization header with bearer token is\n        returned by the Dremio server.\n    \"\"\"\n\n    def __init__(self, factory):\n        self.factory = factory\n\n    def received_headers(self, headers):\n        auth_header_key = \"authorization\"\n        authorization_header = []\n        for key in headers:\n            if key.lower() == auth_header_key:\n                authorization_header = headers.get(auth_header_key)\n        self.factory.set_call_credential(\n            [b\"authorization\", authorization_header[0].encode(\"utf-8\")]\n        )\n</code></pre>"},{"location":"reference/dremio/#msk_cdm.dremio._dremio_api.DremioClientAuthMiddlewareFactory","title":"<code>DremioClientAuthMiddlewareFactory</code>","text":"<p>               Bases: <code>ClientMiddlewareFactory</code></p> <p>A factory that creates DremioClientAuthMiddleware(s).</p> Source code in <code>msk_cdm/dremio/_dremio_api.py</code> <pre><code>class DremioClientAuthMiddlewareFactory(flight.ClientMiddlewareFactory):\n    \"\"\"A factory that creates DremioClientAuthMiddleware(s).\"\"\"\n\n    def __init__(self):\n        self.call_credential = []\n\n    def start_call(self, info):\n        return DremioClientAuthMiddleware(self)\n\n    def set_call_credential(self, call_credential):\n        self.call_credential = call_credential\n</code></pre>"},{"location":"reference/minio/","title":"msk_cdm.minio","text":""},{"location":"reference/minio/#msk_cdm.minio._minio_api.MinioAPI","title":"<code>MinioAPI</code>","text":"<p>               Bases: <code>object</code></p> <p>Object to simplify reading/writing to/from Minio.</p> Source code in <code>msk_cdm/minio/_minio_api.py</code> <pre><code>class MinioAPI(object):\n    \"\"\"Object to simplify reading/writing to/from Minio.\"\"\"\n\n    def __init__(\n        self,\n        *,\n        ACCESS_KEY: Optional[str] = None,\n        SECRET_KEY: Optional[str] = None,\n        ca_certs: Optional[str] = None,\n        url_port: Optional[str] = \"pllimsksparky3:9000\", \n        fname_minio_env: Optional[Union[Path, str]] = None,\n        bucket: Optional[str] = None,\n    ):\n        \"\"\"Initialization\n\n                Args:\n                    - ACCESS_KEY: Minio access key. Optional if `fname_minio_env` is passed, in which case it may be present in the env file picked up by .env\n                    - SECRET_KEY: Minio secret key. Optional if `fname_minio_env` is passed, in which case it may be present in the env file picked up by .env\n                    - ca_certs: optional filename pointer to ca_cert bundle for `urllib3`. Only specify if not passing `fname_minio_env`.\n                    - fname_minio_env: A filename with KEY=value lines with values for keys `CA_CERTS`, `URL_PORT`, `BUCKET`.\n                    - bucket: optional default minio bucket to use for operations. Can also be specified as environment variable $BUCKET.\n        \"\"\"\n        self._ACCESS_KEY = ACCESS_KEY\n        self._SECRET_KEY = SECRET_KEY\n        self._ca_certs = ca_certs\n        self._url_port = url_port\n\n        self._bucket = bucket\n        self._client = None\n        self._httpClient = None\n\n        if fname_minio_env is not None:\n            self._process_env(fname_minio_env)\n        self._connect()\n\n    def load_obj(\n            self,\n            path_object: str,\n            bucket_name: Optional[str] = None\n    ) -&gt; urllib3.response.HTTPResponse:\n        \"\"\"Read an object from minio\n\n        Raises `urllib3.exceptions.HTTPError` if request is unsuccessful.\n\n        Args:\n            path_object: Object file to read from minio.\n            bucket_name: Optional bucket name, otherwise defaults to BUCKET passed\n            via minio env fniame to constructor\n\n        Returns:\n            urllib3.response.HTTPResponse\n\n        \"\"\"\n        if self._bucket is not None:\n            bucket_name = self._bucket\n\n        obj = self._client.get_object(bucket_name, path_object)\n\n        return obj\n\n        # try:\n        #     obj = self._client.get_object(bucket_name, path_object)\n        #     if obj.status != 200:\n        #         raise RuntimeError(\n        #             f\"Got non-OK HTTP status {obj.status} requesting \" \"{obj_name}.\"\n        #         )\n        #     return obj\n        #\n        #     # From here, the object can be read in pandas\n        #     # df = pd.read_csv(obj, sep=sep, low_memory=False)\n        #\n        # finally:\n        #     obj.close()\n        #     obj.release_conn()\n\n    def save_obj(\n        self,\n        df,\n        path_object: str,\n        sep: Optional[str] = \",\",\n        bucket_name: Optional[str] = None,\n    ):\n        \"\"\"Save an object to minio\n\n        Args:\n            df: Pandas dataframe to be saved to Minio\n            path_object: Object filename for `df`\n            sep: Separator when saving the Pandas dataframe\n            bucket_name: Optional bucket name, otherwise defaults to BUCKET passed\n            via minio env fniame to constructor\n\n        \"\"\"\n\n        if self._bucket is not None:\n            bucket_name = self._bucket\n\n        csv_bytes = df.to_csv(index=False, sep=sep).encode(\"utf-8\")\n        csv_buffer = BytesIO(csv_bytes)\n\n        self._client.put_object(\n            bucket_name=bucket_name,\n            object_name=path_object,\n            data=csv_buffer,\n            length=len(csv_bytes),\n            content_type=\"application/csv\",\n        )\n\n        return None\n\n    def load_df(\n            self,\n            fname,\n            sep: Optional[str] = \"\\t\",\n            dtype: Optional[str] = object\n    ):\n        obj = self.load_obj(path_object=fname)\n        df= pd.read_csv(obj, dtype=dtype, sep=sep)\n        return df\n\n    def save_df(\n            self,\n            df,\n            fname,\n            sep: Optional[str] = \"\\t\"\n    ):\n        self.save_obj(\n            df=df,\n            path_object=fname,\n            sep=sep\n        )\n        print(f'Saved data to: {fname}')\n\n    def print_list_objects(\n        self,\n        bucket_name: Optional[str] = None,\n        prefix: Optional[str] = None,\n        recursive: Optional[bool] = True,\n    ):\n        \"\"\"Create a Python list of objects in a specified minio bucket\n\n        Args:\n            - bucket_name: Optional bucket name, otherwise defaults to  BUCKET passed via minio env fname to constructor\n            - prefix: Optional string used to find an object starting with &lt;prefix&gt;\n\n        Returns:\n            obj_list: List of strings containing path locations in minio bucket.\n\n        \"\"\"\n        if self._bucket is not None:\n            bucket_name = self._bucket\n\n        objs = self._client.list_objects(\n            bucket_name=bucket_name,\n            recursive=recursive,\n            prefix=prefix\n        )\n        obj_list = []\n        for obj in objs:\n            obj_list.append(obj.object_name)\n\n        return obj_list\n\n    def remove_obj(self, path_object: str, bucket_name: Optional[str] = None):\n        \"\"\"Remove an object from minio\n\n        Args:\n            path_object: Object file to be removed from minio\n            bucket_name: Optional bucket name, otherwise defaults to  BUCKET passed via minio env fname to constructor\n\n        \"\"\"\n        # Remove list of objects.\n        self._client.remove_object(bucket_name=bucket_name, object_name=path_object)\n        print(\"Object removed. Bucket: %s, Object: %s\" % (bucket_name, path_object))\n\n        return None\n\n    def copy_obj(\n        self,\n        source_path_object: str,\n        dest_path_object: str,\n        source_bucket: Optional[str] = None,\n        dest_bucket: Optional[str] = None,\n    ):\n        \"\"\"Copy an object in minio.\n\n        Objects can be copied across different BUCKETS.\n        Warning: objects with greater than 1GB may fail using this.\n        Instead, use `load_obj` and `save_obj` in combination.\n\n        Args:\n            source_path_object: Object file to be copied\n            dest_path_object: Object filename that `source_path_object` will be copied to\n            bucket_name: Optional bucket name, otherwise defaults to  BUCKET passed\n            via minio env fniame to constructor\n\n        Returns:\n            output: Object name and version ID of object\n        \"\"\"\n        if self._bucket is not None:\n            source_bucket = self._bucket\n            dest_bucket = self._bucket\n\n        result = self._client.copy_object(\n            dest_bucket,\n            dest_path_object,\n            CopySource(source_bucket, source_path_object),\n        )\n\n        output = [result.object_name, result.version_id]\n\n        return output\n\n    def _process_env(self, fname_minio_env):\n        print(\"Minio environment file: %s\" % fname_minio_env)\n        dict_config = dotenv_values(fname_minio_env)\n\n        env_access_key = os.getenv(\"ACCESS_KEY\")\n        if env_access_key:\n            dict_config[\"ACCESS_KEY\"] = env_access_key\n\n        env_secret_key = os.getenv(\"SECRET_KEY\")\n        if env_secret_key:\n            dict_config[\"SECRET_KEY\"] = env_secret_key\n\n        if not self._ACCESS_KEY:\n            self._ACCESS_KEY = dict_config.get(\"ACCESS_KEY\", None)\n        if not self._SECRET_KEY:\n            self._SECRET_KEY = dict_config.get(\"SECRET_KEY\", None)\n        if not self._ca_certs:\n            self._ca_certs = dict_config.get(\"CA_CERTS\", None)\n        if not self._url_port:\n            self._url_port = dict_config.get(\"URL_PORT\", None)\n        if not self._bucket:\n            self._bucket = dict_config.get(\"BUCKET\", None)\n\n        # # Print out for QC\n        # print('Access Key: %s' % self._ACCESS_KEY)\n        # print('Secret Key: %s' % self._SECRET_KEY)\n        # print('CA Cert: %s' % self._ca_certs)\n        # print('URL Port: %s' % self._url_port)\n        # print('Bucket: %s' % self._bucket)\n\n        return None\n\n    def _connect(self):\n        # required for self-signed certs\n        httpClient = urllib3.PoolManager(\n            cert_reqs=\"CERT_REQUIRED\", ca_certs=self._ca_certs\n        )\n\n        # Create secure client with access key and secret key\n        client = Minio(\n            endpoint=self._url_port,\n            access_key=self._ACCESS_KEY,\n            secret_key=self._SECRET_KEY,\n            secure=True,\n            http_client=httpClient,\n        )\n\n        self._client = client\n        self._httpClient = httpClient\n\n        return None\n</code></pre>"},{"location":"reference/minio/#msk_cdm.minio._minio_api.MinioAPI.__init__","title":"<code>__init__(*, ACCESS_KEY=None, SECRET_KEY=None, ca_certs=None, url_port='pllimsksparky3:9000', fname_minio_env=None, bucket=None)</code>","text":"<p>Initialization</p> <p>Parameters:</p> Name Type Description Default <code>-</code> <code>ACCESS_KEY</code> <p>Minio access key. Optional if <code>fname_minio_env</code> is passed, in which case it may be present in the env file picked up by .env</p> required <code>-</code> <code>SECRET_KEY</code> <p>Minio secret key. Optional if <code>fname_minio_env</code> is passed, in which case it may be present in the env file picked up by .env</p> required <code>-</code> <code>ca_certs</code> <p>optional filename pointer to ca_cert bundle for <code>urllib3</code>. Only specify if not passing <code>fname_minio_env</code>.</p> required <code>-</code> <code>fname_minio_env</code> <p>A filename with KEY=value lines with values for keys <code>CA_CERTS</code>, <code>URL_PORT</code>, <code>BUCKET</code>.</p> required <code>-</code> <code>bucket</code> <p>optional default minio bucket to use for operations. Can also be specified as environment variable $BUCKET.</p> required Source code in <code>msk_cdm/minio/_minio_api.py</code> <pre><code>def __init__(\n    self,\n    *,\n    ACCESS_KEY: Optional[str] = None,\n    SECRET_KEY: Optional[str] = None,\n    ca_certs: Optional[str] = None,\n    url_port: Optional[str] = \"pllimsksparky3:9000\", \n    fname_minio_env: Optional[Union[Path, str]] = None,\n    bucket: Optional[str] = None,\n):\n    \"\"\"Initialization\n\n            Args:\n                - ACCESS_KEY: Minio access key. Optional if `fname_minio_env` is passed, in which case it may be present in the env file picked up by .env\n                - SECRET_KEY: Minio secret key. Optional if `fname_minio_env` is passed, in which case it may be present in the env file picked up by .env\n                - ca_certs: optional filename pointer to ca_cert bundle for `urllib3`. Only specify if not passing `fname_minio_env`.\n                - fname_minio_env: A filename with KEY=value lines with values for keys `CA_CERTS`, `URL_PORT`, `BUCKET`.\n                - bucket: optional default minio bucket to use for operations. Can also be specified as environment variable $BUCKET.\n    \"\"\"\n    self._ACCESS_KEY = ACCESS_KEY\n    self._SECRET_KEY = SECRET_KEY\n    self._ca_certs = ca_certs\n    self._url_port = url_port\n\n    self._bucket = bucket\n    self._client = None\n    self._httpClient = None\n\n    if fname_minio_env is not None:\n        self._process_env(fname_minio_env)\n    self._connect()\n</code></pre>"},{"location":"reference/minio/#msk_cdm.minio._minio_api.MinioAPI.copy_obj","title":"<code>copy_obj(source_path_object, dest_path_object, source_bucket=None, dest_bucket=None)</code>","text":"<p>Copy an object in minio.</p> <p>Objects can be copied across different BUCKETS. Warning: objects with greater than 1GB may fail using this. Instead, use <code>load_obj</code> and <code>save_obj</code> in combination.</p> <p>Parameters:</p> Name Type Description Default <code>source_path_object</code> <code>str</code> <p>Object file to be copied</p> required <code>dest_path_object</code> <code>str</code> <p>Object filename that <code>source_path_object</code> will be copied to</p> required <code>bucket_name</code> <p>Optional bucket name, otherwise defaults to  BUCKET passed</p> required <p>Returns:</p> Name Type Description <code>output</code> <p>Object name and version ID of object</p> Source code in <code>msk_cdm/minio/_minio_api.py</code> <pre><code>def copy_obj(\n    self,\n    source_path_object: str,\n    dest_path_object: str,\n    source_bucket: Optional[str] = None,\n    dest_bucket: Optional[str] = None,\n):\n    \"\"\"Copy an object in minio.\n\n    Objects can be copied across different BUCKETS.\n    Warning: objects with greater than 1GB may fail using this.\n    Instead, use `load_obj` and `save_obj` in combination.\n\n    Args:\n        source_path_object: Object file to be copied\n        dest_path_object: Object filename that `source_path_object` will be copied to\n        bucket_name: Optional bucket name, otherwise defaults to  BUCKET passed\n        via minio env fniame to constructor\n\n    Returns:\n        output: Object name and version ID of object\n    \"\"\"\n    if self._bucket is not None:\n        source_bucket = self._bucket\n        dest_bucket = self._bucket\n\n    result = self._client.copy_object(\n        dest_bucket,\n        dest_path_object,\n        CopySource(source_bucket, source_path_object),\n    )\n\n    output = [result.object_name, result.version_id]\n\n    return output\n</code></pre>"},{"location":"reference/minio/#msk_cdm.minio._minio_api.MinioAPI.load_obj","title":"<code>load_obj(path_object, bucket_name=None)</code>","text":"<p>Read an object from minio</p> <p>Raises <code>urllib3.exceptions.HTTPError</code> if request is unsuccessful.</p> <p>Parameters:</p> Name Type Description Default <code>path_object</code> <code>str</code> <p>Object file to read from minio.</p> required <code>bucket_name</code> <code>Optional[str]</code> <p>Optional bucket name, otherwise defaults to BUCKET passed</p> <code>None</code> <p>Returns:</p> Type Description <code>HTTPResponse</code> <p>urllib3.response.HTTPResponse</p> Source code in <code>msk_cdm/minio/_minio_api.py</code> <pre><code>def load_obj(\n        self,\n        path_object: str,\n        bucket_name: Optional[str] = None\n) -&gt; urllib3.response.HTTPResponse:\n    \"\"\"Read an object from minio\n\n    Raises `urllib3.exceptions.HTTPError` if request is unsuccessful.\n\n    Args:\n        path_object: Object file to read from minio.\n        bucket_name: Optional bucket name, otherwise defaults to BUCKET passed\n        via minio env fniame to constructor\n\n    Returns:\n        urllib3.response.HTTPResponse\n\n    \"\"\"\n    if self._bucket is not None:\n        bucket_name = self._bucket\n\n    obj = self._client.get_object(bucket_name, path_object)\n\n    return obj\n</code></pre>"},{"location":"reference/minio/#msk_cdm.minio._minio_api.MinioAPI.print_list_objects","title":"<code>print_list_objects(bucket_name=None, prefix=None, recursive=True)</code>","text":"<p>Create a Python list of objects in a specified minio bucket</p> <p>Parameters:</p> Name Type Description Default <code>-</code> <code>bucket_name</code> <p>Optional bucket name, otherwise defaults to  BUCKET passed via minio env fname to constructor</p> required <code>-</code> <code>prefix</code> <p>Optional string used to find an object starting with  required <p>Returns:</p> Name Type Description <code>obj_list</code> <p>List of strings containing path locations in minio bucket.</p> Source code in <code>msk_cdm/minio/_minio_api.py</code> <pre><code>def print_list_objects(\n    self,\n    bucket_name: Optional[str] = None,\n    prefix: Optional[str] = None,\n    recursive: Optional[bool] = True,\n):\n    \"\"\"Create a Python list of objects in a specified minio bucket\n\n    Args:\n        - bucket_name: Optional bucket name, otherwise defaults to  BUCKET passed via minio env fname to constructor\n        - prefix: Optional string used to find an object starting with &lt;prefix&gt;\n\n    Returns:\n        obj_list: List of strings containing path locations in minio bucket.\n\n    \"\"\"\n    if self._bucket is not None:\n        bucket_name = self._bucket\n\n    objs = self._client.list_objects(\n        bucket_name=bucket_name,\n        recursive=recursive,\n        prefix=prefix\n    )\n    obj_list = []\n    for obj in objs:\n        obj_list.append(obj.object_name)\n\n    return obj_list\n</code></pre>"},{"location":"reference/minio/#msk_cdm.minio._minio_api.MinioAPI.remove_obj","title":"<code>remove_obj(path_object, bucket_name=None)</code>","text":"<p>Remove an object from minio</p> <p>Parameters:</p> Name Type Description Default <code>path_object</code> <code>str</code> <p>Object file to be removed from minio</p> required <code>bucket_name</code> <code>Optional[str]</code> <p>Optional bucket name, otherwise defaults to  BUCKET passed via minio env fname to constructor</p> <code>None</code> Source code in <code>msk_cdm/minio/_minio_api.py</code> <pre><code>def remove_obj(self, path_object: str, bucket_name: Optional[str] = None):\n    \"\"\"Remove an object from minio\n\n    Args:\n        path_object: Object file to be removed from minio\n        bucket_name: Optional bucket name, otherwise defaults to  BUCKET passed via minio env fname to constructor\n\n    \"\"\"\n    # Remove list of objects.\n    self._client.remove_object(bucket_name=bucket_name, object_name=path_object)\n    print(\"Object removed. Bucket: %s, Object: %s\" % (bucket_name, path_object))\n\n    return None\n</code></pre>"},{"location":"reference/minio/#msk_cdm.minio._minio_api.MinioAPI.save_obj","title":"<code>save_obj(df, path_object, sep=',', bucket_name=None)</code>","text":"<p>Save an object to minio</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <p>Pandas dataframe to be saved to Minio</p> required <code>path_object</code> <code>str</code> <p>Object filename for <code>df</code></p> required <code>sep</code> <code>Optional[str]</code> <p>Separator when saving the Pandas dataframe</p> <code>','</code> <code>bucket_name</code> <code>Optional[str]</code> <p>Optional bucket name, otherwise defaults to BUCKET passed</p> <code>None</code> Source code in <code>msk_cdm/minio/_minio_api.py</code> <pre><code>def save_obj(\n    self,\n    df,\n    path_object: str,\n    sep: Optional[str] = \",\",\n    bucket_name: Optional[str] = None,\n):\n    \"\"\"Save an object to minio\n\n    Args:\n        df: Pandas dataframe to be saved to Minio\n        path_object: Object filename for `df`\n        sep: Separator when saving the Pandas dataframe\n        bucket_name: Optional bucket name, otherwise defaults to BUCKET passed\n        via minio env fniame to constructor\n\n    \"\"\"\n\n    if self._bucket is not None:\n        bucket_name = self._bucket\n\n    csv_bytes = df.to_csv(index=False, sep=sep).encode(\"utf-8\")\n    csv_buffer = BytesIO(csv_bytes)\n\n    self._client.put_object(\n        bucket_name=bucket_name,\n        object_name=path_object,\n        data=csv_buffer,\n        length=len(csv_bytes),\n        content_type=\"application/csv\",\n    )\n\n    return None\n</code></pre>"},{"location":"reference/datasets/datasets/","title":"msk_cdm.datasets","text":"<p>Utilities to load various clinical datasets related to the MSK-IMPACT cohort (de-identified and PHI versions), and datasets derived from IDB queries.</p>"},{"location":"reference/datasets/datasets/#msk-impact-datasets-de-identified","title":"MSK-IMPACT Datasets (De-identified)","text":"Function Name Description <code>load_impact_data_clinical_patient</code> Load the clinical patient summary data from the IMPACT dataset. <code>load_impact_data_clinical_sample</code> Load the clinical sample summary data from the IMPACT dataset. <code>load_impact_data_timeline_surgery</code> Load the surgical timeline data from the IMPACT dataset. <code>load_impact_data_timeline_radiation</code> Load the radiation therapy timeline data from the IMPACT dataset. <code>load_impact_data_timeline_treatment</code> Load the treatment timeline data from the IMPACT dataset. <code>load_impact_data_timeline_diagnosis</code> Load the diagnosis timeline data from the IMPACT dataset. <code>load_impact_data_timeline_specimen</code> Load the specimen timeline data from the IMPACT dataset. <code>load_impact_data_timeline_specimen_surgery</code> Load the specimen surgery timeline data from the IMPACT dataset. <code>load_impact_data_timeline_gleason</code> Load the Gleason score timeline data from the IMPACT dataset. <code>load_impact_data_timeline_pdl1</code> Load the PD-L1 timeline data from the IMPACT dataset. <code>load_impact_data_timeline_mmr</code> Load the MMR timeline data from the IMPACT dataset. <code>load_impact_data_timeline_prior_meds</code> Load the prior medications timeline data from the IMPACT dataset. <code>load_impact_data_timeline_tumor_sites</code> Load the tumor sites timeline data from the IMPACT dataset. <code>load_impact_data_timeline_follow_up</code> Load the follow-up timeline data from the IMPACT dataset. <code>load_impact_data_timeline_progression</code> Load the progression timeline data from the IMPACT dataset. <code>load_impact_data_timeline_cancer_presence</code> Load the cancer presence timeline data from the IMPACT dataset. <code>load_impact_data_timeline_ecog_kps</code> Load the ECOG-KPS timeline data from the IMPACT dataset."},{"location":"reference/datasets/datasets/#msk-impact-datasets-contains-phi","title":"MSK-IMPACT Datasets (Contains PHI)","text":"Function Name Description <code>load_phi_impact_data_timeline_surgery</code> Load the surgical timeline data in the PHI IMPACT format. <code>load_phi_impact_data_timeline_radiation</code> Load the radiation therapy timeline data in the PHI IMPACT format. <code>load_phi_impact_data_timeline_treatment</code> Load the treatment timeline data in the PHI IMPACT format. <code>load_phi_impact_data_timeline_diagnosis</code> Load the diagnosis timeline data in the PHI IMPACT format. <code>load_phi_impact_data_timeline_specimen</code> Load the specimen timeline data in the PHI IMPACT format. <code>load_phi_impact_data_timeline_specimen_surgery</code> Load the specimen surgery timeline data in the PHI IMPACT format. <code>load_phi_impact_data_timeline_gleason</code> Load the Gleason score timeline data in the PHI IMPACT format. <code>load_phi_impact_data_timeline_pdl1</code> Load the PD-L1 timeline data in the PHI IMPACT format. <code>load_phi_impact_data_timeline_prior_meds</code> Load the prior medications timeline data in the PHI IMPACT format. <code>load_phi_impact_data_timeline_tumor_sites</code> Load the tumor sites timeline data in the PHI IMPACT format. <code>load_phi_impact_data_timeline_follow_up</code> Load the follow-up timeline data in the PHI IMPACT format. <code>load_phi_impact_data_timeline_progression</code> Load the progression timeline data in the PHI IMPACT format. <code>load_phi_impact_data_timeline_mmr</code> Load the MMR timeline data in the PHI IMPACT format. <code>load_phi_impact_data_timeline_cancer_presence</code> Load the cancer presence timeline data in the PHI IMPACT format. <code>load_phi_impact_data_timeline_ecog_kps</code> Load the ECOG-KPS timeline data in the PHI IMPACT format. <code>load_phi_impact_id_mapping</code> Load the ID mapping data in the PHI IMPACT format. <code>load_phi_impact_anchor_dates</code> Load the anchor dates data in the PHI IMPACT format."},{"location":"reference/datasets/datasets/#datasets-from-idb-queries","title":"Datasets from IDB Queries","text":"Function Name Description <code>load_phi_idb_demographics</code> Load the demographics data from IDB queries. <code>load_phi_idb_radiology_reports</code> Load the radiology reports data from IDB queries. <code>load_phi_idb_pathology_reports</code> Load the pathology reports data from IDB queries. <code>load_phi_idb_surgeries</code> Load the surgeries data from IDB queries. <code>load_phi_idb_diagnosis</code> Load the diagnosis data from IDB queries. <code>load_phi_idb_medications</code> Load the medications data from IDB queries. <code>load_phi_idb_radiation</code> Load the radiation data from IDB queries. <code>load_phi_idb_interventional_radiology</code> Load the interventional radiology data from IDB queries."},{"location":"reference/datasets/load_impact_data_clinical_patient/","title":"load_impact_data_clinical_patient","text":"<p>Load and return the MSK-IMPACT clinical patient dataset (deidentified).</p>"},{"location":"reference/datasets/load_impact_data_clinical_patient/#msk_cdm.datasets.impact.datasets_impact.load_data_clinical_patient--returns","title":"Returns","text":"<p>data     Dictionary-like object, with the following attributes.</p> <pre><code>- data: pandas DataFrame\n    The data matrix.\n- description_columns (Future release): list\n    The names of the dataset columns.\n- description_dataset (Future release): str\n    The full description of the dataset.\n- filename (Future release): str\n    The path to the location of the data.\n</code></pre>"},{"location":"reference/datasets/load_impact_data_clinical_patient/#msk_cdm.datasets.impact.datasets_impact.load_data_clinical_patient--examples","title":"Examples","text":"<ul> <li>TODO, maybe: Link to a jupyter notebook</li> </ul> Source code in <code>msk_cdm/datasets/impact/datasets_impact.py</code> <pre><code>def load_data_clinical_patient()  -&gt; Bunch:\n    \"\"\"Load and return the MSK-IMPACT clinical patient dataset (deidentified).\n\n    Returns\n    -------\n    data\n        Dictionary-like object, with the following attributes.\n\n        - data: pandas DataFrame\n            The data matrix.\n        - description_columns (Future release): list\n            The names of the dataset columns.\n        - description_dataset (Future release): str\n            The full description of the dataset.\n        - filename (Future release): str\n            The path to the location of the data.\n\n    Examples\n    --------\n    - TODO, maybe: Link to a jupyter notebook\n    \"\"\"\n    df = _loader._load_impact_data_clinical_patient()\n    output = Bunch(data=df)\n    return output\n</code></pre>"},{"location":"reference/user-guide/airflow/","title":"Airflow","text":"<p>A DAG (Directed Acyclic Graph) is the core concept of Airflow, collecting Tasks together, organized with dependencies and relationships to say how they should run. Airflow DAGs can be deployed on tllihpcmind6 and other MSK-MIND servers.</p>"},{"location":"reference/user-guide/airflow/#steps-to-running-a-dag","title":"Steps to Running a DAG","text":"<ul> <li>Clone the DAG repo from cdm-dags and create a new branch.</li> <li>Duplicate the DAG template file <code>cdm_TEMPLATE.py</code> and change the filename (no spaces)</li> <li>Within your new DAG file, change <code>&lt;&lt;&lt;&lt;&lt;CREATE DAG ID&gt;&gt;&gt;&gt;&gt;</code> to the exact filename, minus .py</li> <li>Change <code>&lt;&lt;&lt;&lt;&lt;YOUR EMAIL&gt;&gt;&gt;&gt;&gt;</code> to your email to receive updates on errors</li> <li>Write your DAG</li> <li>From the command line, perform chmod 777 on your DAG file <code>chmod 777 /mind_data/&lt;my_workspace&gt;/cdm-dags/my_dag.py</code></li> <li>Copy the DAG you created in your workspace, and paste into the shared Airflow folders (ex. <code>cp /mind_data/&lt;my_workspace&gt;/cdm-dags/my_dag.py /mind_data/airflow/dags/curation/my_dag.py</code>) </li> <li>AVOID MODIFYING FILES WITHIN THE SHARED AIRFLOW FOLDER. THIS CAN RESULT IN WORK STOPPAGE FOR ALL USERS.</li> <li>After a few moments, the DAG should appear in the Airflow UI</li> <li>Once you have a functioning dag, commit the dag changes back to the cdm-dag repo. Merge your branch if needed.</li> </ul>"},{"location":"reference/user-guide/conda-cheatsheet/","title":"Conda Cheatsheet","text":"<p>Command line package and environment manager Learn to use conda in 30 minutes at bit.ly/tryconda</p>"},{"location":"reference/user-guide/conda-cheatsheet/#getting-started","title":"Getting Started","text":""},{"location":"reference/user-guide/conda-cheatsheet/#setting-up-conda-on-your-machine","title":"Setting up Conda on your machine","text":"<p>When using <code>tllihpcmind6</code>, initialize Conda by <code>source ~/.bashrc</code> <pre><code>$ source ~/.bashrc\n\n$ which conda\n/gpfs/mskmind_ess/&lt;YOUR_USER_NAME&gt;/miniconda/bin/conda\n\n$ conda update conda\n\n$ conda --version\n\n$ conda --help\n\n$ conda list\n</code></pre></p>"},{"location":"reference/user-guide/conda-cheatsheet/#creating-a-virtual-environment","title":"Creating a Virtual Environment","text":"<p>A virtual environment can be defined and created through an <code>environment.yml</code> file. For example, the Conda environment <code>conda-env-cdm</code> can be created with this snippet included in <code>environment.yml</code> <pre><code>name: conda-env-cdm\nchannels:\n  - pytorch\n  - nvidia\n  - conda-forge\ndependencies:\n   - python == 3.10\n   - pandas\n   - pyyaml\n   - requests\n   - setuptools_scm\n   - setuptools\n   - pip\n   - pip: \n       - git+https://github.com/clinical-data-mining/msk_cdm.git\n   - minio\n</code></pre></p>"},{"location":"reference/user-guide/conda-cheatsheet/#conda-basics","title":"Conda Basics","text":"<ul> <li><code>conda info</code>: Verify conda is installed, check version number</li> <li><code>conda update conda</code>: Update conda to the current version</li> <li><code>conda install PACKAGENAME</code>: Install a package included in Anaconda</li> <li><code>spyder</code>: Run a package after install, example Spyder*</li> <li><code>conda update PACKAGENAME</code>: Update any installed program</li> <li><code>COMMANDNAME --help</code>: Command line help<ul> <li><code>conda install --help</code></li> </ul> </li> </ul>"},{"location":"reference/user-guide/conda-cheatsheet/#using-environments","title":"Using Environments","text":"<ul> <li><code>conda create --name py35 python=3.5</code>: Create a new environment named py35, install Python 3.5</li> <li><code>conda env create -f environment.yaml</code>: Create a new environment with specifications in <code>environment.yaml</code> (See Start-up)</li> <li><code>conda activate py35</code>: Activate the new environment to use it</li> <li><code>conda env list</code>: Get a list of all my environments, active environment is shown with *</li> <li><code>conda list</code>: List all packages and versions installed in active environment</li> <li><code>conda env remove --name bio-env</code>: Delete an environment and everything in it</li> <li><code>conda deactivate</code>: Deactivate the current environment</li> </ul>"},{"location":"reference/user-guide/conda-cheatsheet/#installing-packages","title":"Installing Packages","text":"<p>Anaconda includes both the Python and R programming languages, most of the common Python libraries used in science and engineering (including NumPy, SciPy, Matplotlib, and pandas), and many commonly used R packages (https://anaconda.org/).</p> <ul> <li><code>conda install -c anaconda pandas</code>: Install Pandas into your activate environment</li> </ul>"},{"location":"reference/user-guide/data-query-quick-start/","title":"CDM Data Query Quick Start Guide","text":"<p>This repo offers several Jupyter Notebooks that will help query and transform data. Here's how to get started!</p>"},{"location":"reference/user-guide/data-query-quick-start/#create-virtual-environment-from-requirementstxt-file","title":"Create virtual environment from requirements.txt file","text":"<p>At the command line, create a virtual envirnoment called <code>env</code> <pre><code>python3 -m venv env\n</code></pre></p> <p>Then, activate the virtual env: <pre><code>source env/bin/activate\n</code></pre></p> <p>If it's been awhile (or the first time), update pip: <pre><code>&lt;path-to-repo&gt;/cdm-utilities/env/bin/python3 -m pip install --upgrade pip\n</code></pre></p> <p>Once activated, install the packages listed in the <code>requirements.txt</code> file <pre><code>python3 -m pip install -r requirements.txt\n</code></pre></p> <p>After packages have been installed, run <code>jupyter-lab</code> (make sure a web browser is open) and start prototyping!</p>"},{"location":"reference/user-guide/data-query-quick-start/#query-data","title":"Query Data","text":"<p>With your environment set up, this Jupyter Notebook will show you how to query data from our Dremio instance.</p> <p>To understand how to use our data (better), check out the data dictionary.</p>"},{"location":"reference/user-guide/git-branch-management/","title":"Git Cheatsheet","text":""},{"location":"reference/user-guide/git-branch-management/#basics","title":"Basics","text":"<ul> <li><code>git help &lt;command&gt;</code>: get help for a git command</li> <li><code>git init</code>: creates a new git repo, with data stored in the .git directory</li> <li><code>git status</code>: tells you what\u2019s going on</li> <li><code>git add &lt;filename&gt;</code>: adds files to staging area</li> <li><code>git commit</code>: creates a new commit<ul> <li>`Write good commit messages!</li> <li>`Even more reasons to write good commit messages!</li> </ul> </li> <li><code>git log</code>: shows a flattened log of history</li> <li><code>git log --all --graph --decorate</code>: visualizes history as a DAG</li> <li><code>git diff &lt;filename&gt;</code>: show changes you made relative to the staging area</li> <li><code>git diff &lt;revision&gt; &lt;filename&gt;</code>: shows differences in a file between snapshots</li> <li><code>git checkout &lt;revision&gt;</code>: updates HEAD and current branch</li> </ul>"},{"location":"reference/user-guide/git-branch-management/#branching-and-merging","title":"Branching and merging","text":"<ul> <li><code>git branch</code>: shows branches</li> <li><code>git branch &lt;name&gt;</code>: creates a branch</li> <li><code>$ git branch -d [name_of_your_new_branch]</code>: Delete a branch on your local filesystem</li> <li><code>git checkout -b &lt;name&gt;</code>: creates a branch and switches to it</li> <li><code>git merge &lt;revision&gt;</code>: merges into current branch</li> <li><code>git mergetool</code>: use a fancy tool to help resolve merge conflicts</li> <li><code>git rebase</code>: rebase set of patches onto a new base</li> </ul>"},{"location":"reference/user-guide/git-branch-management/#remotes","title":"Remotes","text":"<ul> <li><code>git remote</code>: list remotes</li> <li><code>git remote add &lt;name_of_your_remote&gt; &lt;name_of_your_new_branch&gt;</code>: add a remote</li> <li><code>git push &lt;remote&gt; &lt;local branch&gt;:&lt;remote branch&gt;</code>: send objects to remote, and update remote reference<ul> <li><code>$ git push origin [name_of_your_new_branch]</code></li> </ul> </li> <li><code>git branch --set-upstream-to=&lt;remote&gt;/&lt;remote branch&gt;</code>: set up correspondence between local and remote branch</li> <li><code>git fetch</code>: retrieve objects/references from a remote</li> <li><code>git pull</code>: same as git fetch; git merge</li> <li><code>git clone</code>: download repository from remote</li> </ul>"},{"location":"reference/user-guide/git-branch-management/#undo","title":"Undo","text":"<ul> <li><code>git commit --amend</code>: edit a commit\u2019s contents/message</li> <li><code>git reset HEAD &lt;file&gt;</code>: unstage a file</li> <li><code>git checkout -- &lt;file&gt;</code>: discard changes</li> </ul>"},{"location":"reference/user-guide/git-branch-management/#advanced-git","title":"Advanced Git","text":"<ul> <li><code>git config</code>: Git is highly customizable</li> <li><code>git clone --depth=1</code>: shallow clone, without entire version history</li> <li><code>git add -p</code>: interactive staging</li> <li><code>git rebase -i</code>: interactive rebasing</li> <li><code>git blame</code>: show who last edited which line</li> <li><code>git stash</code>: temporarily remove modifications to working directory</li> <li><code>git bisect</code>: binary search history (e.g. for regressions)</li> <li><code>.gitignore</code>: specify intentionally untracked files to ignore</li> </ul>"},{"location":"reference/user-guide/git-branch-management/#references","title":"References","text":"<ol> <li>MIT lecture on version control</li> </ol>"},{"location":"reference/user-guide/htcondor/","title":"Submitting Jobs (HTCondor)","text":""},{"location":"reference/user-guide/htcondor/#cluster-setup","title":"Cluster Setup","text":""},{"location":"reference/user-guide/htcondor/#pool-1-research","title":"Pool 1 - Research","text":"<ul> <li>Central manager node: pllimskhpc1</li> <li>Submit and Execute nodes: pllimskhpc123 </li> </ul>"},{"location":"reference/user-guide/htcondor/#pool-2-engineering","title":"Pool 2 - Engineering","text":"<ul> <li>Central manager node: pllimsksparky1</li> <li>Submit and Execute nodes: pllimsksparky1234</li> </ul>"},{"location":"reference/user-guide/htcondor/#single-node-phi-machine","title":"Single node - PHI machine","text":""},{"location":"reference/user-guide/htcondor/#notes","title":"Notes:","text":"<ul> <li>pllimskhpc1 and pllimskspakry1 are main nodes in the hpc, sparky cluster. To not overload the main nodes, less resources are made available on these nodes.</li> <li>pllimskhpc1/2 pllimskspakry1/2 have one large partitionable slot to accommodate larger jobs. Be sure to specify your GPU/CPU/Memory requirements when submitting to these machines.</li> <li>Pool configs can be updated by MIND engineers. Please reach out to the engineering team if these configurations don't fit your analysis.</li> <li>If you want to submit lots of jobs, please set <code>max_idle</code> and <code>max_materialize</code> in your condor submit file in order to not run the cluster at max load. Think of max_materialize as the maximum number of jobs you want running at any given time, and max_idle as the maximum number of jobs you want to move into the running state at any given time as running jobs complete.</li> <li>In order to distribute the jobs across all nodes within a pool, please add the following property to your submit file. This ensures that all nodes run jobs as your user, and read/write files from the shared filesystem (gpfs in our case). See example code.</li> </ul>"},{"location":"reference/user-guide/htcondor/#submit-file","title":"Submit File","text":"<pre><code>Requirements = TARGET.UidDomain == \"mskcc.org\" &amp;&amp; \\\nTARGET.FileSystemDomain == \"mskcc.org\n</code></pre> <ul> <li>Occasionally, one or more of our nodes lose storage mounts and this may cause failures in your jobs that were executed on those nodes. If you notice this, you can disable certain nodes using the following directive in your submit file.\u00a0</li> </ul>"},{"location":"reference/user-guide/htcondor/#disable-node","title":"Disable Node","text":"<pre><code>\"Requirements = (Machine != \"pllimsksparky2.mskcc.org\")\"\n</code></pre>"},{"location":"reference/user-guide/htcondor/#useful-guides","title":"Useful Guides","text":""},{"location":"reference/user-guide/htcondor/#htcondor-docs","title":"HTCondor Docs","text":"<ul> <li>Full manual</li> <li>Fun tutorial/exercises</li> <li>More reading on running jobs</li> </ul>"},{"location":"reference/user-guide/htcondor/#useful-faqs","title":"Useful FAQs","text":"<ul> <li>Submitting Multiple Jobs to Condor from input file</li> <li>GPU jobs</li> <li>Submit examples</li> <li>Docker with Condor</li> <li>Using a shared filesystem</li> </ul>"},{"location":"reference/user-guide/htcondor/#example-job-submission-templates","title":"Example job submission templates","text":"<ul> <li>MSK-MIND examples</li> <li>https://github.com/CHTC/templates-GPUs</li> <li>https://github.com/CHTC/example-multiple-jobs</li> <li>https://github.com/CHTC/example-python-pandas</li> </ul>"},{"location":"reference/user-guide/htcondor/#example-flow","title":"Example Flow","text":"<ol> <li>Prepare an executable job (shell, python script or docker), and a submit.sub</li> <li>Submit your job! <code>condor_submit submit.sub</code></li> <li>Check you job status in the queue <code>condor_q</code></li> <li>Or check the details of one job with: <code>condor_q -better-analyze &lt;job_id&gt;</code></li> <li>Check machine availability: <code>condor_status -claimed</code></li> <li>Check output and logs, as specified in the <code>submit.sub</code></li> <li>kill a job <code>condor_rm &lt;job_id&gt;</code></li> </ol> <p>Caution!: Before submitting your real jobs, perform always some simple tests in order to make sure that both your submit file and program will work in a proper way: if you are going to submit hundreds of jobs and each job takes several hours to finish, before doing that try with just a few jobs and change the input data in order to let them finish in minutes. Then check the results to see if everything went fine before submitting the real jobs. Also we recommend you use condor_submit -dry-run to debug your jobs and make sure they will work as expected). Bear in mind that submitting untested files and/or jobs may cause a waste of time and resources if they fail, and also your priority will be lower in following submissions.</p>"},{"location":"reference/user-guide/htcondor/#useful-commands","title":"Useful Commands","text":""},{"location":"reference/user-guide/htcondor/#check-nodes","title":"Check nodes","text":"<ul> <li>condor_status</li> <li>condor_status -compact</li> <li>condor_status -compact -constraint 'TotalSlotGpus &gt; 0'</li> <li>condor_status -claimed</li> <li>condor_status"},{"location":"reference/user-guide/htcondor/#check-jobs-in-the-queue","title":"Check jobs in the queue","text":"<ul> <li>condor_q</li> <li>condor_q -all</li> <li>(devops) to check jobs submitted by all users</li> </ul>"},{"location":"reference/user-guide/htcondor/#check-jobs-history","title":"Check jobs history","text":"<ul> <li>condor_history</li> </ul>"},{"location":"reference/user-guide/htcondor/#check-details-of-your-submitted-job-for-debugging-purposes-with-jobcluster-id-from-previous-condor_q-command","title":"Check details of your submitted job for debugging purposes with JOB/CLUSTER ID from previous <code>condor_q</code> command","text":"<ul> <li>condor_q -better-analyze"},{"location":"reference/user-guide/htcondor/#use-conda-environment","title":"Use conda environment","text":"<p>Include the following lines in the executable script that condor submit file calls <pre><code>source /&lt;path-to-your-anaconda3&gt;/etc/profile.d/conda.sh\nconda activate &lt;your-env&gt;\n</code></pre></p>"},{"location":"reference/user-guide/htcondor/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>When specifying the target Machine, be sure to match the full Machine Name. For example, pllimskhpc2 will not match to a slot if pllimskhpc2.mskcc.org is the Machine name that you see when running condor_status -compact Ex) requirements = (Machine == \"pllimskhpc2.mskcc.org\")</li> <li>Exec format error - Make sure the first line of your script has the proper shebang. e.g. bash scripts should start with <code>#!/bin/bash</code></li> <li>MemoryError - could be resolved by increasing the memory requirement in the submit file e.g. request_memory = 16GB\u00a0 note: resource specification in the submit file is per job, not per cluster if you are submitting more than one job.</li> <li>If your job is stuck in hold, use 'condor_q -l' to view the long view of the queue and find the 'HoldReason' key for details about why the job is stuck in the hold state.\u00a0</li> </ul> <p>Documentation adapted from MSK-MIND</p>"},{"location":"reference/user-guide/jupyterhub/","title":"JupyterHub","text":""},{"location":"reference/user-guide/jupyterhub/#location","title":"Location","text":"<p>JupyterHub URL for CDM Development</p> <p>Login: MSK credentials. If you are unable to log in, please contact one of the MIND engineers to have you added as a linux user of the pllimsksparky2 machine and you must log into this machine once using the terminal in order to set up your home directory. The first time you log in, jupyterhub may take a minute or so to spawn the user server and jupyter lab environment. Since an MSK self-signed certificate is being used, your browser may warn you that the connection is insecure, but the connection is secure (https encrpyted) and only lacks trust verification with a third-party certificate authority which is not needed for security in the internal MSK network. On Chrome click on the 'Advanced' button to continue to proceed to using the web application.\u00a0</p> <p>Base Directory: The directory under which jupyterhub's file view launches is <code>/mind_data/shared_data_folder</code>. You may choose to create a sub-directory under this directory with your username and store your notebooks in there. You may control access to your notebooks via linux file permissions. Please contact one of the MIND engineers to set up the sub-directory for you.</p>"},{"location":"reference/user-guide/jupyterhub/#creating-python-virtual-environments","title":"Creating Python Virtual Environments","text":"<p>Note: These instructures are for creating virtual environments in Jupyterhub only. For Python environments on the terminal, please see Python.</p> <p>Virtual environments [ref1] help you to isolate your execution environment from other notebooks and other users. You may use virtual environment to select specific version of python and its of libraries without having to worry about collisions. Follow these instructions below to setup, test and teardown your virtual environments for notebooks on Jupyterhub.</p>"},{"location":"reference/user-guide/jupyterhub/#setup","title":"Setup","text":"<p>Open a terminal in your Jupyter environment by selecting File -&gt; New -&gt; Terminal and execute the following commands.\u00a0</p> <p>$ cd [LOCATION-WHERE-YOU-WANT-TO-CREATE-THE-VIRTUAL-ENV]</p>"},{"location":"reference/user-guide/jupyterhub/#creating-a-virtual-environment","title":"Creating a Virtual Environment","text":"<pre><code># create the virtual environment\npython3 -m venv my-test-venv\n\n# activate the virtual environment\n$ source my-test-venv/bin/activate\n\n# upgrade pip\n$ python3 -m pip install --upgrade pip\n\n# install ipykernel\n$ pip install ipykernel\n\n# Register this env with jupyter lab. It will now show up in the\n# launcher &amp; kernels list once you refresh the page\n$ python3 -m ipykernel install --user --name my-test-venv --display-name \"my test virtual env\"\n\n# List kernels to ensure it was created successfully\n$ jupyter kernelspec list\n\nmy-test-venv    /home/pashaa/.local/share/jupyter/kernels/my-test-venv\npython3      /gpfs/mskmindhdp_emc/sw/env/share/jupyter/kernels/python3\n\n# for testing purposes, install a specific package into this environment\n$ pip install fire\n\n# ensure the package installed successfully\n$ pip list | grep fire\n\nfire                0.4.0\n\n# deactivate the virtual environment in the terminal\n$ deactivate\n</code></pre>"},{"location":"reference/user-guide/jupyterhub/#test","title":"Test","text":"<p>Now, apply the new kernel to your notebook by first selecting the default kernel (which is typically \"Python 3\") from the top right corner of your notebook and then selecting your new kernel \"my test virtual env\".\u00a0NOTE:\u00a0It may take a minute for the drop-down list to update.\u00a0</p> <p>Test your new environment by running the following commands in your notebook code cells.\u00a0</p> <p>Verify that you are using python from your virtual env <pre><code>import sys\nprint(sys.executable)\n\n/gpfs/mskmindhdp_emc/user/pashaa/my-test-venv/bin/python\n</code></pre> Verify you are able to import the test package and that it belongs to your virtual env <pre><code>import fire\nprint(fire.__file__)\nprint(fire.__version__)\n\n'/gpfs/mskmindhdp_emc/user/pashaa/my-test-venv/lib64/python3.6/site-packages/fire/__init__.py'\n 0.4.0\n</code></pre> Alternatively, you can check your dependencies by listing the installed libraries, or checking the details of a specific library. <pre><code>%pip list\nor\n%pip show pip\n</code></pre> You can install libraries in your virtual env within your notebook by using the % operator. <pre><code>%pip install &lt;new-lib&gt;\n</code></pre></p>"},{"location":"reference/user-guide/jupyterhub/#teardown","title":"Teardown","text":"<p>Execute the following commands in your jupyter terminal.</p>"},{"location":"reference/user-guide/jupyterhub/#uninstall-virtual-environment","title":"Uninstall Virtual Environment","text":"<pre><code>$ jupyter kernelspec uninstall my-test-venv\n$ rm -rf my-test-venv\n</code></pre>"},{"location":"reference/user-guide/jupyterhub/#creating-r-virtual-environments","title":"Creating R Virtual Environments","text":""},{"location":"reference/user-guide/jupyterhub/#setup_1","title":"Setup","text":"<p>R packages are installed into libraries, which are directories in the file system containing a subdirectory for each package installed there [ref 2]. Users can have one or more libraries, normally specified by the environment variable R_LIBS_USER, if the corresponding directory actually exists (which by default it will not).</p> <p>Users may set up separate directories for these libraries for separate projects and install project specific libraries into these directories as shown below.\u00a0</p> <p>First, use a terminal to create a project directory in your user space. <pre><code>$ mkdir /gpfs/mskmind_ess/pashaa/my_R_libs_prj1\n</code></pre> Next, open a Jupyter notebook in Jupyterhub, select the system-wide R kernel and configure R to use this directory as a user libs directory. \u00a0</p>"},{"location":"reference/user-guide/jupyterhub/#r-script-in-jupyter-irkernel","title":"R Script in Jupyter IRKernel","text":"<pre><code>[1] Sys.getenv('R_LIBS_USER')\n\n     '~/R/x86_64-pc-linux-gnu-library/4.0'\n\n[2] .libPaths()\n\n     '/opt/R/4.0.5/lib/R/library'\n\n[3] Sys.setenv(R_LIBS_USER=\"/gpfs/mskmind_ess/pashaa/my_R_libs_prj1\")\n[4] .libPaths(c( Sys.getenv('R_LIBS_USER'), .libPaths()  ))\n</code></pre>"},{"location":"reference/user-guide/jupyterhub/#test_1","title":"Test","text":"<p>Test your IRKernel environment. </p> <p>NOTE: You may change the order of the paths to make R pick libraries from the system-wide project directory first by changing the order of the libraries in the .libPaths() setup call above. But, in general you may find it beneficial to give precedence to the libraries that you have installed for your project.\u00a0 <pre><code>[5] Sys.getenv('R_LIBS_USER')\n\n'/gpfs/mskmind_ess/pashaa/my_R_libs_prj1'\n\n[6] .libPaths()\n\n     '/gpfs/mskmind_ess/pashaa/my_R_libs_prj1''/opt/R/4.0.5/lib/R/library'\n</code></pre> List the current set of packages, then install a package and list packages again to check if the package has been installed. Note when loading the packages, you must specify the \"lib.loc\" library location to ensure you are loading from your library.\u00a0 <pre><code>[7] my_packages &lt;- library()$results[,1]\n[8] print(my_packages)\n\n        [1] \"bslib\"         \"cachem\"        \"commonmark\"    \"httpuv\"       \n        [5] \"jquerylib\"     \"later\"         \"promises\"      \"sass\"         \n        [9] \"sourcetools\"   \"xtable\"        \"askpass\"       \"assertthat\"   \n        ...\n        [125] \"utf8\"          \"utils\"         \"uuid\"          \"vctrs\"        \n        [129] \"viridisLite\"   \"vroom\"         \"withr\"         \"xfun\"         \n        [133] \"yaml\"\n\n[9] install.packages('shiny', lib=\"/gpfs/mskmind_ess/pashaa/my_R_libs_prj1\")\n\n        Installing package into \u2018/gpfs/mskmind_ess/pashaa/my_R_libs_prj1\u2019\n        (as \u2018lib\u2019 is unspecified)\n\n[10] my_packages &lt;- library()$results[,1]\n[11] print(my_packages)\n\n        [1] \"bslib\"         \"cachem\"        \"commonmark\"    \"httpuv\"       \n        [5] \"jquerylib\"     \"later\"         \"promises\"      \"sass\"         \n        [9] \"sourcetools\"   \"xtable\"        \"askpass\"       \"assertthat\"   \n        ...\n        [125] \"utf8\"          \"utils\"         \"uuid\"          \"vctrs\"        \n        [129] \"viridisLite\"   \"vroom\"         \"withr\"         \"xfun\"         \n        [133] \"yaml\"          \"shiny\"\n\n[12] library(\"shiny\", lib.loc=\"/gpfs/mskmind_ess/pashaa/my_R_libs_prj1\")\n</code></pre> On the terminal, you may check to see if the package and its dependencies have been installed in your project location.\u00a0</p>"},{"location":"reference/user-guide/jupyterhub/#terminal","title":"Terminal","text":"<p><pre><code>$ ls /gpfs/mskmind_ess/pashaa/my_R_libs_prj1\nbslib  cachem  commonmark  httpuv  jquerylib  later  promises  sass  shiny  sourcetools  xtable\n</code></pre> More than 1 project directory may be set up in this manner. However, it is advisable to configure R to use one user lib directory at a time so there is no confusion as to where libraries are being installed or uninstalled from.\u00a0</p>"},{"location":"reference/user-guide/jupyterhub/#teardown_1","title":"Teardown","text":"<p>You may uninstall a package as follows.</p>"},{"location":"reference/user-guide/jupyterhub/#r-script-in-jupyter-irkernel_1","title":"R Script in Jupyter IRKernel","text":"<p><pre><code>[12] remove.packages('shiny')\n</code></pre> For a full teardown, reset the R configuration and remove the project directory.\u00a0 <pre><code>[13] Sys.setenv(R_LIBS_USER=\"~/R/x86_64-pc-linux-gnu-library/4.0\")\n[14] .libPaths(.libPaths()[2])\n</code></pre> <pre><code>$ rm -rf /gpfs/mskmind_ess/pashaa/my_R_libs_prj1\n</code></pre></p>"},{"location":"reference/user-guide/jupyterhub/#pre-installed-r-packages","title":"Pre-installed R Packages","text":"<p>For ease of use there is already a set of commonly used packages installed in the base R installation on pllimsksparky2. In addition to the libraries that come with R, we have explicitly installed the following:</p> <ul> <li>tidyverse</li> <li>tidyr</li> <li>ggplot2</li> <li>tidymodels</li> <li>mgcv</li> <li>nlme</li> <li>car</li> <li>randomForest</li> <li>multcomp</li> <li>glmnet</li> <li>survival</li> <li>caret</li> <li>shiny</li> <li>rmarkdown</li> <li>BiocManager</li> <li>flowCore</li> </ul> <p>Chances are that even if the package you're looking for isn't on the above list, it was probably installed as a dependency. To see a complete list of all installed libraries, open an R terminal and run the following:</p>"},{"location":"reference/user-guide/jupyterhub/#r-script-in-jupyter-irkernel_2","title":"R script in Jupyter IRKernel","text":"<p><pre><code>&gt; print(library()$results[,1])\n</code></pre> If you're interested in using a package and can't install it in a virtual environment due to core system dependencies or any other issues, let the MIND Engineering team know.</p>"},{"location":"reference/user-guide/jupyterhub/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Notebook cells are not executing or are hanging: Simply restart the kernel by selecting menu item Kernel \u2192 Restart Kernel</li> <li>Notbook and terminal not working in Safari: Make sure you have Safari version 15.4 or higher, and add untrusted cert to keychain to make it trusted (https://github.com/jupyterhub/jupyterhub/issues/292)</li> <li>\"Unexpected error while saving file: .ipynb attempt to write a readonly database\": Restart your server by going to File\u2192Hub Control Panel.\u00a0 <li>Trouble initiating Jupyterhub in Safari: Try Google Chrome</li> <li>Trouble loading, saving, or creating new jupyter notebooks: Try restarting the Jupyterhub service by: Going to File -&gt; Hub Control Panel Click Start My Server</li>"},{"location":"reference/user-guide/jupyterhub/#references","title":"References","text":"<ol> <li>Creating a kernel for virtual environments</li> <li>R Admin - Managing Libraries</li> <li>List R packages</li> <li>Using Virtual Environments in Jupyter Notebook and Python</li> </ol> <p>Documentation adapted from MSK-MIND</p>"},{"location":"reference/user-guide/minio/","title":"Minio","text":"<p>Minio is an object store very similar to AWS S3. It allows your data to be remotely accessible via the S3 protocol.</p>"},{"location":"reference/user-guide/minio/#python-api","title":"Python API","text":"<p>Steps to access data using Python Minio client API</p> <ol> <li>Log into Minio. If you get an error \"Expecting a policy to be set for user <code>X</code> or one of their groups\", contact Pasha, Arfath or Kohli, Armaan with the 'X' String and the names of the buckets you need access to so we can give you access to these buckets.\u00a0</li> <li>Create a \"Service Account\" with a simple access key and the recommended secret key. Copy the secret key to clipboard on creation.</li> </ol> <p></p> <ol> <li>Store your access key and secret key in a python-dotenv file.</li> </ol>"},{"location":"reference/user-guide/minio/#create-env","title":"Create .env","text":"<pre><code># create and populate .env file\n$ echo \"SECRET_KEY=YOUR_SECRET_KEY\" &gt;&gt; .env\n$ echo \"ACCESS_KEY=YOUR_ACCESS_KEY\" &gt;&gt; .env\n\n\n# verify .env file\n$ cat .env\nSECRET_KEY=YOUR_SECRET_KEY\nACCESS_KEY=YOUR_ACCESS_KEY\n\n$ ls -la\ndrwxr-xr-x    4 xxx  xxx   128 Jun  1 11:20 .\ndrwxr-xr-x  121 xxx  xxx  3872 May 26 13:50 ..\n-rw-r--r--    1 xxx  xxx    42 Jun  1 11:20 .env\n</code></pre> <ol> <li>Change permissions to the dotenv file so only you can read and write to it.\u00a0</li> </ol>"},{"location":"reference/user-guide/minio/#permissions","title":"Permissions","text":"<p><pre><code>$ chmod 600 .env\n$ ls -la-rw------- 1 xxx xxx 42 Jun 1 11:20 .env\n</code></pre> 5. <code>pip install python-dotenv minio</code> 6. Get the full SSL certifcate chain for the Minio instance</p>"},{"location":"reference/user-guide/minio/#ssl-cert","title":"SSL Cert","text":"<pre><code>openssl s_client -showcerts -verify 5 -connect HOST:PORT &gt; certificate.crt\n\nexample:\nopenssl s_client -showcerts -verify 5 -connect tllihpcmind6:9000 &gt; certificate.crt\n</code></pre> URL HOST PORT https://tllihpcmind6/minio tllihpcmind6 9000 https://pllimsksparky3/minio/large pllimsksparky3 9006 https://pllimsksparky3/minio/lake pllimsksparky3 9007 https://pllimsksparky3/minio/small pllimsksparky3 9008 https://pllimsksparky3/minio/user pllimsksparky3 9009 <ol> <li> <p>Set up python boiler-plate code for creating a minio client object.\u00a0 <pre><code>from minio import Minio\nimport urllib3\nimport os\nfrom dotenv import load_dotenv\nfrom pathlib import Path\n\nenv_path = Path('.')/'.env'\nload_dotenv(dotenv_path=env_path)\n\nACCESS_KEY = os.getenv('ACCESS_KEY')\nSECRET_KEY = os.getenv('SECRET_KEY')\n\n# required for self-signed certs\nhttpClient = urllib3.PoolManager(\ncert_reqs='CERT_REQUIRED',\nca_certs='certificate.crt'\n)\n\n# Create secure client with access key and secret key\nclient = Minio(\n\"tllihpcmind6:9000\",\naccess_key=ACCESS_KEY,\nsecret_key=SECRET_KEY,\nsecure=True,\nhttp_client=httpClient\n)\n\n\n# list objects in a bucket\nfor ii in client.list_objects(\"test\"):\nprint(ii.__dict__)\n</code></pre></p> </li> <li> <p>Try other commands like get_object and put_object from the Minio API. <pre><code>import pandas as pd\n\nobj = client.get_object(&lt;BUCKET&gt;,&lt;CSV_FILE_PATH&gt;)\n\ndf = pd.read_csv(obj)\ndf\n\n# for parquet files\nfrom io import BytesIO\nobj = client.get_object(&lt;BUCKET&gt;,&lt;PARQUET_FILE_PATH&gt;)\n\ndf = pd.read_parquet(BytesIO(obj.data))\n\npq_obj = df.to_parquet()\nclient.put_object(&lt;BUCKET&gt;, &lt;PARQUET_FILE_PATH&gt;, data=BytesIO(pq_obj), length=len(pq_obj))\n</code></pre></p> </li> </ol>"},{"location":"reference/user-guide/minio/#access-via-r","title":"Access via R","text":"<p>Data stored in minio can be accessed in R in the following fashion: <pre><code># sample r/minio interface via aws.s3\n\n# load external libs\nlibrary(aws.s3)\nlibrary(httr)\nlibrary(dplyr)\n\n# disable ssl verification (required for the time being)\nhttr::set_config(config( ssl_verifypeer = 0L ))\n\n# params need to be written to environment\nSys.setenv(AWS_SECRET_ACCESS_KEY=\"***\",\nAWS_ACCESS_KEY_ID=\"***\",\nAWS_S3_ENDPOINT=\"tllihpcmind6:9000\",  # server name\nAWS_DEFAULT_REGION=\"\") # region env variable doesn't work must be specified with each call\n\n# list all buckets\nbucketlist(region=\"\")\n\n# get bucket\nb &lt;- aws.s3::get_bucket(bucket='sample-bucket', region=\"\")\nprint(b)\n\n# read csv from minio bucket\nobj &lt;- aws.s3::get_object(object=\"test.csv\",\nbucket='sample-bucket', region=\"\")\ncsvcharobj &lt;- rawToChar(obj)  \ncon &lt;- textConnection(csvcharobj)  \ndata &lt;- read.csv(file = con)\nclose(con)\n# head(data, n=2)\n\n\n# write csv file back to minio\ncon &lt;- rawConnection(raw(0), \"r+\")\nwrite.csv(data, con)\naws.s3::put_object(file = rawConnectionValue(con),\nbucket = \"sample-bucket\", object = \"test_write.csv\", region=\"\")\nclose(con)\n</code></pre></p>"},{"location":"reference/user-guide/minio/#troubleshooting","title":"Troubleshooting","text":"<ol> <li> <p>If while making a minio client call (mc) you get a certificate error, use '\u2013insecure' in your call to disable certificate verification. The connection will still be encrypted, ony that the certificate verification process with a certificate authority will be skipped in the SSL protocol. This is sometimes necessary for self-signed certificates. Our certificate is a self-signed certificate issued by MSK Open Systems .</p> </li> <li> <p>mc:  Unable to initialize new alias from the provided credentials. Get \"https://pllimsksparky3:9006\": dial tcp: lookup pllimsksparky3 on 140.163.135.19:53: server misbehaving. <li>Solution: Ping the server to get its IP address and add it to you '/etc/hosts' file as '10.254.130.16 pllimsksparky3'.</li> <p>Documentation adapted from MSK-MIND</p>"},{"location":"reference/user-guide/python-environment/","title":"Python","text":"<p>These instructures are for terminal access only. For Python environments in JupyterHub, please see JupyterHub.</p>"},{"location":"reference/user-guide/python-environment/#virtualenv","title":"Virtualenv","text":"<p>You may create a virtualenv by pointing to a particular version of python. Different versions of Python interpreters are available,</p>"},{"location":"reference/user-guide/python-environment/#python-versions","title":"Python Versions","text":"<p><pre><code>On the phi server:\n\n/mind_data/sw\n\u251c\u2500\u2500 python3.7\n\u251c\u2500\u2500 python3.8\n\u2514\u2500\u2500 python3.9\n\nOn the de-identified servers:\n\n/gpfs/mskmind_ess/sw/\n\u251c\u2500\u2500 python3.7\n\u251c\u2500\u2500 python3.8\n\u2514\u2500\u2500 python3.9\n</code></pre> So, for example you may create a virtual environment with python3.7 as shown below.</p> <p>Note: LD_LIBRARY_PATH needs to be set to include the lib of the version you choose.\u00a0</p>"},{"location":"reference/user-guide/python-environment/#virtualenv_1","title":"virtualenv","text":"<p><pre><code>$ export LD_LIBRARY_PATH=/usr/lib64:/gpfs/mskmind_ess/sw/python3.7/lib\n$ /gpfs/mskmind_ess/sw/python3.7/bin/python3.7 -m venv venv\n$ source venv/bin/activate\n(venv) $ which python\n~/venv/bin/python\n(venv) $ python --version\nPython 3.7.11   \n(venv) $ deactivate\n</code></pre> Install packages with pip : <pre><code>(venv) $ pip install numpy\n</code></pre> N.B., do not use the --user\u00a0 option to pip (otherwise it will install to your home directory).</p>"},{"location":"reference/user-guide/python-environment/#miniconda","title":"Miniconda","text":"<p>Miniconda gives you the Python interpreter itself, along with a command-line tool called conda which operates as a cross-platform package manager geared toward Python packages. You may use Miniconda on the compute servers to manage your own python environments and packages for your projects. This means you do not have to depend on any centralized or system-wide python environment for your projects.\u00a0</p>"},{"location":"reference/user-guide/python-environment/#setup","title":"Setup","text":"<p>Install Miniconda using the instructions below in your ESS sub-directory your mind_data sub-directory</p>"},{"location":"reference/user-guide/python-environment/#miniconda-installation","title":"Miniconda Installation","text":"<p><pre><code># or `cd` into any directory of your choice (e.g. /mind_data/&lt;YOUR_USER_NAME&gt;)\n$ cd /gpfs/mskmind_ess/&lt;YOUR_USER_NAME&gt;/\n\n# Download the python 3.9 (or latest) installer (See Ref 1)\n# NOTE: You can choose different versions of python for different projects later\n# using the conda package manager. This is described in the Test section below.\n# If having issues w/ insecure warnings, add --no-check-certificate flag\n$ wget https://repo.anaconda.com/miniconda/Miniconda3-py39_4.10.3-Linux-x86_64.sh\n\n# Verify Hash\n$ sha256sum Miniconda3-py39_4.10.3-Linux-x86_64.sh\n1ea2f885b4dbc3098662845560bc64271eb17085387a70c2ba3f29fff6f8d52f\n\n$ chmod +x Miniconda3-py39_4.10.3-Linux-x86_64.sh\n\n$ ./Miniconda3-py39_4.10.3-Linux-x86_64.sh\n</code></pre> When prompted for location of installation, point the installer to <code>/gpfs/mskmind_ess/&lt;YOUR_USER_NAME&gt;/miniconda</code>. Make sure the directory does not already exist.</p> <p>Allow the installer to run conda init and select defaults for the rest of the installation.\u00a0</p> <p>Log out of your shell and log back in.\u00a0</p>"},{"location":"reference/user-guide/python-environment/#test","title":"Test","text":"<p>Test your miniconda installation by following the steps below. If these commands execute, your conda environment is set up correctly.\u00a0 <pre><code>$ source ~/.bashrc\n\n$ which conda\n/gpfs/mskmind_ess/&lt;YOUR_USER_NAME&gt;/miniconda/bin/conda\n\n$ conda update conda\n\n$ conda --version\n\n$ conda --help\n\n$ conda list\n</code></pre> You may now create separate python environments and install separate versions of python packages in these environments following the instructions below.\u00a0</p> <p>Create an environment file with the following contents. Note that you can specify the version of python you want to use that is independent of the version of python chosen when installing miniconda.\u00a0</p>"},{"location":"reference/user-guide/python-environment/#conda-environment-file","title":"conda environment file","text":"<pre><code>name: project_1\ndependencies:\n- python=3.9 # or 3.7, 3.10, etc.\n- pip\n- pip:\n    - numpy==1.23.5\n</code></pre> <p>Now create and activate your environment.\u00a0 <pre><code>$ conda env create -f project_1.yml\n\n$ conda activate project_1\n</code></pre> Install additional packages with pip or conda: <pre><code>$ pip install pandas\n\n$ conda install poetry\n</code></pre> N.B., do not <code>pip install</code>\u00a0 packages with the <code>--user</code>\u00a0 option.</p> <p>To deactivate, remove an environment and clean conda cache,\u00a0</p>"},{"location":"reference/user-guide/python-environment/#cleanup-environment","title":"cleanup environment","text":"<pre><code>$ conda deactivate\n\n$ conda remove --name project_1 --all\n\n$ conda clean --all\n</code></pre>"},{"location":"reference/user-guide/python-environment/#teardown","title":"Teardown","text":"<p>To remove miniconda altogether, follow these instructions below. NOTE: With many environment installs and uninstalls, the miniconda dir tends to grow in size, so it would be good to re-install it from time to time.\u00a0</p>"},{"location":"reference/user-guide/python-environment/#uninstall-miniconda","title":"uninstall miniconda","text":"<p><pre><code>$ rm -rf /gpfs/mskmind_ess/&lt;YOUR_USER_NAME&gt;/miniconda\n\n$ rm -rf ~/.conda\n</code></pre> Edit your ~/.bashrc (or .bash_profile) file and remove all lines between, and including these lines.\u00a0 <pre><code># &gt;&gt;&gt; conda initialize &gt;&gt;&gt;\n...\n# &lt;&lt;&lt; conda initialize &lt;&lt;&lt;\n</code></pre></p>"},{"location":"reference/user-guide/python-environment/#references","title":"References","text":"<ol> <li>Miniconda installers</li> <li>Conda Tasks</li> </ol> <p>Documentation adapted from MSK-MIND</p>"},{"location":"reference/user-guide/r-guide/","title":"R User Guide","text":"<p>In a conda env, do</p> <p><code>conda install -c conda-forge r-base</code></p> <p>using default <code>r-base</code> from conda will install old R version (3.6)</p>"},{"location":"reference/user-guide/r-guide/#package-installation","title":"Package Installation","text":"<pre><code># list installed packages\n$ R -e \"installed.packages()\"\n\n# list all packages where an update is available\n$ R -e \"old.packages()\"\n\n# to update only a specific package use install.packages()\n$ R -e \"install.packages(\"plotly\")\"\n\n# to update all packages, without prompts for permission/clarification\n$ R -e \"update.packages(ask = FALSE)\"\n</code></pre> <p>Documentation adapted from MSK-MIND </p>"},{"location":"reference/user-guide/redcap_etl/","title":"Redcap to cBioPortal ETL","text":""},{"location":"reference/user-guide/redcap_etl/#-","title":"---","text":"<p>Step-by-step break down of setting up a Redcap study for data to be exposed on cBioPortal, pulling data from Redcap, and creating configuration files for transforming data into a cBioPortal data format</p> <p>There are three (3) configuration files that can be used for exporting data from Redcap and transforming the data into the cBioPortal format: - API mapping file (Required) - Key variable map file (Required) - Data transformation file (Optional)</p>"},{"location":"reference/user-guide/redcap_etl/#setting-up-redcap-for-the-etl","title":"Setting up Redcap for the ETL","text":""},{"location":"reference/user-guide/redcap_etl/#required-from-redcap-before-starting","title":"Required from Redcap before starting","text":"<ul> <li>A Redcap API token for the study</li> <li>At least one (1) Redcap report</li> <li>These variables in the study codebook:<ul> <li>DOB column</li> <li>record id column</li> <li>de-identified ID column</li> <li>IMPACT sample id column</li> </ul> </li> </ul>"},{"location":"reference/user-guide/redcap_etl/#requirements-for-a-redcap-report","title":"Requirements for a Redcap report","text":"<p>Data on cBioPortal cannot contain PHI such as dates, accession numbers, or identifiable patient IDs. </p> <p>Therefore, the following is needed for each Redcap reports: - The date of birth column, or another reference date. This is only used when the report contains a date attribute that needs to be de-identified. - The de-identified patient ID column. This will map to the IDs used when initially creating a cBioPortal study. - The IMPACT sample id column. Only required for sample-level data.</p>"},{"location":"reference/user-guide/redcap_etl/#ideal-setup-of-redcap-reports","title":"Ideal setup of Redcap reports","text":"<p>Data that will be imported into cBioPortal will need to fit these three categories: - Patient-level summaries (data_clinical_patient.txt) - Sample-level summaries (data_clinical_sample.txt) - Timeline files (data_timeline_specimen.txt)</p> <p>Therefore, Redcap reports will ideally be constructed in a similar way: - Non-repeating instruments, patients (Every row contains data for each patient) - Repeating instruments, samples (Every row contains data for an sample id) - Repeating instruments, events (Every row contains data for a date/time stamp)</p> <p>If Redcap report data does not fit this ideal setup, that's okay! Steps below can be taken to format the data. Once your Redcap study is setup, the following configuration files need to be created. </p>"},{"location":"reference/user-guide/redcap_etl/#configuration-files-for-exporting-data-from-redcap","title":"Configuration files for exporting data from Redcap","text":""},{"location":"reference/user-guide/redcap_etl/#api-mapping-file","title":"API mapping file","text":""},{"location":"reference/user-guide/redcap_etl/#purpose","title":"Purpose","text":"<p>This file is used as high-level instructions for code to perform these functions: - Export the Redcap reports of interest onto your server.  - Decide which reports are needed for transfer and which are to be imported onto cbioPortal - Indicate how Redcap report data should be transformed to fit the cBioPortal format</p>"},{"location":"reference/user-guide/redcap_etl/#columns","title":"Columns","text":"<ul> <li>REPORT_NAME<ul> <li>Name of report on redcap, but can be named anything.</li> </ul> </li> <li>API_ID<ul> <li>API ID number located in the Redcap report summary/edit page</li> </ul> </li> <li>FOR_CBIOPORTAL<ul> <li>Indication if Redcap report will be loaded onto cbioportal or if left as a csv at destination</li> </ul> </li> <li>TIMELINE_LEVEL<ul> <li>Column indicates that the Redcap report will be converted into a cBioPortal timeline file according to the specifications in the data transformation files. Entry for this column should be the filename for the data transformation file.</li> </ul> </li> <li>PATIENT_LEVEL<ul> <li>Column indicates that the Redcap report will be converted into a cBioPortal patient summary file. For direct mapping into a summary file, enter (x). Otherwise, entry for this column should be the filename for the data transformation file. </li> </ul> </li> <li>SAMPLE_LEVEL<ul> <li>Column indicates that the Redcap report will be converted into a cBioPortal sample summary file. For direct mapping into a summary file, enter (x). Otherwise, entry for this column should be the filename for the data transformation file. </li> </ul> </li> <li>TIMELINE_TYPE<ul> <li>Label for the expected cBioPortl timeline file type that the Redcap report will be converted to. Each cBioPortal format file will have to adhere to a specific format. The format types are: treatment, toxicity, and diagnosis (please see the cbioportal docs for more info: cBioPortal Docs)</li> </ul> </li> </ul>"},{"location":"reference/user-guide/redcap_etl/#redcap-key-variable-map-file","title":"Redcap key variable map file","text":""},{"location":"reference/user-guide/redcap_etl/#purpose_1","title":"Purpose","text":"<p>This file is used to identify the patient and sample ID mapping (record_id to a de-identified id), while also using the DOB as a marker for de-identifying dates to age, in days</p>"},{"location":"reference/user-guide/redcap_etl/#columns_1","title":"Columns","text":"<ul> <li>variable<ul> <li>DOB column (col_dte_birth)</li> <li>record id column (col_darwin_id)</li> <li>de-identified ID column (col_sample_id)</li> <li>IMPACT sample id column (col_record_id)</li> </ul> </li> <li>redcap_field<ul> <li>Entries for each attribute defined in Redcap study</li> </ul> </li> </ul>"},{"location":"reference/user-guide/redcap_etl/#data-transformation-files","title":"Data transformation files","text":""},{"location":"reference/user-guide/redcap_etl/#purpose_2","title":"Purpose","text":"<p>This file is used to determine the specifications for how data will be transformed and aggregated to fit the patient-, sample-, or timeline-level data files. </p>"},{"location":"reference/user-guide/redcap_etl/#columns_2","title":"Columns","text":"<ul> <li>COLUMN_NAME_REDCAP<ul> <li>Column name used in Redcap</li> </ul> </li> <li>COLUMN_NAME_CBIOPORTAL<ul> <li>Column name that will be used in cbioportal (\"heading\" name). This name should be all CAPS. For timeline transformations, use START_DATE and END_DATE</li> </ul> </li> <li>KEEP_FOR_CBIOPORTAL<ul> <li>(This column will be deprecated) should be all (x).</li> </ul> </li> <li>TIMELINE<ul> <li>Marker if column shoud be included in a timeline file type. Mark as (x) to include.</li> </ul> </li> <li>AGGREGATE_PATIENT<ul> <li>Marker if column shoud be included in a patient-level file type. Mark as (x) to include. If report transformed with this file is a repeated instrument, notes can be used here to aggregate by other functions (max, min, mean, etc).</li> </ul> </li> <li>AGGREGATE_SAMPLE<ul> <li>Marker if column shoud be included in a sample-level file type. Mark as (x) to include. If report transformed with this file is a repeated instrument, unaligned with sample-level data, notes can be used here to aggregate by other functions (max, min, mean, etc).</li> </ul> </li> <li>MELT_ID_VARS<ul> <li>When melting a Redcap report, (ie. transforming data from patient-level to timeline values) mark the row (x) that indicates the patient ID that will be used.</li> </ul> </li> <li>MELT_VAL_VARS<ul> <li>When melting data, indicate the Redcap report columns to be used as date events. Note that COLUMN_NAME_CBIOPORTAL must be labeled as START_DATE or END_DATE when using.</li> </ul> </li> <li>PIVOT_IDX<ul> <li>(This column will be deprecated) When pivoting a Redcap report (ie. transforming data from repeated-instrument to patient-level values), mark the row (x) that indicates the patient ID that will be used.</li> </ul> </li> <li>PIVOT_SUMMARY<ul> <li>(This column will be deprecated) When pivoting data, mark the rows (x) that will be converted from multiple selection (dropdown, checkbox) into a pivot table of multiple columns of Yes/No</li> </ul> </li> </ul>"},{"location":"reference/user-guide/redcap_etl/#exporting-reports-from-your-redcap-study","title":"Exporting reports from your Redcap study","text":"<p>Once the API mapping configuration file has been created, Redcap reports can be pulled to a destination using:</p> <p><code>data-curation/redcap_tools/redcap_api_report_pull.py</code></p>"},{"location":"reference/user-guide/redcap_etl/#requirements","title":"Requirements","text":"<p>Create a virtual environment containing these packages: - redcap - pandas - numpy - re - argparse</p> <p>Alternatively, after creating a virtual environment, install the required packages using the requirements.txt file using </p> <p><code>pip install -r /path/to/requirements.txt</code></p>"},{"location":"reference/user-guide/redcap_etl/#command-line","title":"Command Line","text":"<pre><code>redcap_api_report_pull.py  \\\n-t REDCAP_API_TOKEN \\\n-u REDCAP_URL \\\n-map API_MAPPING_FILE \\\n-vars KEY_VARIABLE_MAP_FILE \\\n-dest PATH_TO_REDCAP_REPORT_EXPORTS\n</code></pre>"},{"location":"reference/user-guide/redcap_etl/#example","title":"Example","text":"<pre><code>/path_to/venv/bin/python /path_to/redcap_tools/redcap_api_report_pull.py  \\\n-t your_redcap_token \\\n-u https://redcap.mskcc.org/api/ \\\n-map /path/to/mapping_file.csv \\\n-vars /path/to/key_variables_file.csv \\\n-dest /another/path/to/exported/data\n</code></pre>"},{"location":"reference/user-guide/redcap_etl/#formatting-redcap-reports-in-the-cbioportal-format","title":"Formatting Redcap reports in the cBioPortal format","text":"<p>Once data and codebook is exported from Redcap, data must be transformed and formatted into the cBioPortal format before importing. The cBioPortal format consists of three file types: - Patient-level summary - Sample-level summary - Timeline (event) data</p> <p>Depending on the complexity of the Redcap reports being exported (i.e. reports from one instrument [simple] vs. reports from multiple instruments [complex]), built-in or custom made functions can be used. </p>"},{"location":"reference/user-guide/redcap_etl/#built-in-functionality","title":"Built-in functionality","text":""},{"location":"reference/user-guide/redcap_etl/#requirements_1","title":"Requirements","text":"<p>Create a virtual environment containing these packages: - pandas - numpy</p> <p>Alternatively, after creating a virtual environment, install the required packages using the requirements.txt file using </p> <p><code>pip install -r /path/to/requirements.txt</code></p>"},{"location":"reference/user-guide/redcap_etl/#python-function","title":"Python Function","text":"<p><pre><code>class RedcapToCbioportalFormat(fname_report_api, \n                               path_config, \n                               fname_report_map, \n                               fname_variables,\n                               path_save)\n</code></pre> Source Code</p>"},{"location":"reference/user-guide/redcap_etl/#parameters","title":"Parameters:","text":"<pre><code>path_config: (str) Pathname to timeline and summary data transformation files. (TODO: Deprecate. Move this info directly into the API mapping file [fname_report_api])\n\nfname_report_api: (str) Filename for the API mapping file. This file is used as high-level instructions for export Redcap reports and pointing to transformation files.\n\nfname_report_map: (str) Filename for table mapping the Redcap report name to the export Redcap data filename. This typically generatea a file named `*_redcap_report_mapping.tsv`\n\nfname_variables: (str) Filename to the Redcap key variable map file. This is created when setting up the Redcap report export.\n\npath_save: (str) Pathname to save formatted data and cBioPortal header files to be merged.\n</code></pre>"},{"location":"reference/user-guide/redcap_etl/#attributes","title":"Attributes:","text":"<pre><code>create_summary_default(patient_or_sample): \nCreates cBioPortal data and header summary files defined in the the data transformation configuration files. Options for patient_or_sample include `patient` or `sample`. \n\ncreate_timeline_files()\nCreate cBioPortal timeline files that can be directly imported into \"datahub\" \n</code></pre>"},{"location":"reference/user-guide/redcap_etl/#returns","title":"Returns:","text":"<pre><code>summary_manifest_patient.csv\nsummary_manifest_sample.csv\n\nThese manifest files contain a table mapping all patient or sample summary data and cBioPortal headers required to be merged. \n</code></pre>"},{"location":"reference/user-guide/redcap_etl/#example_1","title":"Example","text":"<pre><code>from create_summary_from_redcap_reports import RedcapToCbioportalFormat\n\n\n# Create Redcap formatting object from configuration files and pathnames\nobj_redcap_to_cbio = RedcapToCbioportalFormat(fname_report_api='/path/to/api_map.csv', \n                                              path_config='/path/to/transformation/config/files',\n                                              fname_report_map='/path/to/report_map.csv', \n                                              fname_variables='/path/to/variables.csv', \n                                              path_save='/path/to/headers/and/data')\n# Create out-of-box summary tables from Redcap reports\nobj_redcap_to_cbio.create_summary_default(patient_or_sample='patient')\n\n# Create out-of-box timeline files\nobj_redcap_to_cbio.create_timeline_files()\n</code></pre>"},{"location":"reference/user-guide/redcap_etl/#custom-summary-data-formatting","title":"Custom Summary Data Formatting","text":"<p>If Redcap report data, or any data file requires custom aggregation, cBioPortal formatting can be done manually.  Two files need to be generated:</p> <p>1) The datafile containing summary data 2) The cBioPortal summary header file </p> <p>In addition, one other file needs to be modified or created: 3) summary_manifest_patient.csv and/or summary_manifest_sample.csv</p>"},{"location":"reference/user-guide/redcap_etl/#header-file","title":"Header file","text":"<p>The header file is a dataframe containing 5 columns: 1) <code>label</code>: The text shown on the portal summary page 2) <code>comment</code>: The \"hover-over\" text shown when mouse hovers over widet 3) <code>data_type</code>: Data type of columns (STRING or NUMBER) 4) <code>visible</code>: Mark 0 or 1 to make visible on summary page 5) <code>heading</code>: Column names contained in the datafile (below). These columns MUST match the datafile!</p> <p>For summary and header files, the <code>heading</code> column must be <code>PATIENT_ID</code> or <code>SAMPLE_ID</code>. In addition, the <code>label</code> column must contain <code>#Patient Identifier</code> or <code>#Sample Identifier</code>.</p>"},{"location":"reference/user-guide/redcap_etl/#datafile","title":"Datafile","text":"<p>The datafile contains data to be imported. The column header (first row) must match the <code>heading</code> values in the header file.</p> <p>For summary files, the first column must be <code>PATIENT_ID</code> or <code>SAMPLE_ID</code></p>"},{"location":"reference/user-guide/redcap_etl/#modifying-the-manifest-file","title":"Modifying the manifest file","text":"<p>The patient and sample manifest files contain three columns: 1) <code>REPORT_NAME</code>: The name of the summary file created. Typically taken from API map file, but can be named anything. 2) <code>SUMMARY_FILENAME</code>: Complete path, filename of the summary datafile. 3) <code>SUMMARY_HEADER_FILENAME</code>: Complete path, filename of the corresponding header file.</p> <p>For each set of data and header files, a new row must be entered into the manifest file. This data is required when merging summary data!!</p>"},{"location":"reference/user-guide/redcap_etl/#merging-formatted-files-for-datahub-import","title":"Merging formatted files for datahub import","text":"<p>Once summary data and header files are created, a Python script is required to merge the summary files. </p>"},{"location":"reference/user-guide/redcap_etl/#built-in-merge-functionality","title":"Built-in merge functionality","text":""},{"location":"reference/user-guide/redcap_etl/#requirements_2","title":"Requirements","text":"<p>Create a virtual environment containing these packages: - pandas</p> <p>Alternatively, after creating a virtual environment, install the required packages using the requirements.txt file using </p> <p><code>pip install -r /path/to/requirements.txt</code></p>"},{"location":"reference/user-guide/redcap_etl/#python-function_1","title":"Python Function","text":"<p><pre><code>class cbioportalSummaryFileCombiner(fname_manifest, \n                                    fname_current_summary, \n                                    patient_or_sample)\n</code></pre> Source Code</p>"},{"location":"reference/user-guide/redcap_etl/#parameters_1","title":"Parameters:","text":"<pre><code>fname_manifest: (str) Filename to patient or sample manifest files. Should be either `summary_manifest_patient.csv` or `summary_manifest_sample.csv`\n\nfname_current_summary: (str) Filename of the current patient or sample summary file (i.e. data_clinical_patient.txt)\n\npatient_or_sample: (str) Options for patient_or_sample include `patient` or `sample` for patient or sample level summary merging.\n</code></pre>"},{"location":"reference/user-guide/redcap_etl/#attributes_1","title":"Attributes:","text":"<pre><code>return_orig(): (tuple)\nReturns the header and data in a tuple (header, data)\n\nsave_update(fname): (None)\nSaves updated/merged summary file to `fname`\n</code></pre>"},{"location":"reference/user-guide/redcap_etl/#example_2","title":"Example","text":"<pre><code>import sys\nimport pandas as pd\nsys.path.insert(0, '/path_to/data-curation/cbioportal-study-merge-tools')\nfrom cbioportal_summary_file_combiner import cbioportalSummaryFileCombiner\n\n\nfname_manifest_sample = 'summary_manifest_sample.csv'\nfname_s_sum = '/datahub/you_study/data_clinical_sample_template.txt'\n\n# Sample updates\nobj_s_combiner = cbioportalSummaryFileCombiner(fname_manifest=fname_manifest_sample, \n                                               fname_current_summary=fname_s_sum, \n                                               patient_or_sample='sample')\n\n# Return original summary dataframe and header\norig_header_s, orig_summary_s = obj_s_combiner.return_orig()\n\n# Save updated/merged data\nobj_s_combiner.save_update(fname=fname_cbio_s_save)\n</code></pre>"},{"location":"reference/user-guide/redcap_etl/#manual-summary-merging","title":"Manual summary merging","text":"<p>Manual merging can be done using the class <code>cBioPortalSummaryMergeTool</code> Source</p> <p>More under construction...</p>"},{"location":"reference/user-guide/training/","title":"Required Training for Data Access","text":""},{"location":"reference/user-guide/training/#citi-program","title":"CITI Program","text":"<p>Here\u2019s the link to register for a CITI account. Once you have made an account, you can select the applicable training modules you are required to complete.</p> <p>You need to complete both Human Subjects Protection (HSP) and Good Clinical Practice (GCP) regardless of you being a wetlab or a drylab memeber.</p>"},{"location":"reference/user-guide/training/#additional-resource-related-to-training-on-internal-websites","title":"Additional resource related to training on internal websites","text":"<p>These resources provide an overview of the 12-245 patient consent process and the role of CRA.</p> <ul> <li>12-245 Toolkit</li> <li>Using Specimens in Research (the Genetics link has info about GAP)</li> <li>DUA</li> </ul>"}]}